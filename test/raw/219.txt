English to Chinese Translation: How Chinese Character Matters?AbstractWord segmentation is helpful in Chinese nat- ural language processing in many aspects. However it is showed that different word seg- mentation strategies do not affect the per- formance of Statistical Machine Translation (SMT) from English to Chinese significant- ly. In addition, it will cause some confu- sions in the evaluation of English to Chinese SMT. So we make an empirical attempt to translation English to Chinese in the charac- ter level, in both the alignment model and lan- guage model. A series of empirical compari- son experiments have been conducted to show how different factors affect the performance of character-level English to Chinese SMT. We also apply the recent popular continuous s- pace language model into English to Chinese SMT. The best performance is obtained with the BLEU score 41.56, which improve base- line system (40.31) by around 1.2 BLEU s- core.1 IntroductionWord segmentation is necessary in most Chinese language processing doubtlessly, because there are no natural spaces between characters in Chinese tex- t (Xi et al., 2012). It is defined in this paper as character-based segmentation if Chinese sentence is segmented into characters, otherwise as word seg- mentation.In Statistical Machine Translation (SMT) in which Chinese is target language, few work have∗Correspondence author.shown that better word segmentation will lead to better result in SMT (Zhao et al., 2013; Chang et al., 2008; Zhang et al., 2008). Recently Xi et al. (2012) demonstrate that Chinese character alignment can improve both of alignment quality and translation performance, which also motivates us the hypothe- sis whether word segmentation is not even necessary for SMT where Chinese as target language.From the view of evaluation, the difference be- tween the word-based segmentation methods will al- so makes the evaluation of SMT where Chinese as target language confusing. The automatic evalua- tion methods (such as BLEU and NIST BLEU s- core) in SMT are mostly based on n-gram preci- sion. If the segmentation of test sets are differen- t, the elements of the n-gram of test sets will al- so be different, which means that the evaluation is made on different test sets. To evaluate the qual- ity of Chinese translation output, the International Workshop on Spoken Language Translation in 2005 (IWSLT’2005) used the word-level BLEU metric (Papineni et al.,2002). However, IWSLT’08 and NIST’08 adopted character-level evaluation metrics to rank the submitted systems. Although there are also a lot of other works on automatic evaluation of SMT, such as METEOR (Lavie and Agarwal, 2007), GTM (Melamed et al., 2003) and TER (Snover et al., 2006), whether word or character is more suit- able for automatic evaluation of Chinese translation output has not been systematically investigated (Li et al., 2011). Recently, different kinds of character- level SMT evaluation metrics are proposed, which also support that character-level SMT may have its own advantage accordingly (Li et al., 2011; Liu and 
Ng, 2012).Traditionally, Back-off N-gram Language Mod-els (BNLM) (Chen and Goodman, 1996; Chen and Goodman, 1998; Stolcke, 2002) are being widely used for probability estimation. For a better prob- ability estimation method, recently, Continuous- Space Language Models (CSLM), especially Neu- ral Network Language Models (NNLM) (Bengio et al., 2003; Schwenk, 2007; Le et al., 2011) are be- ing used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012; Wang et al., 2013). These works have shown that CSLMs can improve the BLEU scores of SMT when com- pared with BNLMs, on the condition that the train- ing data for language modeling are in the same size. However, in practice, CSLMs have not been wide- ly used in SMT mainly due to high computational costs of training and using CSLMs. Since the using costs of CSLMs are very high, it is difficult to use C- SLMs in decoding directly. A common approach in SMT using CSLMs is the two pass approach, or n- best reranking. In this approach, the first pass uses a BNLM in decoding to produce an n-best list. Then, a CSLM is used to rerank those n-best translations in the second pass (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012). Near- ly all of the previous works only conduct CSLMs on English, we conduct CSLM on Chinese in this pa- per. Vaswani et al. propose a method for reducing the training cost of CSLM and apply it into SMT decoder (Vaswani et al., 2013). Some other stud- ies try to implement neural network LM or transla- tion model for SMT (Gao et al., 2014; Devlin et al., 2014; Zhang et al., 2014; Auli et al., 2013; Liu et al., 2013; Sundermeyer et al., 2014; Cho et al., 2014; Zou et al., 2013; Lauly et al., 2014; Kalchbrenner and Blunsom, 2013).The remainder is organized as follows: In Section 2, we will review the background of English to Chi- nese SMT. The character based SMT will be pro- posed in Section 3. In Section 4, the experiments will be conducted and the results will be analyzed . We will conclude our work in the Section 5.2 BackgroundThe ancient Chinese (or Classical Chinese, 文言文) can be conveniently split into characters, for mostcharacters in ancient Chinese still keep understood by one who only knows modern Chinese (or Written Vernacular Chinese, 白话文) words. For example, “三人行,则必有我师焉。” is one of the popular sentences in the Analects (论语), and its correspond- ing modern Chinese words and English meaning are shown in TABLE 1. From the table, we can see that the characters in ancient Chinese have indepen- dent meaning, but most of the characters in modern Chinese do not, and they must combine together in- to words to make sense. If we split modern Chinese sentences into characters, the semantic meaning in the words will partially lose. Whether or not this semantic function of Chinese word can be partly re- placed by the alignment model and Language Model (LM) of character-based SMT will be shown in this paper.   Ancient Chinese   Modern Chinese  English Meaning   三 人 行 , 则 必 有 我 师 焉 。                     三个 人 走路,那么 一定 存在 我的 老师 在其中 。           three people walk , so must be my teacher/tutor there .            Table 1: Ancient Chinese and Modern ChineseSMT as a research domain started in the late 1980s at IBM (Brown et al., 1993), which maps individual words to words and allows for deletion and insertion of words. Lately, various research- es have shown better translation quality with phrase translation. Phrase-based SMT can be traced back to Och’s alignment template model (Och and Ney, 2004), which can be re-framed as a phrase trans- lation system. Other researchers augmented their systems with phrase translation, such as Yamada and Knight (Yamada and Knight, 2001), who used phrase translation in a syntax-based model.The phrase translation model is based on the noisy channel model. Bayes rule is mostly used to refor-
mulate the translation probability for translating a foreign sentence f into target e as:argmaxep(e|f) = argmaxep(f|e)p(e) (1)This allows for the probabilities of an LM p(e) and a separated translation model p(f|e). During decoding, the foreign input sentence f is segmented into a sequence of phrases f1i. It is assumed a unifor- m probability distribution over all possible segmen- tations. Each foreign phrase fi in f1i is translated into an target phrase ei. The target phrases may be reordered. Phrase translation is modeled by a prob- ability distribution Ω(fi|ei) . Recall that due to the Bayes rule, the translation direction is inverted.Reordering of the output phrases is modeled by a relative distortion probability distribution d(starti, endi−1), where starti denotes the start position of the foreign phrase that is translated in- to the ith target phrase, and endi−1 denotes the end position of the foreign phrase that was translated in- to the (i − 1) − th target phrase. A simple distortion model d(starti, endi−1) = α|starti−endi−1−1| with an appropriate value for the parameter α is set.In order to calibrate the output length, a factor ω (called word cost) for each generated English word in addition to the tri-gram LM pLM is proposed. This is a simple means to optimize performance. Usually, this factor is larger than 1, biasing toward longer output. In summary, the best output sentence given a foreign input sentence f according to the model is:argmaxep(e|f) = argmaxep(f|e)pLM (e)ωlength(e), (2) where p(f|e) is decomposed into:both the the alignment part p(f|e) and the LM p(e) will help retrieve the sematic meaning in the charac- ters which is originally represented by words. So it is possible that we can process the English to Chi- nese in character level without word segmentation, which may also avoid the confusion in the evalua- tion part as proposed above.3 Character-based versus Word-based SMTThe standards of segmentation between word-based and character-based English to Chinese translation are different, as well as the standard of the evalua- tion of them. That is, the test data contains words as the smallest unit for word-based SMT, and charac- ters for character-based SMT. So the translated sen- tences of word-based translation will be converted into character-based sentence, and evaluated togeth- er with character-based translation BLEU score for fair comparison. We select two popular segmenta- tion segmenters, one of which is based on Forward Maximum Matching (FMM) algorithm with the lex- icon of (Low et al., 2005), and the other is based on Conditional Random Fields (CRF) with the same implementation of (Zhao et al., 2006). Because most Chinese words contains 1 to 4 characters, so we set the word-based LM as default trigram in SRILM, and character-based LM for 5-gram. All the differ- ent methods share the same other default parameters in the toolkits which will be further introduced in Section 4.There seems to be no ambiguity in different char- acter segmentations, however English characters, numbers and other symbols are also contained in the corpus. If they are split into “characters” like “年 增 长 百 分 之 2 0 0”(200%incre- mentperyear)or“J o r d a n 是 伟 大 的 篮 球 运 动 员”(Jordanisagreat basketball player), they will cause a lot of misun- derstanding. So the segmentation is only used for Chinese characters, and the foreign letters, numbers and other symbols in Chinese text are still kept con- sequent.Shown in Table 2, the BLEU score of SMT sys- tem with character-based segmenter is much higher than both FMM and CRF segmenters. The word- based English to Chinese SMT system is trained andp(f1i|ei1) = φi1Ω(fi|ei)d(starti, endi−1).(3)In this paper, the f stands for English and the e stands for Chinese. In short, there are three main parts both in the English to Chinese and Chinese to English SMT: the alignment p(f|e), the LM p(e) and the parameters training (tuning). When Chinese is the foreign language, there is only the alignment model p(f |e) containing Chinese language process- ing. Contrarily, when Chinese is the target language,
tuned in word level and evaluated in character lev- el, so we use the character-based LM to re-score the nbest-list of the results of the FMM and CRF seg- menters. Firstly we convert the translated 1000-best candidates for each sentence into characters. Then calculate their LM scores by the character-based LM, and replace the word-based LM score with character-based LM score. At last we re-calculate the global score to get the new 1-best candidate with the same tuning weight as before. The BLEU s- core of re-ranked method is slightly higher than be- fore, but still much less than the result of charac- ter segmenter. Although we can not conclude the character-based segmenter is better simply accord- ing to this experiment, this result gives us the con- fidence that our approach is reasonable and feasible at least.4 Comparison ExperimentWe use the patent data for the Chinese to English patent translation subtask from the NTCIR-9 paten- t translation task (Goto et al., 2011). The parallel training, development, and test data consists of 1 million (M), 2,000, and 2,000 sentences, respective- ly1 .The basic settings of the NTCIR-9 English to Chi- nese translation baseline system (Goto et al., 2011) was followed2. The Moses phrase-based SMT sys- tem was applied (Koehn et al., 2007), together with GIZA++ (Och and Ney, 2003) for alignment and MERT (Och, 2003) for tuning on the developmen- t data. 14 standard SMT features were used: five translation model scores, one word penalty score, seven distortion scores and one LM score. The translation performance was measured by the case- insensitive BLEU on the tokenized test data3.4.1 The AlignmentIn this subsection we investigate two factors in the phrase alignment. Four different kinds of methods1Since we are the participants of NTCIR-9, so we have the bilingual sides of the evaluation data.2We are aware that the original NTCIR patentMT baseline is designed for Chinese-English translation. In this paper, we follow the same setting of the baseline system, only convert the source language and the target language.3It is available at http://www.itl.nist.gov/iad/ mig/tests/mt/2009/for heuristics and three kinds of maximum length of phrases in phrase table are used for word alignmen- t, with other default parameters in the toolkits. The results are shown in Table 3. The grow − diag − final − and, which will be set as default without special statement in the following sections, is shown better than other settings, and the BLEU score do not increase as the maximum length of phrases in- creases.  Alignment Parameters   BLEU (dev)   BLEU (test)  unionintersect grow-diag-final grow-diag-final-and     42.24 40.64 42.70 42.80       39.33 38.08 39.78 40.31    Maximum Length   BLEU (dev)   BLEU (test)  7 10 13    42.80 42.78 42.85     40.3140.04 40.30      Table 3: Different Heuristics Used for Word Alignment4.2 The N -gram Language ModelIn this part, we will investigate how the factors in the n-gram LM influence the whole system.The scale of the training corpus is one of the most important factors to LM. And “more data is better data” (Brants and Xu, 2009) has been proved to be one of the most important rules for constructing a LMs. First we randomly divide the whole training sets into 4 parts equally. We build the LM with 1, 2 and 4 parts (i.e. for 1/4, 1/2 and the whole corpus respectively), with other setting as default. Then, we add the dictionary information to the LM. The pr s- tands for the size of the dictionary and the pf stands for the characters’ frequency in the dictionary. The results in Table 4 show that using the whole corpus for language training is necessary and using the dic- tionary information does not improve the translation performance.We select the three most popular smoothing al- gorithms, Witten-Bell, Kneser-Ney (KN), and im- proved Kneser-Ney (improved KN), and compare their performance in the character-level English to Chinese SMT task. As shown in Table 5, when 
  Segmentation Methods  BLEU FMM SegmenterFMM Segmenter + Character-based LM Re-rank CRF SegmenterCRF Segmenter + Character-based LM Re-rank Character Segmenter      34.56 35.08 38.28 38.78 40.31          Table 2: Comparison Between Word-based Translation and Character-based Translation  Size ofThe Corpus   BLEU (dev)   BLEU (test)  1/4 Corpus1/2 Corpusthe whole Corpus    42.30 42.51 42.80     39.76 40.19 40.31   Dictionaries    pr=10k pf=5 pr=10k pf=10 pr=20k pf=10 No Dictionary     42.63 42.60 42.73 42.80       40.01 40.17 40.02 40.31       Table 4: Scale of Corpus for LMn is too small , the result is less satisfactory, and the BLEU score continues increase as n increases. However, the BLEU score begins to decrease when the LM becomes too long. The best 9-gram LM with Witten-Bell smoothing, corresponding to 5-gram to 7-gram in word-based LM, which is the widestly used in word-bases English to Chinese SMT.broadly accepted as the evaluate standard when we tune the other parameters using the minimum error rate training, which means that the MERT stage will not stop until it reaches the highest 4-gram BLEU on the development set. However, the same sentence becomes longer if the character based segmentation is applied. That is, four words may be segmented into around 10 characters. Will the system gain a better performance if the n-gram of BLEU score in the MERT convergence standard increases as the n- gram in the LM increases?To evaluate this hypothesis, the alignment model is set the same as the best performance in Table 3, and 5-gram LM with improved KN smoothing is set for LM. The results in Table 6 show that singly in- creasing the n-gram of MERT can not improve the performance of SMT.Table 6: Different Setting on MERT4.4 Parameter CombinationsWe have investigated how different factors affect the performance of English to Chinese SMT. However, most of the other factors are fixed when we discuss one single factor. So in this subsection, we analyze how the combined factors perform in the whole sys- tem.Firstly, we combine the parameters of the smooth- ing methods and the maximum length of phrases to- gether. The LM is set to 9-gram and grow − diag − f inal − and is set for alignment, which has the best BLEU score in n-gram LM experiments. Other fac-   n-gram MERT   n-gram BLEU (dev)  4-gram BLEU (test)   4 7 10     42.80 25.45 15.02   40.3140.30 40.17     Smoothing Method  n-gram LM   BLEU (dev)   BLEU (test)  Kneser-Ney Improved KN Improved KN Improved KN Witten-Bell Witten-Bell Witten-Bell       9 7 9 11 7 9 11        42.55 42.95 42.84 42.44 42.72 42.71 42.44             39.91 40.30 40.55 40.07 40.10 40.62 39.67         4.3Table 5: Different Smoothing Methods for LMThe TuningWe have shown that the different lengths of n-gram LMs make a significant influence in the English to Chinese translation. The 4-gram BLEU score is
  Smoothing Method  n-gram MERT   BLEU (dev)   BLEU (test)  KNKN Improved KN Improved KN Improved KN Witten-Bell Witten-Bell       4 7 4 7 10 4 7        42.55 25.33 42.84 25.93 15.82 42.71 25.45             39.91 40.65 40.55 40.75 40.37 40.62 40.30      tors is set as default in the toolkits. The results are shown in Table 7.    Smoothing Method (LM)  Maximum Length (align)  BLEU (dev)   BLEU (test)   KNKNKN Improved KN Improved KN Improved KN Witten-Bell Witten-Bell Witten-Bell                710137101371013  42.55 42.80 42.89 42.84 43.00 40.07 42.71 42.85 42.85                 39.91 40.49 39.93 40.55 40.24 40.56 40.62 40.06 40.09           Table 7: Parameter Combinations of Smoothing Methods and Maximum Length of Phrase AlignmentThen, the length of n-gram MERT and the differ- ent order n-gram LM are tuned together. We set the Improved KN as the smoothing method, and others as default in the toolkits. The results are shown in Table 8.Table 9: Parameter Combinations of n-gram MERT and Smoothing Methodsbring the best performance up to now in Table 10.4.5 Continues Space Language ModelTraditional Backoff N-gram LMs (BNLMs) have been widely used in many NLP tasks (Jia and Zhao, 2014; Zhang et al., 2012; Xu and Zhao, 2012).Recently, Continuous-Space Language Model- s (CSLMs), especially Neural Network Language Models (NNLMs) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011), are ac- tively used in SMT (Schwenk et al., 2006; Schwenk et al., 2006; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). These models have demonstrated that CSLMs can improve BLEU s- cores of SMT over n-gram LMs with the same sized corpus for LM training. An attractive feature of C- SLMs is that they can predict the probabilities of n- grams outside the training corpus more accurately.A CSLM implemented in a multi-layer neural net- work contains four layers: the input layer projects all words in the context hi onto the projection layer (the first hidden layer); the second hidden layer and the output layer achieve the non-liner probability es- timation and calculate the LM probability P (wi |hi ) for the given context (Schwenk, 2007).The CSLM calculates the probabilities of al- l words in the vocabulary of the corpus given the context at once. However, due to too high computa- tional complexity, the CSLM is only used to calcu- late the probabilities of a subset of the whole vocab- ulary. This subset is called a short-list, which con- sists of the most frequent words in the vocabulary. The CSLM also calculates the sum of the probabil- ities of all words not in the short-list by assigning a  n-gram LM  n-gram MERT   BLEU (dev)   4-gram BLEU (test)  7 7 9 9 9 13      4 7 4 7 10 7       42.95 25.54 42.84 25.93 15.82 25.41           40.30 39.91 40.55 40.75 40.37 40.47       Table 8: Parameter Combinations of n-gram LM and n- gram MERTAt last, the length of n-gram MERT and the s- moothing methods are tuned together. The LM is set as 9-gram, the best BLEU score in n-gram LM experiments, and other factors set as default in the toolkits. The results are shown in Table 9.Among different parameters-combined setting, BLEU score is from 38.08 to 40.75, and the best per- formance is not gained when all the factors which singly perform best are put together. The highest BLEU score occurs when the 9-gram LM, the 7- gram MERT method and the improved KN smooth- ing algorithm. This BLEU score is about one per- cent higher than our baseline. At last, we show three parameter combinations with their NIST scores that
  Factors vs BLEU  (1) 40.75 (2) 40.65 (3) 40.62 Maximum Length of Phrases Heuristic for Alignment Scales of LMDictionary of LMn-gram of LM Smoothing of LM n-gram MERT NIST Score         7 grow-diag-final-and wholenone9Improved KN79.32               10 grow-diag-final-and wholenone9Kneser-Ney79.40        10 grow-diag-final-and whole none 9 Witten-Bell 4 9.23         Table 10: Parameters for TOP PerformanceTable 11: CSLM Re-rank and decoding for TOP Performance  Methods vs BLEU (1) 40.75  (2) 40.65 (3) 40.62 CSLM Re-rank CSLM Decoding  41.15 41.34   41.2741.34   41.1841.57   neuron. The probabilities of other words not in the short-list are obtained from an Backoff N-gram LM (BNLM) (Schwenk, 2007; Schwenk, 2010; Wang et al., 2013; Wang et al., 2015).Let wi, hi be the current word and history, respec- tively. The CSLM with a BNLM calculates the prob- ability of wi given hi, P(wi|hi), as follows:in the second pass (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012).Because CSLM outperforms BNLM in probabili- ty estimation accuracy and BNLM outperforms C- SLM in computational time. To integrate CSLM more efficiently into decoding, some existing ap- proaches calculate the probabilities of the n-grams before decoding and store them (Wang et al., 2013; Wang et al., 2014; Arsoy et al., 2013; Arsoy et al., 2014) in n-gram format. That is, n-grams from BNLM are used as the input of CSLM, and the out- put probabilities of CSLM together with the corre- sponding n-grams of BNLM constitute converted C- SLM. The converted CSLM is directly used in SMT, and its decoding speed is as fast as the n-gram LM.From the above tables, we find the most impor- tant parameter for character-based English to Chi- nese translation is the LM, and other parameters just have a minor influence. To verify this observation, we use 9-gram character based CSLM (Schwenk et al., 2006), with 4096 characters in the short list, the projection layer of dimension 256 and the hidden layer of dimension 192 are set in the CSLM exper- iments. (1) We add the CSLM score as the addi- tional feature to re-rank the 1000-best candidates in the top three performance In Table 10. The weight parameters were tuned by using Z-MERT (Zaidan, 2009). This method is called CSLM Re-rank. (2) We follow (Wang et al., 2013)’s method and con- vert CSLM into n-gram LM. This converted CSLM can be directly applied to SMT decoding and called ∑ Pc(wi|hi) w∈V0 Pc(w|hi)P ( w i | h i ) = Pb (wi |hi )P(h) ifw ∈Votherwise (4)s ii0 where V0 is the short-list, Pc(·) is the probability cal- culated by the CSLM, ∑ Pc(w|hi) is the sum-w∈V0mary of probabilities of the neuron for all the wordsin the short-list, Pb(·) is the probability calculated by the BNLM, andPs(hi) =∑v∈V0Pb(v|hi).(5)We may regard that the CSLM redistributes the probability mass of all words in the short-list, which is calculated by using the n-gram LM.Due to too high computational cost, it is diffi- cult to use CSLMs in decoding directly. As men- tioned in the introduction, a common approach in SMT using CSLMs is a two-pass procedure, or n- best re-ranking. In this approach, the first pass uses a BNLM in decoding to produce an n-best list. Then, a CSLM is used to re-rank those n-best translations
CSLM-decoding.It is shown in Table 11 that the BLEU score nearly improve by 0.4 point to 0.6 point (CSLM Re-rank) and 0.6 point to 0.9 point (CSLM-decoding). This indicates that the CSLMs affect the performance of character based SMT in a significant way. This may indicate that the LM can take part place of the seg- mentation for character based English to Chinese SMT. A better character-based English to Chinese translation can be obtained by building a better LM.5 ConclusionBecause the role of word segmentation in En- glish to Chinese translation is arguable, an attemp- t of character-based English to Chinese translation seems to be necessary. In this paper, we have shown why character-based English to Chinese translation is necessary and feasible, and investigated how dif- ferent factors perform in the system from the align- ment, LM and the tuning aspects. Several empirical studies, including recent popular CSLM, have been done to show how to determine a optimal param- eters for better SMT performance, and the results show that the LM is the most important factor for character-based English to Chinese translation.AcknowledgmentsWe appreciate the anonymous reviewers for valu- able comments and suggestions on our paper. Rui Wang, Hai Zhao and Bao-Liang Lu were partially supported by the National Natural Science Foun- dation of China (No. 60903119, No. 61170114, and No. 61272248), the National Basic Research Program of China (No. 2013CB329401), the Sci- ence and Technology Commission of Shanghai Mu- nicipality (No. 13511500200), the European U- nion Seventh Framework Program (No. 247619), the Cai Yuanpei Program (CSC fund 201304490199 and 201304490171), and the art and science inter- discipline funds of Shanghai Jiao Tong University (A study on mobilization mechanism and alerting threshold setting for online community, and media image and psychology evaluation: a computational intelligence approach).ReferencesEbru Arsoy, Stanley F. Chen, Bhuvana Ramabhadran, and Abhinav Sethy. 2013. Converting neural network language models into back-off language models for ef- ficient decoding in automatic speech recognition. In Proceeding of International Conference on Acoustic- s, Speech and Signal Processing, Vancouver, Canada, May.Ebru Arsoy, Stanley F. Chen, Bhuvana Ramabhadran, and Abhinav Sethy. 2014. Converting neural net- work language models into back-off language models for efficient decoding in automatic speech recognition. IEEE/ACM Transactions on Audio, Speech, and Lan- guage Processing, 22(1):184–192.Michael Auli, Michel Galley, Chris Quirk, and Geoffrey Zweig. 2013. Joint language and translation model- ing with recurrent neural networks. In Proceedings of the 2013 Conference on Empirical Methods in Natu- ral Language Processing, pages 1044–1054, Seattle, Washington, USA, October.Yoshua Bengio, Re ́jean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic lan- guage model. Journal of Machine Learning Research (JMLR), 3:1137–1155, March.Thorsten Brants and Peng Xu. 2009. Distributed lan- guage models. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Com- putational Linguistics, Companion Volume: Tutorial Abstracts, NAACL-Tutorials ’09, pages 3–4, Boulder, Colorado, USA. Association for Computational Lin- guistics.Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematic- s of statistical machine translation: parameter estima- tion. Comput. Linguist., 19(2):263–311, June.Pi-Chuan Chang, Michel Galley, and Christopher D. Manning. 2008. Optimizing Chinese word segmen- tation for machine translation performance. In Pro- ceedings of the Third Workshop on Statistical Machine Translation, StatMT ’08, pages 224–232, Columbus, Ohio, USA. Association for Computational Linguis- tics.Stanley F. Chen and Joshua Goodman. 1996. An empir- ical study of smoothing techniques for language mod- eling. In Proceedings of the 34th annual meeting on Association for Computational Linguistics, ACL ’96, pages 310–318, Santa Cruz, California, USA. Associ- ation for Computational Linguistics.Stanley F. Chen and Joshua Goodman. 1998. An empiri- cal study of smoothing techniques for language model- ing. Technical report, Computer Science Group, Har- vard Univ.
Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase represen- tations using rnn encoder–decoder for statistical ma- chine translation. In Proceedings of the 2014 Con- ference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724–1734, Doha, Qatar, October.Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul. 2014. Fast and robust neural network joint models for statis- tical machine translation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1370–1380, Baltimore, Maryland, June.Jianfeng Gao, Xiaodong He, Wen-tau Yih, and Li Deng. 2014. Learning continuous phrase representations for translation modeling. In Proceedings of the 52nd Annual Meeting of the Association for Computation- al Linguistics, pages 699–709, Baltimore, Maryland, June.Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and Benjamin K. Tsou. 2011. Overview of the paten- t machine translation task at the NTCIR-9 workshop. In Proceedings of NTCIR-9 Workshop Meeting, pages 559–578, Tokyo, Japan, December.Zhongye Jia and Hai Zhao. 2014. A joint graph mod- el for pinyin-to-chinese conversion with typo correc- tion. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1512–1523, Baltimore, Maryland, June.Nal Kalchbrenner and Phil Blunsom. 2013. Recurren- t continuous translation models. In Proceedings of the 2013 Conference on Empirical Methods in Natu- ral Language Processing, pages 1700–1709, Seattle, Washington, USA, October.Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Con- stantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceed- ings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Pro- ceedings of the Demo and Poster Sessions, pages 177– 180, Prague, Czech Republic, June. Association for Computational Linguistics.Stanislas Lauly, Hugo Larochelle, Mitesh Khapra, Balaraman Ravindran, Vikas C Raykar, and Amrita Saha. 2014. An autoencoder approach to learning bilingual word representations. In Advances in Neural Information Processing Systems, pages 1853–1861.Alon Lavie and Abhaya Agarwal. 2007. Meteor: an au- tomatic metric for mt evaluation with high levels ofcorrelation with human judgments. In Proceedings of the Second Workshop on Statistical Machine Transla- tion, StatMT ’07, pages 228–231, Prague, Czech Re- public. Association for Computational Linguistics.Hai-Son Le, I. Oparin, A. Allauzen, J. Gauvain, and F. Yvon. 2011. Structured output layer neural network language model. In Acoustics, Speech and Signal Pro- cessing (ICASSP), 2011 IEEE International Confer- ence on, pages 5524–5527, Prague, Czech Republic, May. IEEE.Maoxi Li, Chengqing Zong, and Hwee Tou Ng. 2011. Automatic evaluation of Chinese translation output: Word-level or character-level? In Proceedings of the 49th Annual Meeting of the Association for Compu- tational Linguistics: Human Language Technologies, pages 159–164, Portland, Oregon, USA, June. Associ- ation for Computational Linguistics.Chang Liu and Hwee Tou Ng. 2012. Character-level machine translation evaluation for languages with am- biguous word boundaries. In Proceedings of the 50th Annual Meeting of the Association for Computation- al Linguistics: Long Papers - Volume 1, ACL ’12, pages 921–929, Jeju Island, Korea, USA. Association for Computational Linguistics.Lemao Liu, Taro Watanabe, Eiichiro Sumita, and Tiejun Zhao. 2013. Additive neural networks for statistical machine translation. In Proceedings of the 51st Annu- al Meeting of the Association for Computational Lin- guistics, pages 791–801, Sofia, Bulgaria, August.Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005. A Maximum Entropy Approach to Chinese Word Seg- mentation. In Proceedings of the 4th SIGHAN Work- shop on Chinese Language Processing, Jeju Island, Korea, October. Association for Computational Lin- guistics.I. Dan Melamed, Ryan Green, and Joseph P. Turi- an. 2003. Precision and recall of machine transla- tion. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Com- putational Linguistics on Human Language Technol- ogy: companion volume of the Proceedings of HLT- NAACL 2003–short papers - Volume 2, NAACL-Short ’03, pages 61–63, Edmonton, Canada. Association for Computational Linguistics.Tomas Mikolov, Martin Karafia ́t, Lukas Burget, Jan Cer- nocky`, and Sanjeev Khudanpur. 2010. Recurren- t neural network based language model. In INTER- SPEECH, pages 1045–1048.Jan Niehues and Alex Waibel. 2012. Continuous space language models using restricted boltzmann machines. In Proceedings of the International Workshop for Spo- ken Language Translation, IWSLT 2012, pages 311– 318, Hong Kong.
Franz Josef Och and Hermann Ney. 2003. A systemat- ic comparison of various statistical alignment models. Comput. Linguist., 29(1):19–51, March.Franz Josef Och and Hermann Ney. 2004. The alignmen- t template approach to statistical machine translation. Comput. Linguist., 30(4):417–449, December.Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Compu- tational Linguistics, pages 160–167, Sapporo, Japan, July. Association for Computational Linguistics.Holger Schwenk, Daniel Dchelotte, and Jean-Luc Gau- vain. 2006. Continuous space language models for statistical machine translation. In Proceedings of the COLING/ACL on Main conference poster sessions, COLING-ACL ’06, pages 723–730, Sydney, Australi- a. Association for Computational Linguistics.Holger Schwenk, Anthony Rousseau, and Mohammed Attik. 2012. Large, pruned or continuous space lan- guage models on a gpu for statistical machine transla- tion. In Proceedings of the NAACL-HLT 2012 Work- shop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT, WLM ’12, pages 11–19, Montreal, Canada, June. Associa- tion for Computational Linguistics.Holger Schwenk. 2007. Continuous space language models. Computer Speech and Language, 21(3):492– 518.Holger Schwenk. 2010. Continuous-space language models for statistical machine translation. The Prague Bulletin of Mathematical Linguistics, pages 137–146.Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of trans- lation edit rate with targeted human annotation. In In Proceedings of Association for Machine Translation in the Americas, pages 223–231.Le Hai Son, Alexandre Allauzen, Guillaume Wisniewski, and Franc ̧ois Yvon. 2010. Training continuous space language models: some practical issues. In Proceed- ings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 778–788, Cambridge, Massachusetts, October. Asso- ciation for Computational Linguistics.LeHaiSon,AlexandreAllauzen,andFranc ̧oisYvon. 2012. Continuous space translation models with neu- ral networks. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, NAACL HLT ’12, pages 39–48, Montreal, Canada, June. Association for Computational Linguis- tics.Andreas Stolcke. 2002. Srilm-an extensible language modeling toolkit. In Proceedings International Con-ference on Spoken Language Processing, pages 257–286, Seattle, USA, November.Martin Sundermeyer, Tamer Alkhouli, Joern Wuebker,and Hermann Ney. 2014. Translation modeling with bidirectional recurrent neural networks. In Proceed- ings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 14–25, Doha, Qatar, October.Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and David Chiang. 2013. Decoding with large-scale neu- ral language models improves translation. In Proceed- ings of the 2013 Conference on Empirical Method- s in Natural Language Processing, pages 1387–1392, Seattle, Washington, USA, October.Rui Wang, Masao Utiyama, Isao Goto, Eiichro Sumi- ta, Hai Zhao, and Bao-Liang Lu. 2013. Convert- ing continuous-space language models into n-gram language models for statistical machine translation. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 845– 850, Seattle, Washington, USA, October. Association for Computational Linguistics.Rui Wang, Hai Zhao, Bao Liang Lu, Masao Utiyama, and Eiichiro Sumita. 2014. Neural network based bilin- gual language model growing for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 189–195, Doha, Qatar, October.Rui Wang, Hai Zhao, Bao-Liang Lu, M. Utiyama, and E. Sumita. 2015. Bilingual continuous-space lan- guage model growing for statistical machine trans- lation. Audio, Speech, and Language Processing, IEEE/ACM Transactions on, 23(7):1209–1220, July.Ning Xi, Guangchao Tang, Xinyu Dai, Shujian Huang, and Jiajun Chen. 2012. Enhancing statistical ma- chine translation with character alignment. In Pro- ceedings of the 50th Annual Meeting of the Associa- tion for Computational Linguistics (Volume 2: Short Papers), pages 285–290, Jeju Island, Korea, July. As- sociation for Computational Linguistics.Qiongkai Xu and Hai Zhao. 2012. Using deep linguistic features for finding deceptive opinion spam. In Pro- ceedings of 24th International Conference on Compu- tational Linguistics, pages 1341–1350, Mumbai, Indi- a, December.Kenji Yamada and Kevin Knight. 2001. A syntax-based statistical translation model. In Proceedings of the 39th Annual Meeting on Association for Computation- al Linguistics, ACL ’01, pages 523–530, Toulouse, France. Association for Computational Linguistics.Omar F. Zaidan. 2009. Z-MERT: A fully configurable open source tool for minimum error rate training of machine translation systems. The Prague Bulletin of Mathematical Linguistics, 91:79–88.
Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita. 2008. Improved statistical machine translation by multiple Chinese word segmentation. In Proceedings of the Third Workshop on Statistical Machine Trans- lation, StatMT ’08, pages 216–223, Columbus, Ohio, USA. Association for Computational Linguistics.Xiaotian Zhang, Hai Zhao, and Cong Hui. 2012. A ma- chine learning approach to convert CCGbank to Penn treebank. In Proceedings of 24th International Con- ference on Computational Linguistics, pages 535–542, Mumbai, India, December.Jingyi Zhang, Masao Utiyama, Eiichiro Sumita, and Hai Zhao. 2014. Learning hierarchical translation spans. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 183– 188, Doha, Qatar, October.Hai Zhao, Chang-Ning Huang, and Mu Li. 2006. An im- proved Chinese word segmentation system with con- ditional random field. In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 162–165, Sydney, Australia, July. Association for Computational Linguistics.Hai Zhao, Masao Utiyama, Eiichiro Sumita, and Bao- Liang Lu. 2013. An empirical study on word seg- mentation for Chinese machine translation. In Pro- ceedings of the 14th international conference on Com- putational Linguistics and Intelligent Text Processing - Volume 2, CICLing’13, pages 248–263, Berlin, Hei- delberg. Springer-Verlag.Will Y. Zou, Richard Socher, Daniel Cer, and Christo- pher D. Manning. 2013. Bilingual word embeddings for phrase-based machine translation. In Proceedings of the 2013 Conference on Empirical Methods in Nat- ural Language Processing, pages 1393–1398, Seattle, Washington, USA, October.