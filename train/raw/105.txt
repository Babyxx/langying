Reducing Lexical Features in Parsing by Word Embeddings AbstractThe high-dimensionality of lexical features in parsing can be memory consuming and cause over-fitting problems. We propose a general framework to replace all lexical feature tem- plates by low-dimensional features induced from word embeddings. Applied to a near state-of-the-art dependency parser (Huang et al., 2012), our method improves the baseline, performs better than using cluster bit string features, and outperforms a recent neural net- work based parser. A further analysis shows that our framework has the effect hypothe- sized by Andreas and Klein (2014), namely (i) connecting unseen words to known ones, and (ii) encouraging common behaviors among in- vocabulary words.1 IntroductionLexical features are powerful machine learning in- gredients for many NLP tasks, but the very high- dimensional feature space brought by these features can be memory consuming and cause over-fitting problems. Is it possible to use low-dimensional word embeddings to reduce the high-dimensionality of lexical features? In this paper, we propose a gen- eral framework for this purpose. As a proof of con- cept, we apply the framework to dependency pars- ing, since this is a task where lexical features are essential.Our approach is illustrated in Figure 1. Con- sider a transition-based dependency parser (Yamada and Matsumoto, 2003; Nivre et al., 2006; Zhang and Clark, 2008; Huang and Sagae, 2010; ZhangTemplates:s0wsaw = (... 1 s0wlook = (... 0...0 ...) 1 ...)q0wyou q0wme= (... 1 = (... 0 ...0 ...) 1 ...)Weights:⋮W(s0wsaw) W(s0wlook) ⋮ W(q0wyou) W(q0wme)⋮W(s0e1) ⋮W(s0ed)      Replace lexical feature templates by embedding features··= Scores= Scores    Features:s0esaw = (0.6, ... , 0.2)q0eyou= (0.5, ... , 0.8)  s0elook = (0.4, ... , 0.3)q0emeW(q0e1) ......⋮= (0.7, ... , 0.9)  W(q0ed)Figure 1: Each lexical feature template is replaced by asmall number of embedding features.and Nivre, 2011), in which the words on top of the stack and the queue (denoted by s0w and q0w, re- spectively) are typically used as features to calcu- late scores of transitions. When s0w is used as a feature template, the features in this template (e.g. s0 wsaw and s0 wlook ) can be viewed as one-hot vec- tors of a dimension of the lexicon size (Figure 1). Corresponding to s0w, a weight is assigned to each word (e.g. W (s0 wsaw ) and W (s0 wlook )) for calcu- lating a transition score. Instead, we propose to utilize a d-dimensional word embedding, and re- place the feature template s0w by d features, namely s0e1, . . . , s0ed. Given the vector representation of a word (e.g., esaw = (0.6, . . . , 0.2)), we replace the lexical feature (e.g. s0 wsaw ) by a linear combination of the d features (e.g., s0esaw := 0.6s0e1 + . . . + 0.2s0 ed ). Then, instead of the weights in a num- ber of lexicon size assigned to s0w, now we use d
s3 s2 s1 s0stackhas contributed to the power of neural based ap- proaches. In this work, we conjecture that the power may partly come from the low-dimensionality of word embeddings, and this advantage can be trans- ferred to traditional feature based systems. Our ex- periments support this conjecture, and we expect the proposed method to help more mature, proven-to- work existing systems.Machine learning techniques have been proposed for reducing model size and imposing feature spar- sity (Suzuki et al., 2011; Yogatama and Smith, 2014). Compared to these methods, our approach is simple, without extra twists of objective functions or learning algorithms. More importantly, by using word embeddings to reduce lexical features, we ex- plicitly exploit the inherited syntactic and semantic similarities between words.Another technique to reduce features is dimen- sion reduction by matrix or tensor factorization (Ar- gyriou et al., 2007; Lei et al., 2014), but typically applied to supervised learning. In contrast, we use word embeddings trained from unlabeled or auto- matically labeled corpora, bringing the aspects of semi-supervised learning or self-training.3 FormalizationIn this section, we formalize the framework of re- ducing lexical features. We take transition-based parsing as an example, but the framework can be ap- plied to other systems using lexical features.3.1 Transition-based ParsingIn typical transition-based parsing, input words are put into a queue and partially built parse trees are cached in a stack (Figure 2). At each step, a shift- reduce action is selected, which consumes words from the queue and/or build new structures in the stack. For the set of actions, we adopt the arc- standard system (Yamada and Matsumoto, 2003; Nivre, 2008; Huang and Sagae, 2010), in which the actions are: weights (i.e., W(s0e1),...,W(s0ed)) to calculate a transition score. In this work, we reduce feature space dimensionality by replacing all lexical fea- tures, including combined features such as s0wq0w, by the word embedding features.In experiments, we applied the framework to a near state-of-the-art dependency parser (Huang et al., 2012), evaluated different vector operations for replacing combined lexical features, and ex- plored different word embeddings trained from un- labeled or automatically labeled corpora. We ex- pect word embeddings to augment parsing accuracy, by the mechanism hypothesized in Andreas and Klein (2014), namely (i) to connect unseen words to known ones, and (ii) to encourage common be- haviors among in-vocabulary words. In contrast to the negative results reported in Andreas and Klein (2014), we find that our framework indeed has these effects, and significantly improves the baseline. As a comparison, our method performs better than the technique of replacing words by cluster bit strings (Koo et al., 2008; Bansal et al., 2014), and the results outperform a neural network based parser (Chen and Manning, 2014).2 Related WorkA lot of recent work has been done on training word vectors (Mnih and Hinton, 2009; Mikolov et al., 2013; Lebret and Collobert, 2014; Pennington et al., 2014), and utilizing word vectors in various NLP tasks (Turian et al., 2010; Andreas and Klein, 2014; Bansal et al., 2014). The common approach (Turian et al., 2010; Koo et al., 2008; Bansal et al., 2014) is to use vector representations in new fea- tures, added to (near) state-of-the-art systems, and make improvement. As a result, the feature space gets even larger. We instead propose to reduce lex- ical features by word embeddings. To our own sur- prise, though the feature space gets much smaller, the resulted system performs better.Another stream of research is to use word embed- dings in whole neural network architectures (Col- lobert et al., 2011; Socher et al., 2013; Chen and Manning, 2014; Weiss et al., 2015; Dyer et al., 2015; Watanabe and Sumita, 2015). Though this is a promising direction and has brought breakthroughs in the field, the question is left open on what exactlyq0 q1 q2 queue saw   youwithher s0lFigure 2: An internal state of a dependency parser. I
1. Shift, which pops the top of the queue and pushes it to the stack;2. Reduce-Left, which replaces the top two trees in the stack by their consolidated tree, left as child;3. Reduce-Right, which replaces the top two trees in the stack by their consolidated tree, right as child.Following Huang et al. (2012), we use the max- violation perceptron for global learning and beam- search for decoding.In order to select the appropriate action, a set of features are used for calculating transition scores of each action. The features are typically extracted from internal states of the queue and the stack. For example, if we denote the elements in the stack by s0, s1, . . . from the top, and elements in the queue by q0, q1, . . . from the front; then, the words such as s0 w and q0 w, the POS-tags such as s0 t, and the combined word and POS-tags such as s0wt are used as features. Other features include the POS-tag s0lt (where s0l denotes the leftmost child of s0, and s0r denotes the rightmost child of s0), and the combined feature s0wq0w, etc.If the corresponding words and POS-tags are specified in a concrete state, we use subscripts of w and t to denote the concrete feature. For ex- ample, from the state illustrated in Figure 2, we can extract features such as s0wsaw, q0wyou, s0tVBD, s0wsawtVBD, s0ltPRP, and s0wsawq0wyou, etc.For the purpose of this work, we mainly focus on the words (e.g., wsaw , wyou ) in the above features. Other parts, including positions such as s0 and q0, and POS-tags such as tVBD, are regarded as formal symbols.3.2 Reducing Lexical FeaturesFormally, we define lexial features as the features comprising one or more words, possibly in combina- tion with other symbols. We propose to replace lex- ical features as follows, and leaving other features (e.g. s0tVBD) unchanged in the system.Lexical Feature of One Word Let sw be a one word lexical feature, where w is the word and s is an arbitrary symbol. Let e = (vi)1≤i≤d be ad-dimensional vector representation of the word w, where vi is the i-th entry. Then, we replace sw by se, a linear combination of se1, . . . , sed:se := d i=1vi · (sei).For example, assume that the word “saw” has a vec- tor representation esaw = (0.6, . . . , 0.2). Then, the feature s0wsaw is replaced bys0esaw := 0.6s0e1 + . . . + 0.2s0ed.In the above, s0e1,...,s0ed are introduced to re- place the feature template s0w. Note that, instead of using a different feature s0wx for each different word x, now we only have d features, s0e1, . . . , s0ed, commonly used by all words, across the feature tem- plate s0 w.As another example, in the case of features com- bining a word and its POS tag, such as s0 tVBD wsaw , we treat s0tVBD as a formal symbol and replace the feature as the following:s0tVBDesaw := 0.6s0tVBDe1 + . . . + 0.2s0tVBDed.Lexical Feature of Two or More Words For lex- ical features of two or more words, such as s0wq0w, we replace the words by a combination of the two or more corresponding word vectors. More precisely, for a two-word lexical feature sw1w2, assume that the vectors e1 = (ui)1≤i≤d and e2 = (vi)1≤i≤d rep- resent w1 and w2, respectively. Then, we propose the following operations1 to replace sw1w2:• OUTER PRODUCT (⊗):dds(e1 ⊗e2):=  uivj ·(seie ̃j), i=1 j=1Forexample,ifesaw =(0.6,...,0.2)andeyou = (0.5, . . . , 0.8), then s0wsawq0wyou is replaced by:s0q0(esaw ⊗ eyou) := (0.6 × 0.5)s0q0e1e ̃1 + . . . + (0.6 × 0.8)s0q0e1e ̃d + . . .+ (0.2 × 0.8)s0q0ede ̃d. Here, e ̃1,...,e ̃d are copies of e1,...,ed.1Operations for more than three word vectors are similar. 
    Dev Test Unseen Huang et al. (2012)  91.93 91.68 89.01 Different Operations, using STATE embedding: OUTERSUM CONCATENATION    92.57∗ 92.25∗ 92.18     92.20∗ 91.85 91.86   90.27∗ 90.10∗ 89.96   Different Embeddings, using OUTER operation: PLAIN TREE STATE    92.33∗ 92.37∗ 92.57∗     91.78 92.09∗ 92.20∗   90.08∗ 89.82 90.27∗   Cluster Bit String: PLAINTREE STATE Bansal et al. (2014)     91.71 90.38 91.31 92.06       91.20 90.07 90.96 91.75    89.18 88.00 89.04 90.13    Neural Network (Chen and Manning, 2014): Random PLAIN TREE STATE     86.37 90.68 91.06 91.03       86.19 90.48 90.82 90.57    81.06 87.02 87.38 87.88   • SUM (+):s(e1 + e2) :=  (ui + vi) · (sei). d i=1Following the previous example, s0wsawq0wyou is replaced by:s0q0(esaw + eyou) := (0.6 + 0.5)s0q0e1 + . . . + (0.2 + 0.8)s0q0ed.• CONCATENATION (⊕):dds(e1 ⊕e2):= ui ·(sei)+ vj ·(se ̃j).i=1 j=1Following the example, replace s0wsawq0wyou bysq(e ⊕e ):=0.6sqe +...+0.2sqe 00sawyou 001 00d+ 0.5s0q0e ̃1 + . . . + 0.8s0q0e ̃d.Theoretically, OUTER PRODUCT is the natural operation, because if s0 wx and q0 wy are regarded as high-dimensional one-hot vectors (Figure 1), the feature combination s0 wx q0 wy corresponds to the outer product of s0 wx and q0 wy (i.e., s0 wx q0 wy fires when s0wx and q0wy fire). Empirically, we find that OUTER indeed performs the best among the three operations; however, the outer product also intro- duces d2 embedding features, many more than the d features in SUM or 2d features in CONCATENA- TION. We also find that SUM performs better than CONCATENATION, being both effective and low- dimensional (Section 4.1).4 ExperimentsWe reimplemented the parser of Huang et al. (2012) and replaced all lexical feature templates by em- bedding features, according to our framework. We set beam size to 8, and report unlabeled attachment scores (UAS) on the standard Penn Treebank (PTB) split, using the data attached to Huang et al. (2012)’s system2. POS-tags are assigned by Stanford Tag- ger3. To highlight the effect of word embeddings on unseen words, we also report UAS on 148 sentences in the Dev. set which contain words in vocabularyTable 1: Parsing Results (UAS). Numbers marked by as- terisk (∗) are statistically significant (p < 0.05), com- pared to the baseline (Huang et al., 2012) under a paired bootstrap test.of the embeddings but unseen in PTB training data (Unseen).We built 300 dimensional word embeddings from 6 months articles in New York Times Corpus4 (01/2007-06/2007, 1.5M sentences), for words of frequencies greater than 50. Word vectors are ob- tained from singular value decomposition (SVD) of the PPMI matrices (Levy and Goldberg, 2014b), for co-occurrence matrices of target words with various types of contexts (Levy and Goldberg, 2014a), to be specified later. We choose SVD for training word vectors because it is fast; and recent research sug- gests that SVD can perform as well as other embed- ding methods (Levy et al., 2015).We investigated the following types of contexts for training word vectors: PLAIN, which uses words within a window of 3 to each side of the target word as contexts; TREE, which uses words within 3 steps of the target in the dependency trees, obtained from applying Huang et al. (2012)’s parser to the cor- pus; and STATE, which records the internal states of2 http://acl.cs.qc.edu/ ̃lhuang/3 http://nlp.stanford.edu/software/corenlp.shtml 4 https://catalog.ldc.upenn.edu/LDC2008T19  
    %#      !&'  !"'  !%' # !# !%# !"# '         %'  "'    %#      !%#  !%&  !# # !# !%# !"# &          #  %&  %#  "#'       "#$ "#& " "!"#& !"#$ "#$ "#& " "!"#& !"#$                  "#$  "#(  "#)  "#*  &   "#&  "#$  "#'  "#(  "#)  "#*  "#+  "#,  "#-  &               ( !"' !"& !"% !"# ( !"' !"& !"% !"#        Figure 3: We plot X by the weight of the feature s0wx, and Y by the weight of s0ex, for x of high (Left) and middle (Right) frequency words.Huang et al. (2012)’s parser, and uses words at posi- tions {s1, s2, s3, s0l, s0r, s1l, s1r, q0, q1, q2} as con- texts for a target s0. These positions are where pars- ing features are extracted from. We expect TREE and STATE to encode more syntactic related infor- mation.4.1 Parsing ResultsThe parsing results are shown in Table 1. We find that, the OUTER operation used for combined fea- tures and the STATE contexts for training word vec- tors perform the best for transition-based parsing, but other settings also improve the baseline (Huang et al., 2012), especially for sentences containing un- seen words. We conducted paired bootstrap test to compare our proposed method with the baseline, and find out that most improvements are statistically sig- nificant.We also compared with the method of replacing words in lexical features by cluster bit strings (Koo et al., 2008; Bansal et al., 2014). We use bit strings constructed from hierarchical clusters induced from the previous word embeddings; as well as the the bit strings constructed in Bansal et al. (2014)5. Lengths of the bit strings are set to 4, 6, 8, 12, 16, and 20. It turns out that the performance gains are not as sig- nificant as our proposed method.For reference, we report results by a neural net- work based parser (Chen and Manning, 2014), since our method shares a similar motivation with Chen and Manning’s work, i.e. to use low-dimensional dense features instead of high-dimensional sparse features in parsing, aiming to obtain better gener- alization. For initializing word embeddings in the neural network, we tried 300 dimensional random5 http://ttic.uchicago.edu/ ̃mbansal/Figure 4: We plot X by cosine similarities between words, and Y by cosine similarities of weights, learned for lexical features (Upper) and embedding features (Lower). Words are of high (Left) and middle (Right) frequencies.vectors and the PLAIN, TREE, STATE vectors as de- scribed previously. We find that pre-trained word embeddings can improve performance, with TREE and STATE slightly better than PLAIN, suggesting that TREE and STATE may contain more informa- tion useful to parsing. However, the STATE vector is not as powerful as used with Huang et al. (2012)’s parser, suggesting that for a given baseline, it may be more helpful to train word vectors from contexts specific to that baseline. Chen and Manning’s parser generally performs worse than Huang et al. (2012)’s baseline, suggesting that we cannot immediately ob- tain a better parser by switching to neural networks; other factors, such as global optimization and care- fully selected features may still have merits, which makes our method useful for improving existing ma- ture parsers.4.2 AnalysisIs our modified parser really a feature reduction of the baseline system, i.e. is the parsing model trained for embedding features actually correlated to the baseline parsing model using lexical fea- tures? In Figure 3, we plot weights learned for the feature s0 wx as X , and weights for s0 ex as Y , where x ranges over high or middle frequency words. The weight for s0ex is calculated by taking inner product of the vector s0ex and the weight vec- tor (W (s0e1), . . . , W (s0ed)). As the direction of"#'                     !  !"# !"%  !"&  !"'  ( !  !"# !"%  !"&  !"'  (  
                                                  ààà   ààà         ààà   ààà     ààà   à    ààà   à“The Rochester, N.Y., photographic giant recently began marketing T- Max 3200, one of the fastest and most sensitive monochrome films.”Figure 6: Improved parsing results on parallel structure of adjectives.mensionality. This property may have two favor- able effects on parsing, as hypothesized in Andreas and Klein (2014): (i) to connect unseen words to known ones, and (ii) to encourage common behav- iors among in-vocabulary words.The effects on unseen words have been observed To illustrate the effects on in-vocabulary words, we take a specific parallel structure of adjectives. More precisely, we consider an internal state of the parser such that: s1 t and s0 t have POS-tags JJ, JJS or JJR; and s0lt has a POS-tag CC or Comma. Then, in 98.8% instances of such a state in the train- ing data, the golden label action is Reduce-Left, suggesting a strong tendency of the state to become a parallel structure of adjectives, such as “black and white”. However, when we parse New York Times data using the baseline parser, the proportion of Reduce-Left action when facing the state de- creases to 96.7%, suggesting that this tendency is “While it is possible that the Big Green initiative will be ruled unconsti- tutional, it is of course conceivable that in modern California it could slide through.”  Figure 5: Improved parsing results with unseen (bold) words.ààà        à     the regression lines show, weights learned for s0exare positively correlated to weights learned for s0wx.It suggests that the parsing model trained for em-bedding features is indeed correlated to the parsingmodel of the baseline, which implies that the base- ààà        àline parser and our modified parser would have sim- ilar behaviors. This may explain the significanceresults reported in Table 1: though our improve- ments against the baseline is fairly moderate, they are still statistically significant because our modi- fied parser behaves similarly as the baseline parser, but would correct the mistakes made by the base- line while preserving most originally correct labels. Such improvements are easier to achieve statistical significance (Berg-Kirkpatrick et al., 2012), and are arguably indicating better generalization.in the Unseen column in Table 1, and we present a concrete example in Figure 5. In this example, “con- ceivable” is unseen in the training data, thus cannot be recognized by the baseline parser; however, its word vector is similar to “subjective” and “undeni- ably”, whose behaviors are learned and generalized to “conceivable”, by our modified parser using em- bedding features.So how does our modified parser improve from the baseline? In Figure 4, we plot cosine similar- ities between word vectors as X, and cosine simi- larities between weight vectors of all one-word lex- ical features as Y , compared to the similarities of weights of the corresponding embedding features. The plots show that, for similar words, the learned weights for the corresponding lexical features are only slightly similar; but after the lexical features are reduced to low-dimensional embedding features, the learned weights for the corresponding features are more strongly correlated. In other words, weights for embedding features encourage similar behaviors between similar words, due to a much lower di-
                                 baselineSTATE + OUTER                        9290888684820.01 0.1 1 Used proportion of training data Figure 7: UAS on Dev. set, of models trained on less data.not fully generalized as a rule for parallel structure of adjectives. This is not astonishing, because POS- tags and surface forms of lexical features are diverse in the training data. However, when we use our mod- ified parser, the proportion of Reduce-Left ac- tion turns out to be 99.4%, significantly higher than using the baseline parser according to a permutation test. It suggests that our modified parser generalizes and strengthens the rule of parallel structure, by en- forcing similar behaviors among similar adjectives. A concrete example of improvement is presented in Figure 6.In Figure 7, we vary the size of training data and plot UAS of the obtained parsing models. As the figure shows, our modified parser using embedding features constantly outperforms the baseline. How- ever, the performance of both settings decrease as the training data size decreases, suggesting that there may not be much syntactic information encoded in the word embeddings, even though the word embed- dings are trained on internal states of the baseline parser, which is trained on full training data. We believe this graph indicates that, word embeddings can help parsing, but not because they encode ex- tra syntactic information; rather, it is because word embeddings bring better generalization.5 ConclusionWe have proposed a framework for reducing lexi- cal features by word embeddings, and applied the framework to transition-based dependency parsing.A near state-of-the-art parser is improved, even though the features are reduced. This work is still preliminary, as we have only tested on one parser; however, our results are promising and our analysis suggests that the proposed method may indeed bring better generalization. We believe our framework can help more systems to reduce lexical features and al- leviate the risk of overfitting, thanks to its generality.ReferencesJacob Andreas and Dan Klein. 2014. How much do word embeddingsencodeaboutsyntax? InProceedingsof ACL.Andreas Argyriou, Theodoros Evgeniou, and Massimil- iano Pontil. 2007. Multi-task feature learning. In Ad- vances in NIPS.Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2014. Tailoring continuous word representations for depen- dency parsing. In Proceedings of ACL.Taylor Berg-Kirkpatrick, David Burkett, and Dan Klein. 2012. An empirical investigation of statistical signifi- cance in nlp. In Proceedings of EMNLP-CoNLL.Danqi Chen and Christopher D Manning. 2014. A fast and accurate dependency parser using neural net- works. In Proceedings of EMNLP.Ronan Collobert, Jason Weston, Le ́on Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. J. Mach. Learn. Res., 12.Chris Dyer, Miguel Ballesteros, Wang Ling, Austin Matthews, and Noah A. Smith. 2015. Transition- based dependency parsing with stack long short-term memory. In Proceedings of ACL-IJCNLP.Liang Huang and Kenji Sagae. 2010. Dynamic program- ming for linear-time incremental parsing. In Proceed- ings of ACL.Liang Huang, Suphan Fayong, and Yang Guo. 2012. Structured perceptron with inexact search. In Proceed- ings of NAACL-HLT.Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Pro- ceedings of ACL.Re ́miLebretandRonanCollobert. 2014. Wordem- beddings through Hellinger PCA. In Proceedings of EACL.Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, and Tommi Jaakkola. 2014. Low-rank tensors for scoring dependency structures. In Proceedings of ACL.Omer Levy and Yoav Goldberg. 2014a. Dependency- based word embeddings. In Proceedings of ACL.UAS acuracy
Omer Levy and Yoav Goldberg. 2014b. Neural word em- bedding as implicit matrix factorization. In Advances in NIPS.Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Im- proving distributional similarity with lessons learned from word embeddings. Trans. ACL, 3.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in NIPS.Andriy Mnih and Geoffrey E. Hinton. 2009. A scalable hierarchical distributed language model. In Advances in NIPS.Joakim Nivre, Johan Hall, Jens Nilsson, Gu ̈ls ̧en Eryigˇit, and Svetoslav Marinov. 2006. Labeled pseudo- projective dependency parsing with support vector ma- chines. In Proceedings of CoNLL.Joakim Nivre. 2008. Algorithms for deterministic incre- mental dependency parsing. Comput. Linguist.Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word rep- resentation. In Proceedings of EMNLP.Richard Socher, John Bauer, Christopher D. Manning, and Ng Andrew Y. 2013. Parsing with compositional vector grammars. In Proceedings of ACL.Jun Suzuki, Hideki Isozaki, and Masaaki Nagata. 2011. Learning condensed feature representations from large unsupervised data sets for supervised learning. In Pro- ceedings of ACL-HLT.Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings of ACL.Taro Watanabe and Eiichiro Sumita. 2015. Transition- based neural constituent parsing. In Proceedings of ACL-IJCNLP.David Weiss, Chris Alberti, Michael Collins, and Slav Petrov. 2015. Structured training for neural net- work transition-based parsing. In Proceedings of ACL- IJCNLP.Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In In Proceedings of IWPT.Dani Yogatama and Noah A. Smith. 2014. Linguistic structured sparsity in text categorization. In Proceed- ings of ACL.Yue Zhang and Stephen Clark. 2008. A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing. In Proceedings of EMNLP.Yue Zhang and Joakim Nivre. 2011. Transition-based dependency parsing with rich non-local features. In Proceedings of ACL.