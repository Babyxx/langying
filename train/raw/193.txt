Mechanical Turk-based Experiment vs Laboratory-based Experiment: A Case Study on the Comparison of Semantic Transparency Rating DataAbstractIn this paper, we conducted semantic trans- parency rating experiments using both the traditional laboratory-based method and the crowdsourcing-based method. Then we com- pared the rating data obtained from these two experiments. We observed very strong cor- relation coefficients for both overall seman- tic transparency rating data and constituent se- mantic transparency data (rho > 0.9) which means the two experiments may yield com- parable data and crowdsourcing-based experi- ment is a feasible alternative to the laboratory- based experiment in linguistic studies. We also observed a scale shrinkage phenomenon in both experiments: the actual scale of the rat- ing results cannot cover the ideal scale [0, 1], both ends of the actual scale shrink towards the center. However, the scale shrinkage of the crowdsourcing-based experiment is stronger than that of the laboratory-based experiment, this makes the rating results obtained in these two experiments not directly comparable. In order to make the results directly compara- ble, we explored two data transformation al- gorithms, z-score transformation and adjusted normalization to unify the scales. We also in- vestigated the uncertainty of semantic trans- parency judgment among raters, we found that it had a regular relation with semantic trans- parency magnitude and this may further reveal a general cognitive mechanism of human judg- ment.1 IntroductionIn experimental linguistic studies, researchers are frequently frustrated by the problem of linguisticdata bottleneck which constantly limits the feasi- bility, efficiency, and reliability of various research projects. It’s caused by the practical difficulties of conducting traditional laboratory-based linguis- tic experiments. Firstly, it’s very difficult to obtain large samples using laboratory-based experiments for they are usually very time-consuming and expen- sive. In order to solve this problem, we need to find a more efficient and economic way to conduct lin- guistic experiments. Secondly, what’s more difficult is to recruit highly diverse subjects due to the diffi- culties in subject recruitment and the spacial limita- tions of laboratory-based experiments. As a result, researchers heavily and even blindly rely on rela- tively small sample size which is 30 or so (Sprouse, 2011) and the undergraduate subject pool. From the point of view of sampling, this is not a good prac- tice, since it raises the concern of external validity, i.e., the extent to which the experimental results can be generalized, because a small and homogeneous sample usually cannot be representative enough. In fact the external validity problem that results from using mainly undergraduate subjects is a typical one and has a dedicated term called the college sopho- more problem (Stanovich, 2007; Jackson, 2012). Al- though there are several responses to this criticism (Stanovich, 2007), the really convincing way to re- solve this problem is to use a more diverse subject pool.Mechanical Turk (MTurk) has emerged in recent years to be a promising solution to the problem of lin- guistic data bottleneck by providing a new paradigm for linguistic experiments, i.e., the MTurk-based ex- periment (Mason and Suri, 2012; Horton et al., 2011;
Paolacci et al., 2010; Schnoebelen and Kuperman, 2010; Buhrmester et al., 2011; Sprouse, 2011; Berin- sky et al., 2012), which can hopefully address all the problems mentioned above. MTurk is qualified as a genre of both crowdsourcing which refers to the ac- tivities to outsource tasks to undefined and generally large crowds on the web via open call (Howe, 2006; Estellés-Arolas and González-Ladrón-de Guevara, 2012; Wang et al., 2013; Schenk and Guittard, 2011; Howe, 2009), and human computation (Quinn and Bederson, 2009; von Ahn, 2005; Quinn and Beder- son, 2011). MTurk needs to be implemented through a website, or more precisely, an MTurk platform. An MTurk platform is an on-line crowdsourcing la- bor marketplace where requesters post small tasks (conventionally called Human Intelligence Tasks, or HITs) and workers undertake tasks for small pay (Mason and Suri, 2012; Sprouse, 2011). The most famous MTurk platform is Amazon’s Mechanical Turk (AMT, www.mturk.com) which was lunched publicly in November 2005; it started early and is so popular in the academic world that it is the de facto standard of MTurk implementation, and the genre name MTurk actually originated from its name and is used by some writers to refer to AMT spe- cially. There are other MTurk implementations, for example another well known MTurk platform is Crowdflower (www.crowdflower.com). Relevant demographics shows that the workers on either AMT (Ross et al., 2010; Pavlick et al., 2014; Ipeirotis, 2010) or Crowdflower1 are come from all over the world, so both can be treated as international MTurk platforms.In the early stage of the development of MTurk, it’s potential to be an efficient and economic tool for linguistic data collection (e.g., annotation, transcrip- tion, translation, etc.) and behavioral research (e.g., survey and experimentation) for social sciences has already been recognized and attempted (Snow et al., 2008; Kittur et al., 2008; Chen et al., 2009). Es- pecially since around 2010, there have been more and more reports on conducting experimental re- search using MTurk (Mason and Suri, 2012; Rand, 2012; Buhrmester et al., 2011; Horton et al., 2011;1For the demographics of Crowdflower’s worker pool, see https://success.crowdflower.com/hc/en-us/articles/202703345- Contributors-Crowd-Demographics, retrieved on Apr. 22, 2015.Paolacci et al., 2010; Schmidt, 2010; Munro et al., 2010; Schnoebelen and Kuperman, 2010; Sprouse, 2011; Enochson and Culbertson, 2015; Kuperman et al., 2012) and several of them focus on linguis- tic experiments (Munro et al., 2010; Schnoebelen and Kuperman, 2010; Sprouse, 2011; Enochson and Culbertson, 2015; Kuperman et al., 2012). Ex- periments conducted on MTurk platforms are usu- ally survey-based and use web questionnaires com- posed using the GUI toolkits provided by the plat- forms, and advanced users can make use of HTML, CSS, JavaScript, Adobe Flash (Simcox and Fiez, 2014; Enochson and Culbertson, 2015), etc., to re- alize additional elements, customized appearance, special control, and apparatus they need. Compared to laboratory-based experiment, the MTurk-based experiment has many attractive merits: 1) the re- cruitment and compensation of subjects is automatic, painless, on demand, and 24x7 based; 2) MTurk workers are willing to take part in experiments with much less pay than subjects of laboratory-based ex- periments; 3) it is a lot easier to obtain very large samples; 4) MTurk worker pool is far more diverse than typical undergraduate subject pool widely used in laboratory-based experiment; 5) the anonymous nature of MTurk-based experiment can largely help to avoid experimenter effect, subject crosstalk (Pao- lacci et al., 2010) and the problem of socially desir- able responses.Data quality is the key concern in conducting re- search using MTurk-based experiments because the MTurk setting is not so controllable as the labora- tory setting, a host of studies have been carried out to address this concern. The comparison between the data obtained from MTurk-based experiments and laboratory-based experiments suggests that MTurk- based experiments can provide comparable or even better data (Munro et al., 2010; Schnoebelen and Ku- perman, 2010; Sprouse, 2011; Horton et al., 2011). And a large set of classic effects discovered pre- viously in laboratory-based experiments have been successfully replicated using MTurk-based experi- ments even in the case of the experiments which require millisecond accuracy timing (Enochson and Culbertson, 2015; Simcox and Fiez, 2014; Crump et al., 2013; Horton et al., 2011). These positive results repeatedly confirm that MTurk is a reliable tool to conduct experimental research which not only yields 
valid data but also minimizes the cost in time, effort, and expense. Conducting research using MTurk- based experiments lets researchers concentrate on data analysis, creative thinking, and writing instead of being disturbed by various administrative tasks of laboratory-based experiments from time to time, therefore increases their academic productivity. Al- though, this methodology has not been completely established, its future seems to be guaranteed (Hor- ton et al., 2011).In order to evaluate a new method, it is a com- mon strategy to compare the results yield by the new method with the results yield by the established method to see their agreement. Although neither method is perfect or completely reliable, since the established method is well acceptable, if the new method agrees well enough with it, then the new method is also acceptable to be an alternative. We conducted two similar semantic transparency rat- ing experiments using the Mechanical Turk-based method and the traditional laboratory-based method. We will compare the results from these two experi- ments to see their agreement hence we can further evaluate the Mechanical Turk-based experimenta- tion.2 Method2.1 MTurk-based Semantic TransparencyRating Experiment2 2.1.1 MaterialsWe selected a total of 1, 176 disyllabic Chinese nominal compounds which have mid-range word frequencies and appear in both Sinica Corpus 4.0 (Chen et al., 1996) and the “Lexicon of Common Words in Contemporary Chinese 现代汉语常用词 表”, see Wang et al. (2014) for details.2.1.2 Experimental DesignNormally, a crowdsourcing experiment should be reasonably small in size. We randomly divide these 1,176 words into 21 groups, Gi (i = 1, 2, 3, ..., 21); each group has 56 words.Questionnaires We collect overall semantic trans- parency (OST) and constituent semantic trans- parency (CST) data of these words. In order to avoid2We have reported this experiment in Wang et al. (2014).interaction, we designed two kinds of questionnaires to collect OST data and CST data respectively. So Gi (i = 1, 2, 3, ..., 21) has two questionnaires, one OST questionnaire for OST data collection and one CST questionnaire for CST data collection. Besides titles and instructions, each questionnaire has 3 sec- tions. Section 1 is used to collect identity informa- tion includes gender, age, education and location. Section 2 contains four very simple questions about the Chinese language; the first two questions are open-ended Chinese character identification ques- tions, the third question is a close-ended homophonic character identification question, and the fourth one is a close-ended antonymous character identification question; different questionnaires use different ques- tions. Section 3 contains the questions for semantic transparency data collection. Suppose AB is a di- syllabic nominal compound, we use the following question to collect its OST rating scores: “How is the sum of the meanings of A and B similar to the meaning of AB?” And use the following two ques- tions to collect its CST rating scores of its two con- stituents: “How is the meaning of A when it is used alone similar to its meaning in AB?” and “How is the meaning of B when it is used alone similar to its meaning in AB?”. 7-point scales are used in section 3; 1 means “not similar at all” and 7 means “almost the same”.In order to evaluate the data received in the ex- periments, we embedded some evaluation devices in the questionnaires. We mainly evaluated intra- group and inter-group consistency; and if the data have good intra-group and inter-group consistency, we can believe that the data quality is good. In each group we choose two words and make them appear twice, we call them intra-group repeated words and we can use them to evaluate the intra-group consis- tency. We insert into each group two same extra words, w1“地步”, w2“高山”, to evaluate the inter- group consistency.Quality Control Measures On a crowdsourcing platform like Crowdflower, the participants are anonymous, they may try to cheat and submit in- valid data, and they may come from different coun- tries and speak different languages rather than the required one. There may be spammers who contin- uously submit invalid data at very high speed and 
they may even bypass the quality control measures to cheat for money. In order to ensure that the par- ticipants are native Chinese speakers and to improve data quality, we use the following measures, (1) a participant must correctly answer the first two Chi- nese character identification questions in the section 2s of the questionnaires, and he/she must correctly answer at least one of the last two questions in these section 2s; (2) If a participant do not satisfy the above conditions, he/she will not see Section 3s; (3) each word stimulus in section 3s has an option which al- lows the participants to skip it in case he/she does not recognize that word; (4) all the questions in the ques- tionnaires must be answered except the ones which allow to be skipped and are explicitly claimed to be skipped; (5) we wrote a monitor program to detect and resist spammers automatically; (6) after the ex- periment is finished, we will analyze the data and fil- ter out invalid data, and we will discuss this in detail in section 2.1.3.OST CST Gi n % n %Experimental Platform and Procedure Wechoose Crowdflower as our experimental platform,because according to our previous experiments, it isa feasible crowdsourcing platform to collect Chineselanguage data. We create one task for each question-naire on the platform; there are 21 groups of wordand each group has one OST questionnaire and oneCST questionnaire, so there are a total of 42 tasksTost,Tcst (i = 1,2,3,...,21). We publish these 42 iitasks successively, and for each task we create a monitor program to detect and resist spammers. All of these tasks use the following parameters: (1) each task will collect 90 responses; (2) we pay 0.15USD for each response of OST questionnaire and pay 0.25USD for each response of CST questionnaire; (3) each worker account of Crowdflower can only submit one response for each questionnaire and each IP address can only submit one response for each questionnaire; (4) we only allow the workers from the following regions (according to IP addresses) to submit data: Mainland China, Hong Kong, Macau, Taiwan, Singapore, Malaysia, USA, UK, Canada, Australia, Germany, France, Italy, New Zealand, and Indonesia; and we can dynamically disable or enable certain regions on demand in order to ensure both data quality and quantity.2.1.3 Data Cleansing and Result CalculationThe OST dataset produced by the OST task T ost (i = 1, 2, 3, ..., 21) is Dost. The CST dataset    G1 62 68.89 70G2 60 66.67 64G3 61 67.78 58G4 57 63.33 58G5 51 56.67 59G6 55 61.11 54G7 54 60 55G8 60 66.67 48G9 52 57.78 55G10 58 64.44 59G11 52 57.78 56G12 55 61.11 63G13 52 57.78 57G14 56 62.22 54G15 54 60 53G16 58 64.44 56G17 52 57.78 50G18 53 58.89 51G19 53 58.89 50G20 53 58.89 51G21 52 57.78 51Min 51 56.67 48 Max 62 68.89 7077.78 71.11 64.44 64.44 65.56 60 61.11 53.33 61.11 65.56 62.22 70 63.33 60 58.89 62.22 55.56 56.67 55.56 56.67 56.6753.33 77.78 61.11 62.01 5.91ii MedianMean 55.24SD 3.4produced by the CST task T cst is Dcst. Each dataset iicontains 90 responses. Because of the nature of crowdsourcing environment, there are many invalid responses in each dataset; so firstly we need to fil- ter them out in order to refine the data. A response is invalid if (1) its completion time is less than 135 seconds (for OST responses); its completion time is less than 250 seconds (for CST responses); or (2) it failed to correctly answer the first two questions of section 2s of the questionnaires; or (3) it wrongly answered the last two questions of section 2s of the questionnaires; or (4) it skipped more than six words in section 3s of the questionnaires; or (5) it used less than three numbers on the 7-point scales in section 3s of the questionnaires. We also filtered out the responses from the workers who appeared in more than one countries/regions according to their IP ad- dresses. The statistics of valid response are shown54 60 55 61.38 55.813.78 5.32 Table 1: Amount of valid response in the OST and CST datasets of each group.
in Table 1.The OST dataset Dost (i = 1, 2, 3, ..., 21) con-the most balanced constituent semantic transparency values. The sample should also cover all these four semantic transparency types. A total of 152 com- pounds were selected; all of the compounds have the modifier-head structure.2.2.2 QuestionnaireThe questionnaire is divided into three parts. Part I is the demographic questions, we ask the subjects to provide their demographic information on 1) gen- der, 2) age, 3) language background, 4) native place, and 5) email address (optional). Part II is the over- all semantic transparency rating task, the subjects are asked to rate the overall semantic transparency of the compound stimuli one by one, and we use the same questions and rating scales as the Mechanical Turk- based experiment. Part III is the constituent seman- tic transparency rating task, the subjects are asked to rate the constituent semantic transparency of the compound stimuli one by one, and we also use the same questions and rating scales as the Mechanical Turk-based experiment. We make “笑脸”, “蓝本”, “火灾”, “脾气” appear twice in the questionnaire, so we can use them to check the consistency of ratings. The questionnaire has a simplified Chinese charac- ter version and a traditional Chinese character ver- sion. And the questionnaires are implemented us- ing Google Form, the whole questionnaire is divided into pages, each page contains six stimuli. At the end of each quarter of the questionnaire, we show the subjects a notice to tell them that they can take a short break (three to five minutes) if they feel tired. It takes about 45 minutes to fill out the questionnaire.2.2.3 SubjectsWe recruited a total of 78 students at the Hong Kong Polytechnic University. Seventy-four of them are undergraduates, and four of them are postgradu- ates. Thirty-nine of them are from mainland China and the other 39 are Hong Kong local. The sub- jects from mainland China came from 19 different provinces: Anhui 安徽, 3; Chongqing 重庆, 3; Fu- jian 福建, 2; Gansu 甘肃, 1; Guangdong 广东, 3; Guizhou 贵州, 2; Hebei 河北, 1; Heilongjiang 黑龙 江, 2; Henan 河南, 1; Hubei 湖北, 1; Jiangsu 江苏, 2; Jilin 吉林, 1; Liaoning 辽宁, 1; Neimenggu 內蒙 古, 3; Shandong 山东, 5; Shanghai 上海, 1; Shanxi 陕西, 5; Tianjin 天津, 1; Zhejiang 浙江, 1. Fourty-itains ni valid responses; it means word w in the OSTdataset of the ith group has ni OST rating scores; the arithmetic mean of these ni OST rating scores is the OST result of word w. The CST results of the two constituents of word w are calculated using the same algorithm.2.2 Laboratory-based Semantic Transparency Rating Experiment2.2.1 MaterialThe Mechanical Turk-based semantic trans- parency rating experiment is a large-scale ex- periment, it collected the overall and constituent semantic transparency rating data for 1,176 com- pounds. This scale is beyond the capacity of common laboratory-based experiment given the time and resource limitations. So it is impossible for us the conduct a completely parallel semantic trans- parency rating experiment in the laboratory setting. As a practically and statistically feasible alternative, we extracted a representative sample of reasonable size for laboratory-based experiment from the 1, 176 compound stimuli of the Mechanical Turk-based experiment. Then the method comparison will be conducted on the basis of the sample.The compound stimuli of the Mechanical Turk- based semantic transparency rating experiment be- long to three structural categories, i.e., NN, AN, VN, the sample should cover all these category types. According the overall semantic transparency value and constituent semantic transparency value of com- pound, compounds are usually divided into four cat- egories: 1) TT, the compounds with the largest overall semantic transparency values and the most balanced constituent semantic transparency values, 2) TO, the compounds with the mid-range overall semantic transparency values and the most unbal- anced constituent semantic transparency values and the CST of the first morpheme is larger than that of the second, 3) OT, the compounds with the mid- range overall semantic transparency values and the most unbalanced constituent semantic transparency values and the CST of the second morpheme is larger than that of the first, and 4) OO, the compounds with the lowest overall semantic transparency values and
one subjects are 16 to 20 years old; 33 are 21 to 25; 4 are 26 to 30. Twenty-two of them are male, the other 56 are female. Their mother tongue is Chinese and all of them can speak Putonghua.2.2.4 ProcedureThe subjects were invited into the laboratories. Because the subjects from mainland China and Hong Kong would use different versions of questionnaire, two laboratories were prepared for the experiment, one was for the subjects from mainland China and the simplified character version questionnaire would be used, and the other laboratory was for the sub- jects from Hong Kong and the traditional character version questionnaire would be used. Each subject was assigned a unique code (or seat number). When the subjects arrived, they were guided to their desks according to their codes. On each desk there was a computer which was displaying a brief introduc- tion to the experiment and at the bottom of the in- troduction, there were two buttons: “I Agree” and “I Disagree” respectively. We briefly explained the ex- periment to the subjects orally, and then asked them to sign the consent forms on their desks first if they agreed to participate in our experiment. After they signed the consent forms, they could then read the introduction on the screen and press “I Agree” to start to fill in the questionnaire. Once a subject fin- ished the experiment, he/she would get an allowance of 100 Hong Kong dollars. All the 78 subjects fin- ished the experiment, so we collected 78 responses.2.2.5 Data Cleansing and Result CalculationWe firstly checked the responses one by one and filtered out invalid ones. A response is considered invalid if 1) more than 15 words were skipped (i.e., the subject claimed that he/she didn’t know these words), or 2) less than three numbers of the 7-point rating scale were used. Only two invalid responses were identified, one was from a mainland subject, the other was from a Hong Kong subject. So there are a total of 76 valid responses, this means each word was rated by 76 subjects. The OST and CST re- sults of these words were calculated based on these 76 responses, the calculation method was the same as the Mechanical Turk-based experiment.3 Results and Discussion 3.1 CorrelationWe can evaluate the semantic transparency rating re- sults from the Mechanical Turk-based experiment by examining to what extent they correlates with the re- sults from the laboratory-based experiment. This is a commonly used practice in psycholinguistics.Strictly speaking, the distributions of the overall and constituent semantic transparency of compound are not normal and do not satisfy the requirement of Pearson’s product-moment correlation coefficient, so the Spearman’s rank-order correlation coefficient is used. We calculated three correlation coefficients, 1) the correlation coefficient between the normal- ized OST results from the two experiments: 0.94, 2) the correlation coefficient between the normal- ized CST results of the first morphemes of the com- pounds from the two experiments: 0.93, and 3) the correlation coefficient between the normalized CST results of the second morphemes of the compounds from the two experiments: 0.92. All of the correla- tion coefficients are larger than 0.9 which indicates that the results from the Mechanical Turk-based ex- periment correlate strongly with the results from the laboratory-based experiment. From the scatter plots (see Figure 1), we can see that although these two kinds of results correlates strongly with each other, we cannot say that they agree with each other very well, because the dots do not distribute around the line of equality (the dashed line).Figure 1: Correlations between normalized OST and CST results from the MTurk-based experiment and the lab- based experiment.3.2 Scale Shrinkage IssueWe also checked and compared the distributions of the semantic transparency rating results from the Mechanical Turk- and laboratory- based exper- iments, see Figure 2. We can see that the distribu- tions of the results from the two experiments have ●● ●● ● ● ●●● ● ● ● ● ● ● ●●● ●●●●● ●●● ●●● ●●● ● ●● ● ●●●● ● ● ● ● ● ●● ●●● ●●● ●●●●● ●●●● ●●●●● ●● ●●● ●● ●●● ● ● ● ●●● ●●● ●● ● ●●●● ●●●● ●●●● ●●● ●● ●● ●●●0.750.500.250.00       0.2 0.4 0.6MTURKNC1CST ●● ●●● ● ●● ●●● ● ●●●● ● ●● ●●● ●● ●● ● ●●●●● ●●● ●● ●●● ● ● ●●● ●●● ●●● ●●●●● ● ● ●● ●●● ●● ●● ● ●● ●●●●● ● ● ● ●● ●● ● ●●●● ● ● ●●●● ● ● ● ● ●● ● ● ● ●●● ●●● ●●●●0.750.500.25      0.4 0.6 0.8MTURKNC2CST ●●●● ●●●● ●● ●●● ●●●● ● ●●●● ●●●●●● ●● ●●●● ●●●●●● ● ●●● ●●●●●●●●● ● ●●● ●●● ● ● ●● ●●● ●● ●●● ●●● ●●● ●● ●● ● ●● ●●●●●●●●●● ●● ● ● ●●1.000.750.500.250.00        0.4 0.6 0.8MTURKNOSTLABNC1CSTLABNC2CSTLABNOST
 12.5   10.07.55.02.50.0             0.2 0.40.6 0.8MTURKNC1CST  9630         0.2 0.4 0.6 0.8MTURKNC2CST  151050         0.4 0.6 0.8MTURKNOST  9630         0.25 0.50 0.75LABNC1CST 12.510.07.55.02.50.0               0.25 0.50 0.75LABNC2CST 12.510.07.55.02.50.0               0.25 0.50 0.75 1.00LABNOST Figure 2: Distributions of semantic transparency rating results from the MTurk-based experiment and the lab- based experiment.the similar overall forms, but the two kinds of re- sults distribute on different scales. The scale of the Mechanical Turk OST results is from 0.26 to 0.79, however the scale of the laboratory OST results is from 0.14 to 0.95; the scale of the Mechanical Turk C1CST results is from 0.19 to 0.76, while the scale of the laboratory C1CST results is from 0.08 to 0.86; the scale of the Mechanical Turk C2CST results is from 0.24 to 0.78, but the scale of the laboratory C2CST results is from 0.14 to 0.89. Since in our compound stimuli, there are completely transparent compounds and completely opaque compounds, so ideally, two kinds of results should share and cover the same scale from 0 to 1. But virtually, for this kind of subjective rating tasks, subjects rarely totally agree with each other and there is always some noise or errors of varied degrees. Consequently, the distri- butions of the results of subjective rating tasks rarely cover the whole scale. The actual scales usually shrink towards the center. The scale shrinkage of the results from the Mechanical Turk-based experiment is larger than that of the results from the laboratory- based experiment; this is perhaps because that the Mechanical Turk-based experiment has higher noise level than the laboratory-based experiment.3.3 Data TransformationBecause the semantic transparency results from the Mechanical Turk- and laboratory-based experiments use different scales and have different units, they are not directly comparable. In order to make the kinds of results comparable, we need to transformthe results so that they will use the same scale. We are going to examine two kinds of data transforma- tion methods: 1) Z-score transformation (standard- ization), 2) adjusted normalization; next we are go- ing to discuss them one by one.Z-score TransformationThe z score is calculated by the following formula:zscore= rawscore−mean standard deviationThe raw scores (normalized OST and CST results) from Mechancial Turk- and laboratory-based exper- iments are transformed into z scores according to the above formula; we call the z-score transformed nor- malized OST and CST results the standardized OST and CST results. After z-score transformation, the standardized semantic transparency results from the two experiments will share the same scale.Then we can further examine the agreement of the standardized semantic transparency results from the two experiments, see Figure 3. On these scat- ter plots, we can see that now all the dots distribute around the line of equality (the dashed line) and the regression line basically coincides with the line of equality; compared with the scatter plots based on the raw scores (see Figure 1), the standardized re- sults agree with each other better which makes the results from the two experiments comparable.Figure 3: Correlations between standardized OST and CST results from the MTurk-based experiment and the lab-based experiment.Adjusted NormalizationThe adjusted normalized score is calculated ac- cording to the following formula:raw score − min raw score ANscore = max raw score − min raw scoreSince the actual scales of the raw scores shrink to- wards the center, we can use the above formula to 210−1−2●●● ●●●●●● ●●●● ●●● ●●●● ● ●●●● ● ●●●● ●●●● ●●● ●● ●●●●●●● ● ●●● ●●●●●● ●● ● ●●●● ●●● ●●● ● ●●● ●●●●●● ● ●●● ●● ●●● ●●●●●●● ●●●● ●● ●●●          −2 −1 0 1 2MTURKSNC1CST ● ●●●● ● ●●●● ●●● ● ●●●● ● ●●●● ● ● ●●● ●●● ●●●●● ● ● ●● ●●● ● ●● ● ●●●● ● ●●● ●● ●●● ●●●● ●●● ● ●● ●● ●●●● ●●●● ● ●●●● ●● ●●●●●● ●●● ●● ●● ●●●● ● ●● ● ●● ●●10−1−2        −2 −1 0 1MTURKSNC2CST 210−1−2● ● ●●●● ●● ●●● ●● ●●●●●●● ●●●● ●●●● ●● ●● ●●● ● ●● ●●●●●●●● ●● ●●●●●● ● ● ● ●● ●●● ● ●● ● ● ●●●● ● ●● ●●● ●●●●●●● ●●●●● ●●●●●●●● ●● ● ●          −2 −1 0 1 2MTURKSNOST LABSNC1CSTLABSNC2CSTLABSNOSTcountcountcountcountcountcount
stretch the scales to [0, 1] again. When using this for- mula, we need to make sure that the maximum and minimum raw scores are not outliers, otherwise this transformation will fail. The results from the two ex- periments are both transformed using this formula, after this, they will again share the same scale. See Figure 4 for the relations between the adjusted nor- malized semantic transparency results from both ex- periments.Compared with the raw scores (see Figure 1), the adjusted normalized results from the Mechanical Turk- and laboratory-based experiments agree with each other better, but the agreement is not as good as the standardized results (see Figure 3). However the adjusted normalization method has an advantage over the standardization method, that is the adjusted normalization will yield results from 0 to 1 and this scale is accord with the definition of semantic trans- parency value.score can only be one of {1,2,3,4,5,6,7}. For the m OST rating scores, suppose the possibilities of the numbers on the 7-point scale to be chosen are p1 , p2 , ..., p7 respectively, the resultant OST value is the mean of these m rating scores and the uncertainty of judgment among raters can be calculated using the formula of information entropy:0.4   ●● ●●●●● ● ●● ● ●●●●●●●●●●●●●0.40.0●● ●● ● ●●●●● ●● ●●● ●●●●● ●● ●● ●● ● ●●●●●●●●●●●●●● ●●●●●●●● ●●●●● ●●●●● ●● ●●●●● ● ●●●●●●●● ●●●●●●●●● ●●●●● ● ● ●●●● ●● ●●●● ●● ● ●● ●●●●● ●●●●●●●● ● ●● ● ● ● ● ●● ● ● ● ● ●●●●●●● ●● ●0.500.250.00∑7 i=1using the same formula, C1CSTRIE and C2CSTRIE can also be calculated. See Figure 5 for the relation- ship between semantic transparency value and un- certainty of judgment among raters; both Mechani- cal Turk data and laboratory data are used to draw the figures. We can observe a very strong and regu- lar relation between them. In terms of this relation- ship, laboratory data show stronger and more regular relationship than Mechanical Turk data. This kind of curve may reveal some kind of general cognitive mechanism of human subjective judgment.2.8●●2.8● ●OSTRIE = −pi log2 pi    ● ●● ●● ●● ●●●●●● ●● 1.00 ●● ● ●●● ●●●●●●● ● ●●●● ●●● ● ● ● ●●● ●●● ●● ●● ●●●● ● ● ● ●●●●●●●●●●● ● 0.8●●●●●0.8●● ●●●●●● ●●●● ●●● 0.75  LABC1CSTRIEMTURKC1CSTRIELABC2CSTRIEMTURKC2CSTRIELABOSTRIEMTURKOSTRIELABANC1CSTLABANC2CSTLABANOST●● ● ● ●● ● ● ● ● ●●●● ●● ●●● ●● ● ● ● ● ●● ●● ● ● ●●    ●●●    ● ●●● 0.0 ● ●●●● ● ●● ●●●2.6● 2.4 ●●●● ●●● ●●●●● ●●●●●●● ●●● ● ● ●●●●● ● ●● ●●●●●●● ●●●●●●● ●● ●●●●●●●●●●●●●●●●● ●●●● ● ●●● ●● ●● ● ● ● ●●●●●●●● ●●● ●● ● ●●●●●●●●●●● ●●●● 2.6●● ● ●●● ●● ●●●●● ●● ●●●● ● ● ●●●●2.6● ● ●●  ●●●●● ●● ●●●● ●●● ●● ● ●●●●● ● ●●●● ● ● ●●●●    ●●●● ● ●●●●● ● ●●●●● ●● ●                0.00 0.25 0.50 0.75 1.00MTURKANC1CST0.00 0.25 0.50 0.75 1.00MTURKANC2CST0.00 0.25 0.50 0.75 1.00MTURKANOST●● ●●● ●●● ●●●● ● 2.4 ●●● ●● 2.4 ●●● ●●2.2 ● ●●●●●●●●● ●● ●●●● ●● ● ●   2.2   ●● 2.0● ●2.2 ● ● ● 2345 345 ● Figure 4: Correlations between adjusted normalized OST and CST results from the MTurk-based experiment and the lab-based experiment.3.4 Uncertainty of Semantic Transparency Judgment among RatersSemantic transparency rating task is a subjective rat- ing task. In such a task, the subjects rarely totally agree with each other and there are usually errors of varied degrees. So we can say that there is usu- ally some uncertainty or inconsistency of judgment among raters. Next we are going to measure the un- certainty of judgment among raters and to examine its relationship with the semantic transparency value.In our semantic transparency rating tasks, 7-point scales are used as the measurement instrument. For a di-morphemic word ab, suppose that m raters rated its overall semantic transparency (OST) and constituent semantic transparency (CST), so ab has m OST ratings scores and also has m C1CST rat- ing scores and m C2CST rating scores; each ratingMTURKC1CST● ●●● ●●●●●●●●● ● ●●●MTURKC2CST● ●● ● ● ●●●●●●●● ●●345MTURKOST● ●●●●●●●●●●●●● ●●●●●● ●● ●● ●              2.52.0●●● ●●2.52.0 ● ● ●●● ●● ●●●●● ●● ●● 2.5 ● ●●●●● ● ●● ●●●●● ●●● ● ● ● ● ● ●● ●●●● ●● ● ●●● ●●● ●●● ●● ●●● ● ● ● ● ● ●●● ● ●● ● ●●●● ● ● ●●●● ●●● ● ● ● ● ●● ● ●● ● ●●2.0●●● ●●● ● ●● ● ● ● ●●●●  ● ● ●● ●● ● ●● 1.5 ● ● ● ● ● ●● ● 1.5 ● ● ● ● ● ● 1.5 ●● ●Figure 5: Uncertainty of semantic transparency judg- ments among the raters of the MTurk-based experiment and the lab-based experiment.AcknowledgmentsThe work described in this paper was supported by a grant from the Research Grants Council of the Hong Kong Special Administrative Region, China (Project No. 544011).ReferencesAdam J Berinsky, Gregory A Huber, and Gabriel S Lenz. 2012. Evaluating online labor markets for experimen-1.023456 23456 23456LABC1CST LABC2CST LABOST●                 
tal research: Amazon. com’s mechanical turk. Politi-cal Analysis, 20(3):351–368.Michael Buhrmester, Tracy Kwang, and Samuel DGosling. 2011. Amazon’s mechanical turk a new source of inexpensive, yet high-quality, data? Per- spectives on Psychological Science, 6(1):3–5.Keh-Jiann Chen, Chu-Ren Huang, Li-Ping Chang, and Hui-Li Hsu. 1996. Sinica corpus: Design method- ology for balanced corpora. In B.-S. Park and J.B. Kim, editors, Proceeding of the 11th Pacific Asia Con- ference on Language, Information and Computation, pages 167–176. Seoul:Kyung Hee University.Kuan-Ta Chen, Chen-Chi Wu, Yu-Chun Chang, and Chin-Laung Lei. 2009. A crowdsourceable qoe evalu- ation framework for multimedia content. In Proceed- ings of the 17th ACM international conference on Mul- timedia, pages 491–500. ACM.Matthew JC Crump, John V McDonnell, and Todd M Gureckis. 2013. Evaluating amazon’s mechanical turk as a tool for experimental behavioral research. PLoS ONE, 8(3):e57410.Kelly Enochson and Jennifer Culbertson. 2015. Collect- ing psycholinguistic response time data using amazon mechanical turk. PLoS ONE, 10(3):e0116946, 03.Enrique Estellés-Arolas and Fernando González-Ladrón- de Guevara. 2012. Towards an integrated crowd- sourcing definition. Journal of Information science, 38(2):189–200.John J Horton, David G Rand, and Richard J Zeckhauser. 2011. The online laboratory: Conducting experi- ments in a real labor market. Experimental Economics, 14(3):399–425.Jeff Howe. 2006. The rise of crowdsourcing. Wired mag- azine, 14(6):1–4.Jeff Howe. 2009. Crowdsourcing: Why the Power of the Crowd Is Driving the Future of Business. Three Rivers Press.Panagiotis G Ipeirotis. 2010. Demographics of mechani- cal turk.Sherri Jackson. 2012. Research methods and statistics: A critical thinking approach. Cengage Learning.Aniket Kittur, Ed H Chi, and Bongwon Suh. 2008. Crowdsourcing user studies with mechanical turk. In Proceedings of the SIGCHI conference on human fac- tors in computing systems, pages 453–456. ACM.Victor Kuperman, Hans Stadthagen-Gonzalez, and Marc Brysbaert. 2012. Age-of-acquisition ratings for 30,000 english words. Behavior Research Methods, 44(4):978–990.Winter Mason and Siddharth Suri. 2012. Conducting be- havioral research on amazon’s mechanical turk. Be- havior research methods, 44(1):1–23.Robert Munro, Steven Bethard, Victor Kuperman, Vicky Tzuyin Lai, Robin Melnick, Christopher Potts, Tyler Schnoebelen, and Harry Tily. 2010. Crowd- sourcing and language studies: the new generation of linguistic data. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 122–130. Association for Computational Linguistics.Gabriele Paolacci, Jesse Chandler, and Panagiotis GIpeirotis. 2010. mechanical turk. 5(5):411–419.Running experiments on amazon Judgment and Decision making,Ellie Pavlick, Matt Post, Ann Irvine, Dmitry Kachaev, and Chris Callison-Burch. 2014. The language demo- graphics of amazon mechanical turk. Transactions of the Association for Computational Linguistics, 2:79– 92.Alexander J Quinn and Benjamin B Bederson. 2009. A taxonomy of distributed human computation. Human- Computer Interaction Lab Tech Report, University of Maryland.Alexander J Quinn and Benjamin B Bederson. 2011. Hu- man computation: a survey and taxonomy of a growing field. In Proceedings of the SIGCHI conference on hu- man factors in computing systems, pages 1403–1412. ACM.David G Rand. 2012. The promise of mechanical turk: How online labor markets can help theorists run be- havioral experiments. Journal of theoretical biology, 299:172–179.Joel Ross, Lilly Irani, M Silberman, Andrew Zaldivar, and Bill Tomlinson. 2010. Who are the crowd- workers?: shifting demographics in mechanical turk. In CHI’10 Extended Abstracts on Human Factors in Computing Systems, pages 2863–2872. ACM.Eric Schenk and Claude Guittard. 2011. Towards a char- acterization of crowdsourcing practices. Journal of In- novation Economics & Management, 7(1):93–107.L Schmidt. 2010. Crowdsourcing for human subjects research. Proceedings of CrowdConf.Tyler Schnoebelen and Victor Kuperman. 2010. Using amazon mechanical turk for linguistic research. Psi- hologija, 43(4):441–464.Travis Simcox and Julie A Fiez. 2014. Collecting re- sponse times using amazon mechanical turk and adobe flash. Behavior research methods, 46(1):95–111.Rion Snow, Brendan O’Connor, Daniel Jurafsky, and An- drew Y Ng. 2008. Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks. In Proceedings of the conference on empirical methods in natural language processing, pages 254– 263. Association for Computational Linguistics.
Jon Sprouse. 2011. A validation of amazon mechanical turk for the collection of acceptability judgments in lin- guistic theory. Behavior research methods, 43(1):155– 167.Keith E Stanovich. 2007. How to think straight about psychology. HarperCollins Publishers.Luis von Ahn. 2005. Human Computation. Ph.D. thesis, School of Computer Science, Carnegie Mellon Univer- sity, Pittsburgh, PA 15213 USA, 12.Aobo Wang, Cong Duy Vu Hoang, and Min-Yen Kan. 2013. Perspectives on crowdsourcing annotations for natural language processing. Language Resources and Evaluation, 47:9–31.Shichang Wang, Chu-Ren Huang, Yao Yao, and Angel Chan. 2014. Building a semantic transparency dataset of chinese nominal compounds: A practice of crowd- sourcing methodology. In Proceedings of Workshop on Lexical and Grammatical Resources for Language Processing, pages 147–156, Dublin, Ireland, August. Association for Computational Linguistics and Dublin City University.