Learning Weighted Sentimental Terms for ClassificationAbstractExtracting sentiment and topic lexicons is im- portant in natural language processing and opinion mining. Previous works have been done to successfully judge whether a term is an emotional term or not. However, little work has been done in measuring degrees of senti- ment for these terms. For example, the word excellent has stronger positive sentiment than the word good or okay. In this paper, we dis- cuss how to model this intricate sentimental d- ifference. A simple and effective model is pro- posed based on logistic regression to extrac- t emotional terms and each term is assigned to a sentiment weight. These sentimentally weighted terms can be used for sentiment clas- sification tasks. The new model is tested using uni-gram, bi-gram and mixed-gram language models on two benchmark datasets. The em- pirical results show the effectiveness and effi- ciency of the new proposed sentiment model.1 IntroductionSentiment analysis is an important task in natural language processing (NLP) for analyzing people’s opinions, attitudes and emotions towards certain ser- vices and products (Pang et al., 2002). It helps to survey unstructured public opinions of large scale like free discussions in online forums or posts in so- cial networks. When we think of emotive reviews or comments, we are inclined to think of predicates of personal taste (boring, fun), exclamatives (awesome,Li and Guo have contributed equally to this work and should be considered as co-first authors.damn) and other emotional words or terms that are more or less contributed to convey sentimental infor- mation of our opinions. Once we get knowledge of these dominant emotional terms, we could judge the sentimental polarity (positive or negative) of a given sentence or document. Unfortunately, for particular utterance in a given context, such significant emo- tional terms are not always so apparent. We hope to find a way to automatically extract these emotional terms from corpus.Sentiment classification can be simply considered as a binary text classification task. Most previous works focused on using supervised machine learn- ing methods based on the N-gram language model to do sentiment classification. However, sentiment weights of terms have not been obviously consid- ered in these works. In this paper, we propose a method to extract sentiment terms by assuming that every distinct term in the corpus has an unique senti- ment weight and the sentiment of a sentence or doc- ument is determined by a nonlinear combination of sentiment weights. We employ logistic regression to estimate sentiment weights of terms and use these weights to predict the sentiment of unseen sentences or documents.N -gram language model, especially uni-gram and bi-gram, are considered in this research. For ex- ample, in movie reviews, we can often see terms like worth watching, that expresses strong positive emotion. Though the word watching is emotionally neutral, it will be assigned to some positive weights alone with worth. Such large weight may become troublesome in sentences containing watching on- ly. In this paper, we develop a mixed-gram model 
by embedding both uni-gram and bi-gram together. Therefore, the bi-gram term worth watching will be assigned with large positive weights.2 Related WorksSentiment analysis has attracted great attention in natural language processing and text mining (Pang et al., 2002). Dominant approaches in sentimen- t classification generally follow traditional classifi- cation approaches (Xia et al., 2011), where a docu- ment is regarded as a bag-of-words (BoW), mapped into a feature vector, and then classified by ma- chine learning methods such as Naive Bayes (NB) (Lewis, 1998), Maximum Entropy (ME) (Nigam et al., 1999) or support vector machines (SVM) (Joshi and Penstein-Rose, 2009). The effectiveness of these machine learning methods for sentiment clas- sification is evaluated in (Pang et al., 2002).(Hu and Liu, 2004) proposed a lexicon-based al- gorithm to explore the sentiment orientation of a sentence. Their model is based on a sentiment lex- icon created by boosting with given negative and positive sentiment word seeds and the antonyms and synonyms relations in WordNet. (Kim and Hovy, 2006) studied supervised learning methods like ME for sentiment classification. While (Aue and Ga- mon, 2005) used semi-supervised learning based on Expectation Maximization (EM) of the NB classifier to learn from both labeled and unlabeled sentences.In terms of representation of documents, BoW could nicely reduce a piece of text with arbitrary length to a fixed length vector. In recent years, by exploiting the co-occurrence pattern of words, em- bedding model is used to gain lower-dimensional, compact and meaningful vectors for words or docu- ments (Le and Mikolov, 2014). Deep neural network approaches like convolutional neural network (CN- N) can also bring significant improvement to senti- ment classification task (Kim, 2014). However, deep learning (DL) methods are always computationally expensive. In this paper, we still focus on BoW and expect to get competitive results compared to non- DL methods.With the respect of sentiment weight extraction, some works have been done include the following: (Kaji and Kitsuregawa, 2007) did the work on tack- ling the problem of determining the semantic ori-entation (or polarity) of words; (Wiebe, 2000) pro- posed an approach to find subjective adjectives us- ing the results of word clustering according to their distributional similarity; (Qiu et al., 2011) proposed a double propagation algorithm to expand opinion words and extract target words. There are also some related works in sentiment lexicon extraction: (Jin and Ho, 2009) and (Li et al., 2010b) showed that supervised learning methods can achieve state-of- the-art results for lexicon extraction. In the domain- specific lexicon extraction, (Park et al., 2015) got significant improvement by using active learning method. (Potts and Schwarz, 2010) used logistic re- gression (LR) to get the terms’ weights correspond- ing to different ratings. (Pappas and Belis, 2014) fo- cused on the relevant weights of sentences in a given document for aspect rating prediction. In this paper, we will compare our model to classical models in sentiment classification and show the results of term extraction.3 Polarity Model of SentimentGiven a sentimental text of being positive or nega- tive, it is easy to see that the sentiment contributions of its consisting terms are different. Some terms like excellent, good occur more often in positive docu- ments, that implies high sentiment contributions and should be assigned to large sentiment weights. For some object nouns or action verbs like take, walk, very likely they appear equally in both positive or negative documents, therefore, they usually carry lit- tle sentiment. Such terms are neutral and should be assigned with small sentiment weights. In this sim- ple Polarity Model, we assume that the sentiment of a sentence is a function of the sentimental weights of its consisting terms. A term is consisting of one word in uni-gram model and two words in bi-gram model. Different terms in a sentence represent dif- ferent weights to some degrees. For example, given the following sentence from a movie review: The movie is good, I really like it can be divided into fol- lowing uni-gram terms:S = {the, movie, is, good, I, really, like, it} or the following bi-gram terms:S = {the movie, movie is, is good, good I, I really, really like, like it}
For the uni-gram case, the associated sentimen- tweighttotermti iswi for1 ≤ i ≤ 8. Thesen- timent score h of the above sentence is:{xj1,xj2,...,xji,...,xjN} denotes the jth docu- ment’s feature vector where xji represents the ith term’s feature value in the jth document. We can initialize w randomly and calculate sentiment score by using logistic function:1hj=1+exp(−wTxj) ; 1≤j≤M (4)where M is the total number of documents in the training corpus. In order to minimize the squared error:(∑8 ) h=f wixii=1(1) where xi is the feature value of a given term ti. It could be the frequency, binary value (appears in the particular document or not) or the TF-IDF value of the term. The TF-IDF value for a term is defined by:nij M xi=tfi∗idfi=∑ n log|j:t ∈d| (2)kkj ijwhere nij is the number of appearance of term ti in document dj and M is the number of documents in training corpus D = {d1, . . . , dM }. Therefore, Eq. (1) can be written in general case as the following:h = f(wT x) (3)Moreover, we define the sentiment polarity of a term xi according to its sentiment weight wi. ti is a positive term iff wi >> 0; ti is a negative term iff wi <<0;whenwi ≈0,itmeansti isaneutralterm which carries little or no sentimental information at all. Such neutral terms are very common in natural language, for example, nouns like car, house, verbs like jump, move but not the verbs like hate, like and enjoy. It is not easy to set thresholds among posi- tive, neutral and negative, because that is quite data dependent. We will test thresholding based on senti- ment weights in Section 4. Function f (·) is a nonlin- ear function to smooth the linear combination of the sentimental weights. We can use logistic function or neural networks.3.1 Sentiment Weight LearningIn this section, we are going to use logistic regres- sion (LR) and Gradient Descent algorithm (GD) to learn the sentiment weight based on given training data. We use w = {w1,w2,...,wN} to represen- t the weight vector where wi : i ∈ {1,2,...,N} is the sentimental weight of term ti, where T = {t1,t2,...,tN} is a set of terms in a corpus based on uni-gram or bi-gram. We use y ∈ {0, 1} to rep- resent the document’s sentiment label (0 for nega- tive and 1 for positive), and h to represent the sen- timent score of the given document. Vector xj =1 ∑M 2 j=1(hj − yj )2 (5)  E =we can update w given hi, yi and xi using the Gra-dient Descent algorithm:w:=w−α(hj −yj)xj (6)where α is the learning rate. We iterate the process until convergence and use the final w to predict the new unseen document’s score hˆ by Eq. (4). We then can predict the sentiment label by:{ˆy= 0 if h<0.5 (7)1 otherwiseIn addition, the computational complexity of LRis O(|V |), where |V | is the size of vocabulary. 3.2 Mixed-gram ModelIn this section, we will discuss some details of the sentiment term extraction based on the N-gram model. Previous work shows that bi-gram is generally better than uni-gram, since bi-gram has more semantic information of word order or word position (Wang and Manning, 2012). As we have seen before, terms like good movie or bad script often appear in pairs in reviews or comments. In uni-gram model, since the word movie appears together with good in positive examples, it is likely to be assigned sentimentally positive value. The neutral word movie may become problematic in classification for its wrongly assigned positive sentiment value. However, if we consider the same example in bi-gram model, the positive sentiment value will be assigned to good movie. The word comes after moive could be very random and it 
 Top 10 Positive Terms           Excellent Perfect Wonderful Amazing Great Favourite Best Brilliant Highly Superb0 50 100 150Sentiment Weight                         won’t be biased on certain special bi-gram terms starting with movie. Ideally, weights should be assigned to sentimentally segmented terms. For example, This is a good movie, actors are excellent! should be segmented to:This | is a| good movie| actors| are| excellent|!This may bring a new problem of sentimental segmentation and it is beyond the scope of this paper. In this paper, we simply use mixed-gram model that is a mixture of both uni-gram and bi-gram. Then, the mixed-gram terms of the above example are the following:This |is |a | good| movie| actors| are| excellent| This is| is a| a good| good movie| movie actors| actors are| are excellent|!4 Experimental Studies 4.1 DatasetsWe choose two benchmark datasets for sentimen- t classification in our experiments. The first is the IMDB data of online movie reviews (Maas et al., 2011), it contains 25000 reviews (12500 positive and 12500 negative) for training and 25000 (12500 posi- tive and 12500 negative) for test. The second dataset is the Product Review including DVD, electronics, books and kitchens; each of them contains 1000 pos- itive and 1000 negative reviews. In the following ex- periments, for the fair comparison to other models, we do not use any external resources like sentiment lexicons or WordNet. In addition, stop words are not removed during the text preprocessing period.4.2 Sentiment Term ExtractionBy using the logistic regression discussed above, we first test the model on the IMDB dataset. Sentiment weight for each term is estimated based on uni-gram and mixed-gram, respectively.Top 20 sentiment terms with associated weights based on the uni-gram model are listed in Table 1. And top 20 sentiment terms of mixed-gram are list- ed in Table 2. The associated weights of top 10 pos- itive, negative and neutral terms are shown in Fig- ure 1 to 3. Weights for positives terms are positive values and weights for negative terms are negativeFigure 1: Top 10 positive terms in the IMDB corpus. ex- cellent is the strongest positive term. Top 10 Negative Terms          Unfortunately Bad Worse Dull Terrible Poor Boring Waste Awful Worst                                          −200 −150 −100 −50 0Sentiment WeightFigure 2: Top 10 negative terms in the IMDB corpus. worst is the strongest negative term. Top 10 Neutral Terms            Shifting Angsty Realisation Cory Tactic Wallows Ungainly Virginity Minutiae Scuttle                                   −1 −0.5 0 0.5 1Sentiment Weight −3 x 10Figure 3: Top 10 neutral terms in the IMDB corpus.
 120001000080006000400020000Mixed−gram Sentiment Weight Distribution                                                    [−50,−5][−5,−1] [−1.0,−0.5][−0.5,−0.2][−0.2,0][0,0.2] [0.2,0.5][0.5,1.0][1,5] [5,50]Sentiment Weight IntervalsTable 1: Top 20 sentiment terms based on the uni-gram model.   Polarity Sentiment Terms (Uni-gram)      Positive     excellent, perfect, wonderful, amazing great, favourite, best, brilliant highly, superb, today, enjoyed loved, fun, bit, enjoyable beautiful, fantastic, job, definitely          Negative     worst, awful, waste, boring poor, terrible, dull, worsebad, unfortunately, annoying, horrible poorly, nothing, stupid, script ridiculous, lame, oh, disappointing        Table 2: Top 20 sentiment terms based on the mixed- gram model.Figure 4: Weight distribution of mixed-gram on the IMD- B dataset.Figure 5: Emotional word clouds of positive and negative reviews from the IMDB dataset.signing the emotional cloud, the fontsize of a term in emotional word-cloud is proportional to its senti- ment weight. For example, in Figure 5, we can see that love is strongly positive and in this review it has a higher feature value than excellent so we put love in the center of word-cloud and select the biggest fontsize for it.   Polarity Sentiment Terms (Mixed-gram)      Positive     great, the best, excellent, perfect wonderful, amazing, a bit, a great well worth, is a, a must, funmy favorite, today, very good, brilliant definitely worth, is great, very well, superb          Negative     the worst, bad, worst, awful boring, poor, no, terrible waste, nothing, waste of, at all worse, not even, dull, horrible poorly, stupid, annoying, lame        values. Weights for neutral terms are close to ze- ro. For 10 most neutral terms, the weight values are within [−0.001, 0.001]. The learned term weights have a big variance, which indicates that a small set of terms carrying strong sentiment than other terms. The weight distribution of mixed-gram can be seen from Figure 4.Most weights are within the range of [−5, 5]. On- ly a small number of terms are with strong sentiment weights and dominant in sentiment analysis such as great, excellent, bad, worst. Furthermore, sentiment weights can help to draw emotional word clouds of positives reviews and negative reviews. The color of term shows its sentiment; the deeper the color is, the stronger sentiment the term has. We use cold colors (blue, cyan) to represent negativity and warm colors (red, yellow and orange) to represent positivity. Two sample emotional clouds for IMDB positive reviews and negative reviews are shown in Figure 5. In de-Length of Intervals
Table 3: Given different percentages of terms with high- est TF-IDF values based on uni-gram, classification re- sults on the IMDB dataset show that some important terms contribute much to the sentimental polarity.Table 5: Influence of types of features on the IMDB dataset.  N-gram Model Feature Type  Test Error Uni-gram Uni-gram Uni-gram   TF-IDF TF Binary    11.19%12.25% 14.61%     Bi-gram Bi-gram Bi-gram   TF-IDF TF Binary    11.24% 12.35% 17.36%     Mixed-gram Mixed-gram Mixed-gram   TF-IDF TF Binary    9.70%10.88% 14.51%       Highest TF-IDF Terms Percentage (%)  Training Error   Test Error   100% 90% 80% 50% 20%     4.26% 6.36% 6.37% 6.56% 9.17%      11.19%11.26% 11.34% 13.13% 23.40%             Table 4: Influence of truncating low-weight terms in d- ifferent percentages base on mixed-gram on the IMDB dataset.4.3 Truncating based on Term Feature and Sentiment WeightIn this experiment, we conduct experiments by deleting some terms with low feature values or sen- timent weights to check the contributions of impor- tant terms to the sentiment of a sentence or docu- ment. After training, terms are sorted by TF-IDF values and different percentages of terms with high- est TF-IDF values are tested for sentiment classifica- tion. Training and test accuracy are shown in Table 3.As we can see from the results, after truncating 20% unimportant words in terms of TF-IDF values, the error rate increases only slightly from 11.19% to 11.34%. Even truncating 50% of unimportant terms, the accuracy can still remains around 13.13%. The results demonstrate the contribution of impor- tant terms to the sentimental polarity. This agrees with our common sense that only a small percentage of important words decide a sentence or document of being positive or negative. Most of words carries little or no sentimental information at all.Table 4 gives the error rate of classification bydeleting neutral terms. We can see from the table, it shows that a big percentage of neutral words have not contributed much to the sentiment of a documen- t. Even we only keep 10% of the highly weighted terms, the error rate is 13.88%, only 4.17% worse comparing to using all the terms. 100% is an ex- treme case that we can only make random guesses.4.4 N-gram Model for Sentiment ClassificationIn this section, we test the performance of vari- ous N -gram models (uni-gram, bi-gram and mixed- gram) with different types of feature values include TF-IDF, TF only and Binary. The experimental re- sults on the IMDB dataset are listed in Table 5. As we can see from the results, TF-IDF feature outper- forms other two types of feature in all N -gram mod- els. And mixed-gram performs best among these models and achieves 9.70% in test accuracy.Many researchers have reported their results on the IMDB dataset for fair comparisons. In partic- ular, one of the most significant improvement re- cently was the work of (Wang and Manning, 2012) in which they found that bigram features works the best and yields a considerable improvement of 2% in error rate. Another important contribution is (Dahl et al., 2012) in which they combine a Restricted Boltzmann Machines model with BoW. The best re- sult so far was reported by (Le and Mikolov, 2014) in which deep learning was used and it involves a big computing resources. The method we proposed (LR+mixed-gram) is the simplest one and with least computational time. By truncating about 5% low- weight terms and with learning rate α = 0.01, the  Percentage  ErrorPercentage  Error 10% 20% 30% 40% 50%      9.71% 9.76% 9.78% 9.83% 9.97%        60% 70% 80% 90% 100%  10.35% 10.77% 11.72% 13.88% 50.00%          
 Influence given Different Truncating Percentage0.90.850.80.750.70.650.60.550.50 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Truncating Percentage                                                                               booksdvd electronics kitchen                                          Table 6: Performance comparison of LR+mixed-gram to other approaches on the IMDB dataset. Our model ranks the 3rd in performance, but it is with least model com- plexity and computational time.  Sentiment Classification Model  Error BoW(bnc) (Maas et al., 2011)LDA (Maas et al., 2011) Full+Unlabeled+BoW (Maas et al., 2011)    12.20% 32.58% 11.11%     WRRBM (Dahl et al., 2012) WRRBM+BoW(bnc) (Dahl et al., 2012)   12.58% 10.77%   MNB-uni (Wang and Manning, 2012) MNB-bi (Wang and Manning, 2012) SVM-uni (Wang and Manning, 2012) SVM-bi (Wang and Manning, 2012) NBSVM-uni (Wang and Manning, 2012) NBSVM-bi (Wang and Manning, 2012) PV+Unlabled (Le and Mikolov, 2014)        16.45% 13.41% 13.05% 10.84% 11.71% 8.78% 7.42%             LR-mixed-gram (Our model)  9.67%     Table 7: Comparisons to baseline approaches on the Product Review dataset. Accuracy of other methods were reported in (Fattah, 2015).test accuracy can be improved up to 9.67% (bottom line of Table 6). It can be ranked among best models in efficiency and scalability.We also conduct experiments on the Product Re- view dataset. In our experiments, like what we have done to the IMDB dataset, we do not remove any stopwords or apply any stemming in preprocessing, while the baseline approaches in (Fattah, 2015) have used these techniques. We don’t handle the problem of orthographic mistakes, abbreviations, idiomatic expressions or ironic sentences either.From Table 7, we can find that our model also has comparable performance to the baseline approaches. Thought it performs slightly worse than SVM, this may be caused by the sparsity of data. And truncat- ing low-weight terms during prediction could bring an improvement on accuracy by 1-2%. Curves in Figure 6 shows the performance by truncating dif- ferent percentages of low-weight terms.Figure 6: Test accuracy with truncating different percent- ages of low-weight terms on the Product Review dataset.5 Conclusion and Future WorkIn this paper, we propose a model of using Logis- tic Regression with Gradient Descent algorithm for extracting sentiment terms and learning sentimen- t weights. We assume the sentiment of a sentence or documents is a function of sentiment weights of consisting terms. The extracted sentiment terms can be drawn as emotional clouds. We find that a small number of terms contribute much to the sentiment of a document, most terms are sort of neutral. Based on experimental studies on two benchmark detests, we find that terms with low feature value and low sen- timent weight make little contribution to sentiment classification. By deleting these unimportant terms, we can save significant computational time without losing much accuracy. In sentiment classification, we have tested our model based on different N -gram models and find that the mixed-gram model outper- forms both uni-gram and bi-gram models. Extensive experimental results show our proposed method can extract precise sentiment terms and achieve a high level accuracy in classification on given benchmark datasets.In future work, we intend to use different features and more complex function instead of logistic func- tion to investigate underlying relations between sen- timent of a sentence and its consisting terms. In dealing with sparse dataset, especially the cases like single-sentence short text, some priori information like training word-vector from deep learning couldCode will be released upon the acceptance of this paper.  Category  ANN SVM LR-mixed-gram Books DVD Electronics Kitchen     18.3 % 18.4 % 16.3 % 14.8 %       17.2 % 16.3 % 15.1 % 13.6 %    19.8 % 19.9 % 15.6 % 13.8 %      Test Accuracy
be used to deal with the problem. In addition, more syntax information should also be taken into consid- eration in our future work.AcknowledgementsThis work is supported by the National Science Foundation of China Nos. 61305047 and 61401012.ReferencesAnthony Aue and Michael Gamon. 2005. Customizing sentiment classifiers to new domains: a case study. In RANLP.Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of machine translation: Parameter estimation. In Com- putational Linguistics, 19(2) pp: 263-311.George E. Dahl, Ryan P. Adams, and Hugo Larochelle. 2012. Training Restricted Boltzmann Machines on word observations. arXiv:1202.5695.Mohamed. A. Fattah. 2015. New term weighting schemes with combination of multiple classifiers for sentiment analysis. In Neurocomputing, pp: 434-442.William A. Gale, Kenneth W. Church. 1993. A program for aligning sentences in bilingual corpora. In Compu- tational Linguistics, 19(1) pp: 75-102.Vasileios Hatzivassiloglou and Kathleen R. McKeown. 1997. Predicting the semantic orientation of adjec- tives, In Proceedings of ACL, pp: 174-181.Minqing Hu and Bing Liu. 2004. Mining and summariz- ing customer reviews. In Proceedings of SIGKDD, pp: 168-177Wei Jin and Hung H. Ho. 2009. A novel lexicalized HMM-based learning framework for web opinion min- ing. In Proceedings of ICML, pp: 465-472.Mahesh Joshi and Carolyn Penstein-Rose. 2009. Gen- eralizing dependency features for opinion mining. In Proceedings of ACL, pp: 313-316Nobuhiro Kaji and Masaru Kitsuregawa. 2007. Building lexicon for Sentiment Analysis from Massive Collec- tion of HTML Documents. In Proceedings of EMNLP, pp: 1075-1083.Nigam, Kamal, John Lafferty, and Andrew McCallum 1999. Using maximum entropy for text classification. In Proceedings of IJCAI, pp: 61-67.Yoon Kim. 2014. Convolutional neural networks for sentence classification. In Proceedings of EMNLP, pp: 1746-1751Soo-Min Kim and Eduard Hovy. 2006. Automatic iden- tification of pro and con reasons in online reviews. In Proceedings of ACL, pp: 483-490Quoc Le and Tomas Mikolov. 2014. Distribut- ed representations of sentences and documents. arX- iv:1405.4053.David D. Lewis 1998. Naive (Bayes) at forty: the inde- pendence assumption in information retrieval, In em Proceedings of LNCS, pp: 4-18.Fangtao Li, Minlie Huang and Xiaoyan Zhu. 2010b. Sentiment analysis with global topics and local depen- dency. In Proceedings of AAAI, pp: 1371-1376.Fangtao Li, Sinno J. Pan, Ou Jin, Qiang Yang and Xiaoy- an Zhu. 2012. Cross-Domain Co-Extraction of Senti- ment and Topic Lexicons. In Proceedings of ACL, pp: 410-419Chenghua Lin and Yulan He. 2009. Joint sentiment/topic model for sentiment analysis. In Proceedings of CIK- M, pp: 375-384.Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Pro- ceedings of ACL, pp: 142-150Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: sentiment classification us- ing machine learning techniques. In Proceedings of EMNLP, pp: 79-86Sungrae Park, Wonsung Lee, Il-Chul Moon. 2015. Ef- ficient extraction of domain specific sentiment lexicon with active learning. In Pattern Recognition Letters, p- p: 38-44Nikolaos Pappas, Andrei Popescu-Belis. 2014. Explain- ing the Stars: Weighted Multiple-Instance Learning for Aspect-Based Sentiment Analysis. In Proceedings of EMNLP, pp: 455-466Christopher Potts, Florian Schwarz. 2010. Affective ‘this’, In Linguistic Issues in Language Technology, p- p: 1-30Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2011. Opinion word expansion and target extraction through double propagation. In Proceedings of ACL, 37(1): pp: 9-27Sida Wang and Christopher D. Manning 2012. Baselines and bigrams: Simple, good sentiment and text classifi- cation. In Proceedings of ACL, pp: 90-94Janyce M. Wiebe. 2000. Learning subjective adjective from corpora. In Proceedings of AAAI, pp: 735-740. Rui Xia, Chengqing Zong, and Shoushan Li. 2011. En-semble of feature sets and classification algorithms forsentiment classification. Inf. Sci, pp: 1138-1152.Lun Yan and Yan Zhang. 2012. News Sentiment Analy- sis Based on Cross-Domain Sentiment Word Lists and Content Classifiers. Advanced Data Mining and Ap-plications. Springer Berlin Heidelberg. pp: 577-588.