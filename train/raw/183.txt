An Unsupervised Text Classification Method Based On LDAAbstracteasier to collect a large number of unmarked samples, however, access to a large number of marked samples is relatively difficult and labeled the samples collected usually spend a lot of time and energy. In fact, in the real world application problems, there are usually a lot of unmarked samples, but marked sample is relatively small, especially in text classification task, if we only use the tag samples, there will be a big problem hard to resolve.In this paper, we propose an improved unsupervised text classification algorithm based on LDA (Latent Dirichlet Allocation) which does not need a large number of documents labeled by humans. The experimental results on datasets prove the effectiveness of our approach.2 Related WorkBlei (2003) used LDA topics as features in text classification, but they use labeled documents while learning a classifier. sLDA (Blei and McAuliffe et al., 2007), DiscLDA (Lacoste-Julien et al., 2008) and MedLDA (Zhu et al., 2009) are few extensions of LDA which model both class labels and words in the documents. These models can be used for text classification, but they need expensive labeled documents.Nigam (2000) proposed semi-supervised approaches for document classification based on labeled and unlabeled datasets. McCallum and Nigam (1999) proposed a semi-supervised approach based on labeling of keywords. In keyword based approaches, finding right set of keywords is a challenge. However, these algorithms are sensitive to initial labeled documents and hyper-parameters of the algorithm.Liu (2004) proposed a weakly supervised classification method based on labeling words. It Text classification is one of the important application in the field of natural language processing, in which supervised classification algorithm requires a lot of manpower and time to annotation data. In this paper, we propose an unsupervised text classification algorithm on the  basis of LDA, which no longer need toask an annotator to assign one class label to every topic, but rather determine which class label the topic belongs to by computing the distances between this topic with all the classes. Besides, we maintain the word order and introduce the bigram grammar. Experimental results show that when this approach reduces manpower and material resources, we also obtain better effect. 1 IntroductionWith the development of science and technology, large amount of information in machine-readable form, people also more and more dependent on the information resources. The text information has been occupies an important position. How can we find the most useful information in such complex text information for us has always been the major problems that need to be solved in information processing. How to realize the classification of information, especially the effective classification of text information is an important branch of information processing research field. It promots the rapid development and extensive application of automatic text classification technology.In the traditional supervised text classification algorithm, it almost needs a large amount of labeled corpus. With the rapid development of data collection and storage technology, it much 
combines clustering and feature selection and then labels a set of representative words for each class through ranking the words in the unlabeled documents according to their importance. Druck (2008), using the generalized expectations criterion, puted forward a weakly supervised learning method based on feature tagged. An important limitation of these algorithms is coming up with a small set of words that should be presented to the annotators for labeling. Also a human annotator may discard or mislabel a polysemous word, which may affect the performance of a text classifier.An approach that is not demanding in terms of labeled documents is ClassifyLDA (Hingmire et al., 2013). In this approach, a topic model on a given set of unlabeled training documents is constructed using LDA, then an annotator assigns a class label to some topics based on their most probable words. These labeled topics are used to create a new topic model such that in the new model topics are better aligned to class labels. A class label is assigned to a test document on the basis of its most prominent topics.In this paper, we propose a new improved algorithm based on LDA which does not need labeled documents. In this algorithm, a topic model on a given set of unlabeled training documents is constructed using LDA and a set of artificial representative words (Hingmire et al., 2014) is created for each class label, then translate the most probable words of each topic and the artificial representative words of each class label into vectors. When calculating the distance between the topic and each class label, we first calculate the average distance between each probable word of this topic between every representative word of some class label, then the sum of all the average distance between each prominent word of the topic and all representative words of this class label is the distance betweenthe topic and the class label. After all the distance between the topic and all the class labels calculated, we assign the topic to the class label which is closest to it in distance. When assigning the topic to the class label, we do not need a human annotator to do this directly, it can avoid the negligence which may be made by the human annotator. And on the premise of keeping the word order in document, we introduce the bigram grammar. We use a random bigram status variable which denotes whether a bigram grammar could be formed between the current word and the previous one. If they could, the previous word will directly impact on choosing a topic for the current word. And if not, choosing a topic for the current word is independent.3 LDALDA is an unsupervised generative probabilistic model for collections of discrete data such as text documents. It is widely used to discover latent semantic structure of a document collection by modeling words in the documents. LDA models each of the documents as a mixture over some latent topics, each of which describes a multinomial distribution over a word vocabulary. The generative process of LDA can be described as follows:1. for each topic t, draw a distribution over words:Œ¶t ~Dirichlet(Œ≤)2. for each document d Œµ Da. draw a vector of topic proportions:Œ∏d ~Dirichlet(Œ±)b. for each word w at position n in d i. draw a topic assignment:zd ,n ~Multinomial(Œ∏d ) ii. draw a word:wd ,n ~ Multinomial(Œ¶z )Where, Œ¶t is the word probabilities for topic t, Œ∏d is the topic probability distribution, zd,n is 
topic assignment and wd,n is word assignment for nth word position in document d respectively. Œ± and Œ≤ are topic and word Dirichlet priors.The key problem in LDA is posterior inference. It involves the inference of the hidden topic structure given the observed documents. Direct and exact estimation of posterior inference is intractable. In this paper we estimate approximate posterior inference using collapsed Gibbs sampling (Griffiths and Steyvers et al., 2004). The Gibbs sampling equation used to update the assignmentofatopicttothewordwŒµWatthe position n in document d, conditioned on Œ± and Œ≤ is:‚àù(1)vŒµW (nv,t+Œ≤v)‚àí1Where ùêßùê∞,ùê≠ is the count of the word w assigned to the topic t, ùêßùê≠,ùêù is the count of the topic t assigned to words in the document d and W is the vocabulary of the corpus. We use a subscript d,¬¨n to denote the current token, ùê≥ùêù,ùêß is ignored in the Gibbs sampling update. After performing collapsed Gibbs sampling using equation 1, probability of the word w assigned to the topic t and the probability of the topic t assigned to the document d is estimated as:knowledge from the text. The words of a document are independent of each other, assigning a topic for the current word do not depend on the previous word and the current word do not affect assigning a topic for the next word. The generative process of each word is a independent.In this paper, the improved algorithm VB-LDA considers the word order, not a pure bag-of-words model. When assigning a topic for the word of a document, we introduce the bigram grammar. We introduce a bigram status variable x (X. Wang and A. McCallum et al., 2007) between two words which are adjacent. If x=1, they form a bigram grammar, if x=0, they do not. What it is different from LDA is that the production of every word of a document is not independent, it may depend on the topic probability distribution of the previous word or affect the generative process of next word. Therefor, the generation of a word is not only dependent on the topic probability distribution, but also the random Bernoulli distribution which decides the current word and the previous one whether to form a bigram grammar or a unigram grammar. The graphical model presentation of the model which is introduced the bigram grammar is shown in Figure 1. Where T is the number of topics in the documents, W is the number of unique words, Œ∏ is the multinomial distribution of topics in a document, zi is the topic associated with the ith token in a document, xi is the bigram status between the (i ‚àí 1)th token and ith token in a document, wi is the ith token in a document, Œ¶ is the multinomial unigram distribution of words. œÜ is the binomial (Bernoulli) distribution of bigram status variables x , œÉ is the multinomial bigram distribution of words, Œ±„ÄÅŒ≤„ÄÅŒ≥„ÄÅŒ¥ are the Dirichlet priors of Œ∏„ÄÅŒ¶„ÄÅœÜ„ÄÅœÉ respectively. The generative process of this model can be described as follows:1. draw Discrete distributions Œ¶z from a Dirichlet prior Œ≤ for each topic z:P zd,n = t zd,‚àín,wd,n = w,Œ±t,Œ≤w nw,t+Œ≤w‚àí1 √ó(nt,d +Œ±t ‚àí1) Œ¶w,t = nw,t+Œ≤wvŒµW (nv,t+Œ≤w)Œ∏ = nt,d+Œ±tt , d Ti = 1 ( n i , d + Œ± i )4 VB-LDA(2) (3)  In this paper, we introduce the word vector and the bigram grammar based on LDA, so the algorithm is called VB-LDA.4.1 Bigram grammarThe LDA does not take the word order into account, it is a typical bag-of-words model. Due to not considering the structural information inherent in the text, it could not help tap deep
Figure 1. the topic model with bigram grammarŒ¶ ~Dirichlet(Œ≤) z2. draw Bernoulli distributions œÜzw from a Beta prior Œ≥ for each topic z and each word w:œÜzw ~Beta(Œ≥)3. draw Discrete distributions œÉzw from aDirichlet prior Œ¥ for each topic z and each word w:œÉzw ~Dirichlet(Œ¥)4. for each document d: √óa. draw a Discrete distribution Œ∏d from a Dirichlet prior Œ±:ii‚àí1 i‚àí1 in z di w di + Œ≤ w d ‚àí 1 iWv = 1 n z di v + Œ≤ v ‚àí 1mzdwd wd +Œ¥wd ‚àí1 i i ‚àí 1 i iWv = 1 m z d w d + Œ¥ v ‚àí 1 i i‚àí1ii f x di = 0 ( 4 ) i f x di = 1 ( 5 )Œ∏d ~Dirichlet(Œ±)b. for each word wd in document d: ii. draw xid from Bernoulli œÜzd wd i‚àí1 i‚àí1xid~Bernoulli(œÜzd wd ) i‚àí1 i‚àí1ii. draw zid from Discrete Œ∏d : zid ~Dirichlet(Œ∏d ):where nzw represents how many times word w is assigned into topic z as a unigram, mzwv represents how many times word v is assigned to topic z as the 2th term of a bigram given the previous word w, pzwk denotes how many times the status variable x=k (0 or 1) given the previous word w and the previous word‚Äôs topic z, and qdz represents how many times a word is assigned to topic z in document d. Note all counts here do include the assignment of the token being visited. Simple manipulations give us the posterior estimates of Œ∏„ÄÅŒ¶„ÄÅœÜ and œÉ as follows:Œ∏d = qdz+Œ±z (6) Œ¶ = nzw+Œ≤w (7) z Tt = 1 ( q d t + Œ± t ) z w Wv = 1 ( n z v + Œ≤ w )iii.draw wid from xid =1:DiscreteœÉz d w di i‚àí1ifwid~Multinomial(œÉzdwd ) i i‚àí1wid~Multinomial(Œ¶zdi )We use Gibbs sampling to conduct approximateinference in this paper. To reduce the uncertaintyintroduced by Œ∏„ÄÅŒ¶„ÄÅœÜ and œÉ, we could integratethem out with no trouble because of the conjugateprior setting in our model. Starting from the jointdistribution P(w,z,x|Œ±„ÄÅŒ≤„ÄÅŒ≥„ÄÅŒ¥), we can work outthe conditional probabilities P( zd , xd | zd , i i ‚àíixd ,w, Œ±„ÄÅŒ≤„ÄÅŒ≥„ÄÅŒ¥) conveniently using Bayes rule, ‚àíiwhere zd denotes the topic assignments for all ‚àíiword tokens except word wd , and xd represents i ‚àíithe bigram status for all tokens except word wid. During Gibbs sampling, we draw the topic assignment zid and the bigram status xid iteratively for each word token wid according to the following conditional probability distribution:p(zd,xd|zd ,xd ,w,Œ±„ÄÅŒ≤„ÄÅŒ≥„ÄÅŒ¥) i i ‚àíi ‚àíi‚àù (Œ≥xd +pzd wd x ‚àí1)(Œ±zdi +qdzd ‚àí1)     else draw wid from Discrete Œ¶zdi :
œÜ = Œ≥k+pzwk zwk 1k=0(Œ≥k+pzwk )Œ¥ = Œ¥v+mzwv zwv Wv=1(Œ¥v+mzwv )(8) (9)class label c.dt,c = Ri=1di,p (R is the number of theprominent words of a topic)Then we can calculate the distance of the topic tand each class label, choosing one which is closest to the topic t for it. In this paper, the algorithm can be described as follows:Input: the Dirichlet priors Œ±„ÄÅŒ≤„ÄÅŒ≥„ÄÅŒ¥, the number of topics T, the document corpus D and the max iteration M  4.2 Assign topic‚Äô class labelIn the LDA model, after performing collapsed Gibbs sampling, we can get the probable words of each topic by the multinomial distribution of the topic over the word vocabulary. Then the human annotator can assign the class label for each topic with its probable words. While in this paper, we do not need the human annotator to do this, because the human annotator may make some carelessness which may affect the performance of the algorithm. We first create a set of artificial representative words for each class label, then get the prominent words through performing collapsed Gibbs sampling. And we use the word2vec (Tomas Mikolov et al., 2013) to translate the words in the documents into word vectors, then we find out the word vectors of the artificial representative words for the class labels and the prominent words for the topics respectively. We introduce Vit to represent the word vector of the ith prominent word of the topic t and Vjc to represent the word vector of the jth representative word of the class label c. We use di,j to denote the geometric distance between the word vector Vit and Vjc, and dt,c to denote the distance between the topic t and the class label c. Their distance is calculated as follows:1. calculate the average distance di,p between the ith prominent word of the topic t and each representative word of the class label c:di,p = Nj=1 di,j N (N is the number of the representative words of a class label)2. the sum of the average distance between each prominent word of the topic t and the class label c is the distance dt,c between the topic t and the1. 2.3. 4. 5. 6.7.8. 9.10.11. 12. 13.14.15. 16. 17.18. 19.20. Initialize count variables in Equation 4 and 5 to 0;Initialize the topic assignments for all the words in the corpus Dfor iter from 1to M doforeach document d in the corpus D doforeach word i in the document d do Exclude word i and its assigned topicfrom the count variables;calculate the xi for the word i and theword (i-1) using Equation 8; if (xi==1) thensample new topic for word i using Equation 5;update qdz , pzw 1 , mzwv using the new topic z for word i;endif (xi==0) thensample new topic for word i using Equation 4;update qdz , pzw 0 , nzw using the new topic z for word i;end endUpdate the posterior estimates for Œ∏dz using Equation 6;endUpdate the posterior estimates for Œ¶zw œÜzwk and Œ¥zwv using Equation 7,8 and 9;end
21. Get each document‚Äô mixture distribution Œ∏ over T latent topics and each topic‚Äô prominent words, then we create a set of artificial representative words for each class label.22. Translate the prominent words of a topic and the representative words of a class label into word vectors respectively using word2vec (Tomas Mikolov et al., 2013).23. Assign a class label for the topic t ( t ‚àà [1,T] ) by calculating the distance between the topic t and each class label.24. Corresponding to the topic proportions Œ∏d in document d, we can find out the topic z which has the largest proportion in document, so the class label c which is assigned to the topic z is what the document d belongs to.2. SRAA: Simulated/Real/Aviation/Auto UseNet data 2: This dataset contains 73,218 UseNet articles from four discussion groups, for simulated auto racing (sim_auto), simulated aviation (sim_aviation), real autos (real_auto), real aviation (real_aviation). Following are the three classification tasks associated with this dataset.1. sim_auto vs sim_aviation vs rea_lauto vs real_aviation2. auto (sim_auto + real_auto) vs aviation(sim_aviation + real_aviation)3. simulated (sim_auto + sim_aviation) vs real(real_auto + real_aviation)3. WebKB: The WebKB dataset contains 8145 web pages gathered from university computer science departments. The task is to classify the webpages as student, course, faculty or project.We randomly split SRAA and WebKB datasets such that 80% is used as training data and remaining 20% is used as test data.We preprocess these datasets by removing HTML tags and stop-words. For various subsets of the 20Newsgroups and WebKB datasets discussed above, we choose number of topics as twice the number of classes. For SRAA dataset we infer 8 topics on the training dataset and label these 8 topics for all the three classification tasks. The Dirichlet priorsŒ±„ÄÅŒ≤„ÄÅŒ≥„ÄÅŒ¥ were chosen to be 50/T, 0.01, 0.1 and 0.01 respectively. The number of the topic‚Äô prominent words and the class‚Äô representative words were all set to be 20. We can obtain the representative words of a class label as follows: setting the number of topics to be 1, after performing collapsed Gibbs sampling, the topic‚Äô prominent words are this class‚Äô representative words.5.2 Results and analysisTable 1 shows the experimental results. We can observe that, VB-LDA almost performs better 5.Experimental EvaluationWe determine the effectiveness of our algorithm in relation to ClassifyLDA (Hingmire et al., 2013) algorithm on the same experimental datasets. We evaluate and compare our text classification algorithm by computing Macro averaged F1. As the inference of LDA is approximate, we repeat all the experiments for each dataset ten times and report average Macro-F1. We also learn supervised SVM classifier for each dataset using topics as features and report average Macro-F1.5.1 DatasetsWe evaluate the effectiveness of VB-LDA and ClassifyLDA and SVM on following three real world text classification datasets.1. 20Newsgroup: This dataset contains messages across twenty newsgroups. In our experiments, we use bydate version of the 20Newsgroup dataset. This version of the dataset is divided into training (60%) and test (40%) datasets which are grouped into 6 major categories. We construct classifiers on training datasets and evaluate them on test datasets.
     Dataset         Text classification(Macro-F1)    # Topics   ClassifyLDA   VB-LDA     SVM    20Newsgroups   comp vs politics    4 0.962  0.985    0.983   religion vs sports    4 0.899  0.903    0.907   politics vs religion    4 0.875  0.889    0.891   comp vs religion vs sports    6 0.908  0.935    0.939   comp vs religion vs politics    6 0.884  0.929    0.944   comp vs religion vs sports vs politics      8   0.835   0.888     0.912    SRAA   sim_auto vs sim_aviation vs real_auto vs real_aviation    8 0.747  0.768    0.813   auto vs aviation      8   0.918   0.930     0.933   simulated vs real    8  0.916  0.921   0.928    WebKB   WebKB     8 0.653   0.687    0.725Table 1: Experimental results of document classification on various datasetsFor the religion vs sports dataset and the simulated vs real dataset VB-LDA and ClassifyLDA almost have the same performance. What is more, the increase of Macro-F1 from ClassifyLDA to VB-LDA for comp vs religion vs politics is more than for comp vs politics, religion vs sports and politics vs religion, and the increase of Macro-F1 from ClassifyLDA to VB-LDA for comp vs religion vs sports vs politics is more than for comp vs religion vs politics and comp vs religion vs sports. Therefor we can see that our algorithm performs better when the classes is more. We can also observe that, performance of VB-LDA is close to supervised SVM in the 10 subsets. That is to say, VB-LDA can perform well without labeled documents.Table 2 shows the topic‚Äô most prominent words and the class which the topic belongs to in thepolitics vs religion subset. We can observe that, a human annotator is very hard to assign the class politics or religion to the topic 1, but in this paper, we can accurately calculate the distance between the topic 1 and the class politics and religion which are 3.49 and 3.47, so the class religion is assigned for the topic 1.Table 3 shows the distance between comp, politics, religion and sports. We can observe that, the distance between comp and politics is clearly longer than religion and sports, politics and religion, so it is relatively easy to distinguish between class comp and politics and their Macro-F1 is higher than the other two susets. We also observe that the distance between religion and sports is longer than politics and religion and the Macro-F1 in religion vs sports subset is a little higher than the other subset.
     ID Most prominent words in the topic    Class(politics / religion)     0 people gun know government president file guns fire state weapons    politics     1    people know make evidence argument well things question wrong moral     religion    2   god jesus christian bible people church christians time life know   religion    3 israel turkish people armenian war israeli armenians government turks turkey   politicsTable 2: Topic assigning in the politics vs religion subset   class     distance    comp   politics      5.72     religionsports  4.83    politics    religion      4.38   Table 3: the distance between each class6. ConclusionsIn this paper we propose a novel text classification algorithm based on LDA. In this algorithm, when assigning the class label to the topic, without a human annotator to do this, we use the tool word2vec to translate the topic‚Äô prominent words and the class‚Äô representative words into vectors, then calculate the distance between topic t and each of class label to find out the class label which the topic t belongs to. We also keep the word order in a document and introduce the bigram grammar which needs a bigram status variable x to denote whether the two words adjacent to be a bigram or not. This algorithm reduces the need to label a large collection of documents. The results in experiments shows that the approach can yield performance comparable to entirely supervised method SVM. In this paper, the datasets in our experiments is English, in the future, we will do some research in Chinese datasets.ReferencesDavid M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet Allocation. The Journal of Machine Learning Research, 3:993‚Äì1022, March.David M. Blei and Jon D. McAuliffe. 2007. Supervised Topic Models. In NIPS.Simon Lacoste-Julien, Fei Sha, and Michael I. Jordan. 2008. DiscLDA: Discriminative Learning for Dimensionality Reduction and Classification. In NIPS.Jun Zhu, Amr Ahmed, and Eric P. Xing. 2009. MedLDA: Maximum Margin Supervised Topic Models for Regression and Classification. In ICML, pages 1257‚Äì1264.Kamal Nigam, Andrew Kachites McCallum, Sebastian Thrun, and Tom Mitchell. 2000. Text Classification from Labeled and Unlabeled Documents using EM. Machine Learning-Special issue on information retrieval, 39(2-3), May-June.A. Mccallum and K. Nigam. Text classi_cation by bootstrapping with keywords, EM and shrinkage. In ACL-99 Workshop for Unsupervised Learning in Natural Language
Processing, pages 52-58, 1999.Bing Liu, Xiaoli Li, Wee Sun Lee, and Philip S. Yu. 2004. Text Classification by Labeling Words. In Proceedings of the 19th national conference on Artifical intelligence, pages 425‚Äì430.Gregory Druck, Gideon Mann, and Andrew McCallum. 2008. Learning from Labeled Features using Generalized Expectation criteria. In SIGIR, pages 595‚Äì602.Swapnil Hingmire, Sandeep Chougule, Girish K. Palshikar, and Sutanu Chakraborti. 2013. Document Classification by Topic Labeling. In SIGIR, pages 877‚Äì880.Swapnil Hingmire,Sutanu Chakraborti.Sprinkling Topics for Weakly Supervised Text Classification[J].In ACL,2014: 55-66.T. L. Gri_ths and M. Steyvers. Finding scientic topics. PNAS, 101(suppl. 1):5228-5235, April 2004.X. Wang, A. McCallum, and X. Wei. Topical N-Grams: Phrase and topic discovery, with an application to Information Retrieval. In Proc. of ICDM, pages 697 ‚Äì702, 2007.Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean. Efficient Estimation of Word Representations in Vector Space.arXiv:1301.3781, 2013.