Well-Formed Dependency to String translation with BTG GrammarAbstractThis paper proposes a well-formed depen- dency to string translation model with BTG grammar. By enabling the usage of well- formed sub-structures and allowing flexible reordering of them, our approach is effective to relieve the problems of parsing error and flatness in dependency structure. To utilize the well-formed dependency rules during decod- ing, we adapt the tree traversal decoding algo- rithm into a bottom-up CKY algorithm. And a lexicalized reordering model is used to en- courage the proper combination of two neigh- bouring blocks. Experiment results demon- strate that our approach can effectively im- prove the performance by more than 2 BLEU score over the baseline.1 IntroductionDue to the merits of holding shallow semantic infor- mation and cross-lingual consistency (Fox, 2002), dependency grammar has attracted much attention in the field of machine translation (Lin, 2004; Quirk et al., 2005; Ding and Palmer, 2005; Shen et al., 2008; Xie et al., 2011).The dependency-to-string model (Xie et al., 2011) falls into the paradigm of ”translation after under- standing”, which tries to understand the structure and meaning of source text. However, there are two typical problems for this approach. One is that the model is prone to be affected by parsing errors. Dependency-to-string model adopts a unique source side tree structure as fixed input and constructs the output by converting each sub-structure into targetside. If there is some errors in the source depen- dency tree, for example, a prepositional subtree is attached to a wrong head, the model can hardly re- cover the error. Figure 1(a) shows another parsing error, in the correct parsing result, ”与北韩...国家” should forms a subtree with ”国家” as the head, and then this subtree is dominant by ”之一”.The other problem is that dependency structure is too flat for the translation task. Since dependency- to-string model requires a head and all its depen- dents to be translated as a whole, the flatness of the structure will make rules difficult to be matched dur- ing decoding. Furthermore, it will also lower the ro- bustness of translation rules, since many giant and low-frequency rules will be extracted. Figure 1(b) shows an example of the flat structure. The head word ”提供” has five dependents in the structure. If no rule can be matched, only the glue rule can be applied. As a result, the prepositional subtree ”为 了...义务” cannot be correctly reordered to the end in the target side.Existing solutions for the above problems include (Meng et al., 2013; Xie et al., 2014). The form- mer incorporates phrasal nodes of constituency trees in the source side of translation rules, and the lat- ter modifies translation rules during decoding to al- low the usage of phrases which are compatible with well-formed structures. Since these two approaches still adopt head-dependent structure as the backbone of the translation rule, the freedom of generating translation candidates is limited.On the contrary, we propose to use BTG grammar to combine the translations of two adjacent well- formed structures. To incorporate the BTG rules
之一有 少数国家与北韩有邦交的美国 不 会提供为了 报酬为了让北韩履行义务 任何报酬       (a)Figure 1: Examples of parsing error (a) and flatness (b)into the model, we adapt the tree traversal decoding algorithm into a bottom-up CKY algorithm. Large scale experiments show that our approach can im- prove the performance by more than 2 BLEU score over the baseline, and it is also superior to the two approaches mentioned above.2 BackgroundWe briefly review the dependency to string model and the BTG grammar in this section, which are the bases of our proposed model.2.1 Dependency-to-String ModelThe dependency-to-string model proposed by (Xie et al., 2011) translates a source dependency tree by applying head-dependents translation rule at each head node in a recursive way. A head-dependents translation rule consists of a head-dependents frag- ment in the source side and its translation correspon- dence in the target side. The rule r1 in Figure 2 is an example of their translation rule. This rule speci- fies the translation of the head node ”提供” and leaf nodes ”布什”, and also the reordering relation of the non-terminal nodes, including the internal node ”为” and the generalized internal node ”优惠”. The word or POS tag at each non-terminal node in the rule describes its matching condition. For example, X2:NN in r1 means the second non-terminal must be a noun while matching this rule. In principle, all nodes, i.e., head, internal and leaf nodes in the de- pendency tree, can be generalized to their POS tags (or other categories) to relieve data sparsity.By including a head and all its dependents into one rule, the dependency-to-string model is good at long distance reordering. However, this structure is not robust enough due to parsing errors and flatness.2.2 BTG Grammar(b)Bracketing transduction grammar (BTG) (Wu, 1997) is a special case of synchronous context free grammar. There are only two types of rules in this grammar:X → [X1,X2] |X →< X1,X2 > X → x/yThe first type of rule is used to merge the trans- lations of two neighbouring blocks X1 and X2 with monotone or swap order, and the second type of rule is used to translate source phrase x in to target phrase y. Due to its simplicy and effectiveness of modeling bilingual correspondence, BTG grammar is widely used translation modeling (Xiong et al., 2006; Li et al., 2013), word alignment (Zhang and Gildea, 2005; Haghighi et al., 2009; Pauls et al., 2010), translation system combination (Karakos et al., 2008), etc.3 Well-Formed Dependency to String ModelIn this section, we describe our well-formed depen- dency to string model with BTG grammar, and ex- plain how it relieves the problems of parsing error and flatness.3.1 Modified Well-formed structureSimilar to (Shen et al., 2008), we define two kinds of well-formed dependency structures, i.e., fixed struc- ture and floating structure. Fixed structure consists of the heads of a sequence of sibling trees and the common head of these trees; and floating structure consists of the heads of a sequence of sibling trees without their common head. The difference between
 提供布什 为 优惠为民众 减税优惠Bush provides tax relief to the pepoler1 : 布什 X1:为 提供 X2:NN ||| Bush provides X2 X1 r2 : X1:为 提供 X2:NN ||| provides X2 X1Head node in the rules are marked with bold face. NN is the POS tag of the source word ”优惠”.Figure 2: examples of dependency to string rule (r1) and well-formed dependency to string rule (r2)our definition and that in Shen et al., 2008 is that we only include the heads of subtrees in our structure, while they include the whole subtrees. The shad- owed part with red box in Figure 1(b) is an example of fixed structure, which consists of a head ”提供” and the heads of two continuous sibling trees ”为 了” and ”报酬”. And the shadowed part with green box in Figure 1(b) is an example of floating struc- ture, which consists of the heads of three continuous sibling trees ”美国”, ”不” and ”会”.Given a sentence w1w2...wn, let di denote the head index of wi, our fixed and floating structure can be formally defined as follows,Definition 1A fixed structure fh,C with head h and children C, where h ∈ [1, n] and C ⊆ {1, ..., n}, is a two-level tree fragment which satisfies the following condi- tions:– ∀k ∈ C, dk = h– ∀min(C)≤k≤max(C), dk ̸=hDefinition 2 A floating structure fC with children C , where C ⊆ {1, ..., n}, is a one level tree fragment which satisfies the following conditions:– ∃h, ∀k∈C, dk =h– ∀min(C)≤k≤max(C), dk ̸=h3.2 Well-Formed Dependency-to-String RuleOur well-formed dependency to string translation rule consists of a well-formed dependency structure in the source side and its translation correspondence in the target side. This definition extends the rule proposed in (Xie et al., 2011) to cover all well- formed dependency structures in the source side, rather than using complete head-dependents struc- tures only. The rule r2 is an examples of our trans- lation rule. Compared with r1, this rule does not contain ”布什” in the source side (and its translation in the target side). Since it contains less context, this rule is more flexible to be applied during decoding. For example, if ”布什” is replaced with a pronoun ”他” in a testing sentence, this rule can still be ap- plied. However, r1 cannot be applied in this case even if it is generalized, since the POS tag of ”他” does not match that of ”布什”.Our translation rules can be extracted from aligned dependency tree and string pair by travers- ing the tree and enumerating the well-formed struc- ture at each node. Following previous work(Koehn et al., 2003; Galley et al., 2004; Chiang, 2005), we impose alignment constraint for rule extraction. The intuition is that words in the one side (source/target) cannot be aligned to words outside the other side, and the word alignment within non-terminals also need to satisfy this constraint.Formally, for a non-terminal node n, we define node span nsp(n) as the closure of the indexes of those words that n is alinged to, and sub-tree span ssp(n) as the closure of node spans of all the nodes in the subtree rooted with n. These two spans are set to φ for terminals. In addition, we use Ns to denote the set of all the terminal indexes in the source side, and Nt to denote the set of all the terminal indexes in the target side. Function a(·) is used to get the indexes of the aligned words for a give word. Then the alignment constraint can be described as follows,– ∀k ∈ Ns, a(k) ∈ Nt– ∀k ∈ Nt, a(k) ∈ Ns– nsp(head) ∩n∈children ssp(n) = ΦA minor difference in our constraint with (Xie et al., 2011) is that we allow the alignment of terminals to be overlaped. For example, in Figure 1(b), if the  
two terminals ”不” and ”会” align to a single target word ”won’t”, we consider the alignment constraint is satisfied, while they consider it as invalid.3.3 Apply Dependency to String Rules with BTG RulesWe use the examples in Figure 1 to illustrate how the well-formed dependency to string rules together with BTG rules can be used to overcome the prob- lems of parsing error and flatness. A plausible derivation for the example in Figure 1(a) is shown in Figure 3, in which the subtree ”与...的”, the float- ing structure ”少数国家” and the head node is trans- lated first, then the translations of the first two parts can be combined with a BTG rule of swap order. The final translation can be achieved by applying another BTG rule of swap order to the translation just obtained and the translation of ”之一”. Note that this is not the only derivation that can lead to a correct translation. We can also combine the trans- lation of floating structure ”少数国家” and the head node first, then combine with the translation of the first subtree. Similarly, for the example in Figure 1(b), we can first translate the floating structure ”美 国不会” and the fixed structure ”为了...提供...报 酬”, then combine them with an BTG rule of mono- tone order to produce the final translation.4 Decoding 4.1 ModelWe use the standard log-linear model (Och and Ney, 2002) to score the translation hypothesis during de- coding. For a specific derivation d that converts a source dependency tree T into a target string s, the score of d will be,P (d) ∝   φi(d)λi iwhere φi are features defined on derivations and λi are corresponding weight for each feature. The features adopted in this paper include bidirec- tional translation probabilities, bidirectional lexical weights, language model, rule penalty, word penalty and reordering probability for BTG rules.For the last feature, we use a maximum entropy model to estimation the probability and the same之一r1,r2,r3one ofa few count.r4r4one of    有 与北韩有邦交的    having dep. rel. with north korea   a few count. having dep. rel. with north korea   one of a few count. having dep. rel. with north korea r1: 之一 ||| one ofr2: 与北韩有邦交的 ||| having dep. rel. with north korea r3: 少数国家 ||| a few countriesr4:X1X2 ||| X2X1Figure 3: a plausible derivation with well-formed de- pendency to string and BTG grammar for the exam- ple in Figure 1(a)features in (Xiong et al., 2006) are adopted, includ- ing beginning and ending words in the two blocks to be reordered, from both the source and target side. So there are eight activated features in total for each instance.4.2 Decoding AlgorithmThe decoding algorithm is described in Algorithm 1. The algorithm begins by translating each word in the sentence, then proceed to translate larger spans少数 国家
 Algorithm 1: CKY decoding algorithm with well formed dependency to string rules Input: Source dependency tree T with N wordsOutput: Target translation for span:1 → N dofor start:1 → N+1-span dogenerate initial candidates with well formed dependency to string rule; for span lhs:1 → span-1 doif {span lhs,span rhs,span} ⊆ well-formed structure then generate initial candidates with BTG rules;end endgenerate KBEST candidates with cube pruning;        end endreturn the top candidate over the whole sentencein an bottom up manner. When translating a span compatible with a well-formed structure, there are two ways to generate translation candidates. One is based on fixed or floating rules which covers the whole span, and the other is combining the transla- tion candidates in two sub-spans with BTG rule of monotone or swap order. The two sub-spans also need to be compatible with well-formed structure. The cube prunning algorithm (Chiang, 2007; Huang and Chiang, 2007) is used to expand the initial can- didates until Kbest candidates have been generated. Finally, the top candidate over the whole sentence will be returned as output.5 ExperimentsWe evaluate the performance of our model on Chi- nese to English translation. And we re-implement the dependency to string model for performance comparison.5.1 Data preparationTwo sets of training data are adopted in our exper- iments. The smaller one consists of 270k sentence pairs, and the larger one consists of 2.1M sentence pairs. All the training data comes from the LDC corpus1. And we use NIST 02 test set as our de- velopment set, NIST 03 and 04 test set as our test1 Including LDC2000T50, LDC2002E18, LDC2003E07, LDC2003E14, LDC200407, LDC2005T06, LDC2002L27, LDC2005T10 and LDC 2005T34.as output;set. The case insensitive NIST BLEU-4 metric (Pa- pineni et al., 2002) is adopted for evaluation. We use the SRILM toolkit to train a 5-gram language model with Kneser-Ney smoothing on the Xinhua portion of the Gigaword corpus.The source side of the training and dev/test set are segmented with our in house segmentation tool (Wang et al., 2010).And they are parsed with Stan- ford Parser (De Marneffe et al., 2006), which also generates POS tag for each word. The dependency relations on edges are not used in this work.Word alignments are obtained with our in house tool (Wang and Zong, 2013), which takes depen- dency cohesion constraints into consideration while doing word alignments. And we use the MaxEnt toolkit2 to to estimate the context sensitive reorder- ing probability for BTG rules. The weights of the features are tuned with MERT (Och, 2003) to maxi- mize the BLEU score on the development set.5.2 ResultsThe strength of our model lies in two aspects. First, our translation unit is more fine-grained than that in the original dependency to tree model, which en- ables the translation of many linguistically plausible phrases; second, we allow flexible reorderings for adjacent blocks under the guide of context informa- tion. To check whether these two points hold, two sets of experiments are conducted in line. Initially,2https://github.com/lzhang10/maxent   
 System 02 (dev) 03 dep2str 33.50 31.92 wf-d2s 35.03 33.31 (mono)wf-d2s 35.86 34.0404 Average 32.59 32.67 34.50 34.2835.20 35.036 Related WorkThe work that is most similar to ours is (Xie et al., 2014). However, there are several significant dif- ferences between these two work. First They incor- porate well-formed dependency rules during decod- ing by modify the matched dependency rules ”on the fly”. For example, assume there is a matched rule ”X1:NR X2:AD X3:VV X1:为了提供 X2:报 酬||| X1 X2 X3 provide X5 X4” for the head- dependents structure in Figure 1 (b). in order to use the phrase ”美国不会||| us won’t” during decoding, they will compress the three nodes into one pseudo node ”NR AD VV”. Then the above rule will be- come ”X1:NR AD VV X2:为了∗ 提供 X3:报酬||| X1 provide X3 X2”. This new rule will inherit the translation probabilities from the original rule. In the case that there is no matched rule or the probability estimation is unreliable due to sparsity, this method won’t work well. Another difference is that they only use phrasal rules corresponding to well formed dependency structures, while we allow variables to be contained in the well-formed dependency rules.The two problems of parsing error and flatness also exist in constituency tree . In order to make full use of the sub-structures, there have been a lot of work, including tree sequence to string transla- tion (Liu et al., 2007), tree binarization (Zhang et al., 2006), forest-based translation (Mi et al., 2008) and fuzzy rule matching (Zhang et al., 2011).7 Conclusion and Future WorkIn this work, we propose a well-formed dependency to string model to address the problems of parsing error and flatness. By introducing translation rules corresponding to well-formed sub-structures, we are able to learn more reliable translation equivalents. During decoding, we propose to use BTG grammar with lexicalized reordering to combine translations of two neighbouring well-formed structures, which is more flexible than previous work. Experiment re- sults demonstrate that our model can significantly improve translation performance.Although our model is more flexible to generate translation candidates, it also brings more challenges to model translation quality. In the future, we will explore more powerful features to better score the translation candidates.  Table 1: Effects of applying well-formed depen- dency to string rules and allowing flexible reorder- ing. The system wf-d2s (mono) denotes our well- formed dependency to string model with monotone reordering, and wf-d2s denotes our model with flex- ible reordering of two directions.     System dep2str wf-d2s02 (dev) 35.24 37.07∗03 04 34.45 34.50 36.38∗ 37.01∗Average 34.73 36.82  Table 2: Experiment results with small and large training data. The ”*” denotes that the results are significantly better than the baseline (dep2str) sys- tem (p<0.01).we only allow BTG rule with monotone order, i.e. translation of each well-formed structure are con- catenated sequentially, which is equivalent to glue rule. Then BTG rules with both orders are enabled, with context sensitive reordering module. We con- duct the experiments with the small training data set. The results are shown in Table 1. Compared with de- pendency to string rules, applying well-formed de- pendency to string rules significantly improves the performance by more than 1.5 BLEU score on av- erage. If flexible reordering is further allowed, ad- ditional improvement of 0.7 BLEU score can be achieved.Table 2 shows the performance of our model with large training set. Experiment results show that our model keeps its edge even with large training data. On average, more than 2 point in BLEU score are gained over the baseline. This improvement is much larger than (Meng et al., 2013) and (Xie et al., 2014). Both of them report improvement of about 0.9 point in BLEU score over the baseline on their dataset.
AcknowledgmentsThis research work was funded by the Natural Science Foundation of China under Grant No. 61402478. The authors would like to thank Keh-Yih Su for insightful discussions.ReferencesDavid Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43rd Annual Meeting of the Association for Com- putational Linguistics (ACL’05), pages 263–270, Ann Arbor, Michigan, June. Association for Computational Linguistics.David Chiang. 2007. Hierarchical phrase-based transla- tion. computational linguistics, 33(2):201–228.Marie-Catherine De Marneffe, Bill MacCartney, and Christopher Manning. 2006. Generating typed depen- dency parses from phrase structure parses. In Proceed- ings of LREC, volume 6, pages 449–454.Yuan Ding and Martha Palmer. 2005. Machine trans- lation using probabilistic synchronous dependency in- sertion grammars. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguis- tics (ACL’05), pages 541–548, Ann Arbor, Michigan, June. Association for Computational Linguistics.Heidi Fox. 2002. Phrasal cohesion and statistical ma- chine translation. In Proceedings of the 2002 Con- ference on Empirical Methods in Natural Language Processing, pages 304–3111. Association for Compu- tational Linguistics, July.Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Daniel Marcu Susan Dumais and Salim Roukos, ed- itors, HLT-NAACL 2004: Main Proceedings, pages 273–280, Boston, Massachusetts, USA, May 2 - May 7. Association for Computational Linguistics.Aria Haghighi, John Blitzer, John DeNero, and Dan Klein. 2009. Better word alignments with supervised itg models. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th Inter- national Joint Conference on Natural Language Pro- cessing of the AFNLP, pages 923–931, Suntec, Singa- pore, August. Association for Computational Linguis- tics.Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In Proceedings of the 45th Annual Meeting of the Asso- ciation of Computational Linguistics, pages 144–151, Prague, Czech Republic, June. Association for Com- putational Linguistics.Damianos Karakos, Jason Eisner, Sanjeev Khudanpur, and Markus Dreyer. 2008. Machine translation sys- tem combination using itg-based alignments. In Pro- ceedings of ACL-08: HLT, Short Papers, pages 81–84, Columbus, Ohio, June. Association for Computational Linguistics.Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceed- ings of the 2003 Conference of the North American Chapter of the Association for Computational Linguis- tics on Human Language Technology-Volume 1, pages 48–54. Association for Computational Linguistics.Peng Li, Yang Liu, and Maosong Sun. 2013. Recursive autoencoders for ITG-based translation. In Proceed- ings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 567–577, Seat- tle, Washington, USA, October. Association for Com- putational Linguistics.Dekang Lin. 2004. A path-based transfer model for machine translation. In Proceedings of Coling 2004, pages 625–630, Geneva, Switzerland, Aug 23–Aug 27. COLING.Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin. 2007. Forest-to-string statistical translation rules. In Pro- ceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 704–711, Prague, Czech Republic, June. Association for Computational Linguistics.Fandong Meng, Jun Xie, Linfeng Song, Yajuan Lu ̈, and Qun Liu. 2013. Translation with source constituency and dependency trees. In Proceedings of the 2013 Conference on Empirical Methods in Natural Lan- guage Processing, pages 1066–1076, Seattle, Wash- ington, USA, October. Association for Computational Linguistics.Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest- based translation. In Proceedings of ACL-08: HLT, pages 192–199, Columbus, Ohio, June. Association for Computational Linguistics.Franz Josef Och and Hermann Ney. 2002. Discrimi- native training and maximum entropy models for sta- tistical machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 295–302, Philadelphia, Pennsylva- nia, USA, July. Association for Computational Lin- guistics.Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Compu- tational Linguistics, pages 160–167, Sapporo, Japan, July. Association for Computational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic eval- uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylva- nia, USA, July. Association for Computational Lin- guistics.Adam Pauls, Dan Klein, David Chiang, and Kevin Knight. 2010. Unsupervised syntactic alignment with inversion transduction grammars. In Human Lan- guage Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 118–126, Los An- geles, California, June. Association for Computational Linguistics.Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De- pendency treelet translation: Syntactically informed phrasal SMT. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguis- tics (ACL’05), pages 271–279, Ann Arbor, Michigan, June. Association for Computational Linguistics.Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algo- rithm with a target dependency language model. In Proceedings of ACL-08: HLT, pages 577–585, Colum- bus, Ohio, June. Association for Computational Lin- guistics.Zhiguo Wang and Chengqing Zong. 2013. Large-scale word alignment using soft dependency cohesion con- straints. Transactions of the Association for Computa- tional Linguistics, 1:291–300.Kun Wang, Chengqing Zong, and Keh-Yih Su. 2010. A character-based joint model for chinese word segmen- tation. In Proceedings of the 23rd International Con- ference on Computational Linguistics (Coling 2010), pages 1173–1181, Beijing, China, August. Coling 2010 Organizing Committee.Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational linguistics, 23(3):377–403.Jun Xie, Haitao Mi, and Qun Liu. 2011. A novel dependency-to-string model for statistical machine translation. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Process- ing, pages 216–226, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.Jun Xie, Jinan Xu, and Qun Liu. 2014. Augment dependency-to-string translation with fixed and float- ing structures. In Proceedings of COLING 2014, the 25th International Conference on Computational Lin- guistics: Technical Papers, pages 2217–2226, Dublin, Ireland, August. Dublin City University and Associa- tion for Computational Linguistics.Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi- mum entropy based phrase reordering model for sta- tistical machine translation. In Proceedings of the 21stInternational Conference on Computational Linguis- tics and 44th Annual Meeting of the Association for Computational Linguistics, pages 521–528, Sydney, Australia, July. Association for Computational Lin- guistics.Hao Zhang and Daniel Gildea. 2005. Stochastic lexical- ized inversion transduction grammar for alignment. In Proceedings of the 43rd Annual Meeting of the Asso- ciation for Computational Linguistics (ACL’05), pages 475–482, Ann Arbor, Michigan, June. Association for Computational Linguistics.Hao Zhang, Liang Huang, Daniel Gildea, and Kevin Knight. 2006. Synchronous binarization for machine translation. In Proceedings of the Human Language Technology Conference of the NAACL, Main Confer- ence, pages 256–263, New York City, USA, June. As- sociation for Computational Linguistics.Jiajun Zhang, Feifei Zhai, and Chengqing Zong. 2011. Augmenting string-to-tree translation models with fuzzy use of source-side syntax. In Proceedings of the 2011 Conference on Empirical Methods in Natu- ral Language Processing, pages 204–215, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.