Selecting Training Data for Unsupervised Domain Adaptation in Word Sense DisambiguationAbstractThis paper describes a method of do- main adaptation, which involves adapting a classifier developed from source to tar- get data. We automatically select the train- ing data set that is suitable for the target data from the whole source data of mul- tiple domains. This is unsupervised do- main adaptation for Japanese word sense disambiguation (WSD). Experiments re- vealed that the accuracies of WSD im- proved when we automatically selected the training data set using two criteria, the degree of confidence and the leave-one- out (LOO)-bound score, compared with when the classifier was trained with all the data.1 IntroductionClassifiers in standard supervised machine learn- ing have been trained for data in domain A using manually annotated data in domain A, e.g., to train classifiers for newswires using newswires. How- ever, classifiers for data in domain B have some- times been necessary when there have been no or few manually annotated data, and there have only been manually annotated data in domain A, which has been related to domain B. Domain adapta- tion involves adapting the classifier that has been trained from data in domain A (source domain) to data in domain B (target domain). This has been studied intensively (see Section 2).However, the optimal method of domain adap- tation varied according to the properties of the data in the source domain (the source data) and the data in the target domain (the target data) when domain adaptation for word sense disambiguation (WSD) was carried out (Komiya and Okumura, 2011). In other words, the optimal training data varied ac- cording to the properties of the source and tar- get data. This paper proposes automatic domainadaptation in an unsupervised manner based on a comparison of multiple classifiers when Japanese WSD is performed (see Section 3). Our experi- ments (see Sections 4 and 5) revealed that the av- erage accuracy of WSD when the training data set that was automatically determined was used was higher than that when all the data were used col- lectively (see Section 6). We discuss the results in Section 7 and conclude this paper in Section 8.2 Related WorkThe domain adaptation problem can be catego- rized into three types depending on the informa- tion for learning, i.e., that in supervised, semi- supervised, and unsupervised approaches. Ac- cording to Daume ́ III et al. (2010), a classifier in a supervised approach is developed from a large amount of labeled source data and a small amount of labeled target data with the aim of classify- ing target data better than a classifier developed from only the target data. A classifier in a semi- supervised approach is developed from a large amount of labeled source data, a small amount of labeled and a large amount of unlabeled target data with the aim of classifying target data better than a classifier developed from only the source data. Fi- nally, a classifier is developed from a large amount of labeled source data and unlabeled target data with the aim of accurately classifying target data in an unsupervised approach. We focused on the un- supervised domain adaptation for Japanese WSD in the research reported in this paper.Many researchers have investigated domain adaptation within or outside the area of natural language processing.Chan and Ng (2006) carried out the domain adaptation of WSD by estimating class priors us- ing an EM algorithm. Chan and Ng (2007) also conducted the domain adaptation of WSD by es- timating class priors using an EM algorithm, but this was supervised domain adaptation using ac-
tive learning.In addition, Daume ́ III (2007) worked on super-vised domain adaptation. He augmented an input space and made triple length features that were general, source-specific, and target-specific. This was easy to implement, could be used with vari- ous domain adaptation methods, and could easily be extended to multi-domain adaptation problems.Daume ́ III et al. (2010) extended the earlier work by Daume ́ (Daume ́ III, 2007) to semi- supervised domain adaptation. It inherited the advantages of the supervised version and outper- formed it by using unlabeled target data.Agirre and de Lacalle (2008) worked on semi- supervised domain adaptation for WSD. They ap- plied singular value decomposition (SVD) to a matrix of unlabeled target data and a large amount of unlabeled source data, and trained a classifier with them. Agirre and de Lacalle (2009) worked on supervised domain adaptation using almost the same method, but they used a small amount of la- beled source data instead of a large amount of un- labeled source data.Jiang and Zhai (2007) demonstrated that perfor- mance increased as examples were weighted when domain adaptation was applied. This method could be used with various other supervised or semi-supervised domain adaptation methods. In addition, they tried to identify and remove source data that misled domain adaptation, but they con- cluded that it was only effective if examples were not weighted.Zhong et al. (2009) proposed an adaptive kernel approach that mapped the marginal distribution of source and target data into a common kernel space. They also conducted sample selection to make the conditional probabilities between the two domains closer.Raina et al. (2007) proposed self-taught learn- ing that utilized sparse coding to construct higher level features from unlabeled data collected from the Web. This method was based on unsupervised learning.Tur (2009) proposed a co-adaptation algorithm where both co-training and domain adaptation techniques were used to improve the performance of the model. The research by Blitzer et al. (Blitzer et al., 2006) involved work on semi- supervised domain adaptation, where they calcu- lated the weight of words around the pivot features (words that frequently appeared both in source andtarget data and behaved similarly in both) to model some words in one domain that behaved similarly in another. They applied SVD to the matrix of the weights, generated a new feature space, and used the new features with the original features.McClosky et al. (2010) focused on the problem where the best model for each document is not ob- vious when parsing a document collection of het- erogeneous domains. They studied it as a new task of multiple source parser adaptation. They pro- posed a method of parsing a sentence that first pre- dicts accuracies for various parsing models using a regression model, and then uses the parsing model with the highest predicted accuracy.Harimoto et al. (2010) measured the distance between domains to conduct domain adaptation using a suitable corpus in parsing. In addition, van Asch and Daelemans (2010) reported that per- formance in domain adaptation could be predicted depending on the similarity between source and target data using automatically annotated corpus in parsing. They focused on how corpora were se- lected for use as source data according to the dis- tance between domains,Komiya and Okumura (2011) and Komiya and Okumura (2012b) determined an optimal method of domain adaptation using decision tree learning given a triple of the target word type of WSD, source data, and target data. They discussed what features affected how the best method was deter- mined.Finally, the closest work to ours is that by Komiya and Okumura (2012a) who determined the optimal method, i.e., the optimal training data set, for each instance using the degree of confidence, which was also used in this paper, for supervised domain adaptation in WSD. We found that the method that Komiya and Okumura (2012a) proposed was also effective for unsuper- vised domain adaptation.3 Automatic selection of training dataWhen we perform WSD on the target data of a certain domain, we assume that the labels, i.e., the word senses, of the target data are unknown. If we have the source data of the multiple domains, we would automatically select the subset of the train- ing data that is suitable for the target data from the whole set of the source data in these domains. Komiya and Okumura (2012a) assumed that the optimal training data set would vary according to
each instance. However, Komiya and Okumura (2011) and Komiya and Okumura (2012b) deter- mined an optimal method of domain adaptation for each word type. Therefore, we investigate which is better for domain adaptation, to determine the optimal training data set according to the word type or the instance. The training data set is au- tomatically determined for each word type or each instance in four steps:(1) Select some instances randomly from the whole source data and create multiple train- ing data sets,(2) Train multiple classifiers based on the training data sets (in (1)) and apply them to the target data,(3) Compare the score of multiple classifiers (in (2)) for each word type or each instance, and(4) Employ the classifier whose score is the high- est for the word type or the instance.We use three types of scores for classifiers and compare them:• Confidence (Komiya and Okumura, 2012a),• LOO: LOO-bound score, which is the score based on the LOO-bound (Vapnik and Chapelle, 2000),• Confidence*LOO: The product of the two scores above.As Komiya and Okumura (Komiya and Okumura, 2012a) reported, the degrees of confidence are the predicted values that indicate how confident clas- sification is and these are often used to select in- stances to be labeled in active-learning. Since the classifier outputs the degree of confidence per in- stance, we use the average for all the instances in the training data set of each word type when we determine the optimal training data for each word type. In other words, the score is an averaged value that indicates how confidently a classifier classifies the whole target data of each word. We use the degree of confidence per instance directly when we determine the optimal training data for each instance. Komiya and Okumura (2012a) fo- cused on the fact that these degrees of confidence are output from classifiers as the probability, and they can carry out ensemble learning by compar- ing them. We use the same method for unsuper- vised domain adaptation.The LOO-bound score is the upper bound of er- ror for the leave-one-out estimation of SVM and is calculated as:SViLOOoriginal,i = TR , (1)iwhere LOOoriginal,i denotes the original LOO- bound score of each (ith) classifier, and SVi and TRi denote the number of support vectors and training data of each classifier, respectively. How- ever, when we select the suitable classifier from multiple classifiers, it is necessary to take into ac- count the numbers of training data of each classi- fier because they varies a great deal. Therefore, we use LOO-bound of a certain classifier weighted by the number of training data of each classifier.LOOselecting,i = T R1 LOOoriginal,1 = SV1 , T Ri T Ri(2)where LOOselecting,i denotes the LOO-bound for selecting the training data set, TR1 and SV1 de- note the number of training data and support vec- tors of a certain classifier in multiple classifiers developed in step (2) respectively, and TRi de- notes the number of training data of each clas- sifier. Since SV1 is constant for every classifier, LOOselecting,i weights the number of training data of each classifier. In addition, the weight is based on SV1, i.e., the number of support vectors of a certain classifier. We use 1 − LOOselecting,i in- stead of LOOselecting,i for the score of the clas- sifiers since the LOO-bound is the error rate. Fi- nally, we use the following equation to avoid ille- gal division because the number of training data could be zero when there is only a single word sense in the training data set; the instances are ran- domly selected.SV1 + 0.5LOOi = 1 − TRi + 0.5, (3)where LOOi denotes the LOO-bound score of each classifier.We are able to automatically determine the best training data set using ensemble learning based on the classifier score for each word type or each in- stance. Therefore, we expect the average accu- racy of WSD, when the training data set that is automatically determined is used for each word type or each instance, to be higher than when the whole training data are collectively used. Nav- igli (2009) introduced this method as an ensem- ble approach to WSD and called it a probability    
mixture. We used the probability mixture assum- ing that each classifier is trained for each training data set, rather than for each method of domain adaptation like reported in (Komiya and Okumura, 2012a) or each method of WSD like introduced in (Navigli, 2009). 4 ExperimentLibsvm (Chang and Lin, 2001), which supports multi-class classification, was used as the classifier for WSD. We used the -b option of libsvm to train a model to estimate probability for the degree of confidence. We trained 100 classifiers for a word type of a domain of source data and employed the classifier with the highest degree of confidence for each word type or each instance. We randomly se- lected the number of instances in one training data set, from one to the number of all the training data we could use, for each word type. Since the exper- iments were greatly affected by the randomness of the setting of each experiment, we performed the experiments 10 times and evaluated the averaged accuracies. A linear kernel was used according to the results obtained from preliminary experiments. Twenty features were introduced to train the clas- sifier.• Morphological features– Bag-of-words– Part-of-speech (POS)– Finer subcategory of POS• Syntactic features– IfthePOSofatargetwordisanoun,theverb that the target word modifies– If the POS of a target word is a verb, the case element of ‘ヲ’ (wo, objective) for the verb• Semantic feature– Semantic classification codeMorphological features and a semantic feature were extracted from the surrounding words (two words to the right and left) of the target word and the target word itself. POS and the finer subcate- gory of POS could be obtained by using a morpho- logical analyzer. We used ChaSen 1 as a morpho-1 http://sourceforge.net/projects/masayu-a/logical analyzer, the Bunruigoihyo thesaurus (Na- tional Institute for Japanese Language and Lin- guistics, 1964) for semantic classification codes (e.g. The code of the ‘program’ was 1.3162.), and CaboCha 2 as a syntactic parser. Five-fold cross validation was used in the experiments.5 DataThree data that were the same as those utilized by Komiya and Okumura (Komiya and Okumura, 2011) and (Komiya and Okumura, 2012a) were used for the experiments: (1) the sub-corpus of white papers in the Balanced Corpus of Contem- porary Japanese (BCCWJ) (Maekawa, 2008), (2) the sub-corpus of documents from a Q&A site on the WWW in BCCWJ, and (3) Real World Com- puting (RWC) text databases (newspaper articles) (Hashida et al., 1998). Domain adaptation was conducted in three directions according to differ- ent source and target data, i.e., one data in three was used for the target data and the other two data were used for the source data in one setting 3. Word senses were annotated in these corpora ac- cording to a Japanese dictionary, i.e., the Iwanami Kokugo Jiten (Nishio et al., 1994). It has three levels for sense IDs, and we used the fine-level sense in the experiments. Multi-sense words that appeared equal or more than 50 times in all the data were selected as the target words in the ex- periment. Twenty-two word types were used in the experiments. Table 1 lists the minimum, max- imum, and average number of instances of each word type for each corpus and Table 2 summarizes the number of instances of WSD for each corpus. Table 3 summarizes the list of target words.Komiya and Okumura (Komiya and Okumura, 2011) and Komiya and Okumura (Komiya and Okumura, 2012b) found that the optimal method of domain adaptation varied depending on each ‘case’ (i.e., a triple of the target word type of WSD, the source data, and the target data). Komiya and Okumura (Komiya and Okumura, 2012a) assumed that it varied according to each2 http://sourceforge.net/projects/cabocha/3 Komiya and Okumura (Komiya and Okumura, 2011) and Komiya and Okumura (Komiya and Okumura, 2012a) con- ducted domain adaptation in six directions with the source data from one domain and the target data from another do- main. They used multi-sense words that appeared equal or more than 50 times in both the source and target data, whereas we used multi-sense words that appeared equal or more than 50 times in the two source data and the target data. Therefore, we used fewer target word types than they did.  
    Target words (in Japanese)  場合 自分  事業情報地方社会思う子供          考える含む 技術  関係 時間 一般 現在 今      前持つ見る入る  言う出る   GenreBCCWJ white papers BCCWJ Q&A site RWC newspaperMin. Max. 58 7,610 130 13,976 56 374Avg. 2,240.14 2,741.95 183.36No. Sense example of senses     in English    2case   Table 1: Minimum, maximum, and average num- ber of instances of each word type for each corpusself3     projectinformation area society suppose child4     think5     containtechnique6     connectiontime general present7     now8     before10     have 12     see 14     enter   Target data white paper Q&A site newspaper TotalNo. of instances 49,283 60,323 4,034 232,116          Table 2: No. of instances of WSD for each corpusinstance. Here, we investigate which is better for domain adaptation, to determine the optimal train- ing data set according to the word type or the in- stance.6 ResultsTable 4 lists the micro- and macro-averaged accu- racies of WSD for the whole data set according to the methods of domain adaptation and Table 5 summarizes the micro- and macro-averaged accu- racies of WSD according to the corpora and meth- ods of domain adaptation.We tested Self, which is standard supervised learning with the whole target data by five-fold cross validation, assuming that fully annotated data were obtained and could be used for learn- ing, MFS, which is the most frequent sense of the target corpus, Averaged, which is the averaged ac- curacies of the supervised learning with the source data of each domain, Bigger, which is standard su- pervised learning with the bigger source data in two domains of the source data, and All, which is standard supervised learning with all the source data as references.When the target data were Q & A sites and the source data were white papers and newswires, for example, Averaged would be the averaged accu- racy of the two accuracies of the Q & A sites: those of the classifier trained with all the white pa- pers and with all the newswires. Bigger would be the accuracy of the classifier trained with all the newswires because the number of instances in the newswires was greater than that in the white pa- pers. Finally, All would be the accuracy of the Qsay     1622     leaveTable 3: List of target words& A sites whose classifier was trained with all the source data, i.e., both all the white papers and all the newswires.Self was an upper bound and Averaged, Bigger, and All were baselines. Confidence (ty) and Con- fidence (in) determined the optimal training data set using Confidence score for each word type and each instance, respectively. Confidence*LOO (ty) and Confidence*LOO (in) determined the optimal training data set using Confidence*LOO score for each word type and each instance, respectively. LOO determined the optimal training data set us- ing LOO score for only each word type because the system output LOO for each word type. The highest accuracies except for Self have been writ- ten in bold for each corpus in Tables 4 and 5.7 DiscussionFirst, Table 4 indicates that the micro-averaged ac- curacy of Bigger is higher than that of All and Ta- ble 5 shows the same when the target data were the Q & A sites, which means that the biggest training data did not always provide the highest accuracy.  
   96.07%Micro avg.      White papersnewswiresQ & A sitesWhite papers91.53%newswires      79.57%91.93%78.59%      78.74%68.59%76.74%69.81%    73.54% 80.72% 81.80%72.94% 74.86% 75.95%79.95%83.50%82.11%77.58%  71.23% 74.39% 74.91%                74.62% 74.01% 82.43% 81.92% 82.15%73.64% 74.54% 76.10% 76.49% 76.06%75.53% 76.03% 82.33% 82.91% 81.95%70.80% 75.64% 76.91%  71.95% 72.12% 77.68% 77.31% 77.54%72.59% 73.14% 75.17% 75.51% 75.17%                         Target dataSelfMFSAveragedBiggerAllConfidence (ty) Confidence (in) Confidence*LOO (ty) Confidence*LOO (in) LOOTable 5: Average accuracies of WSD according to corpora and methods of domain adaptationQ & A sites 87.80% 72.93% 71.57% 72.73% 75.76% 71.93% 72.73% 76.29% 76.23% 76.30%Macro avg.         Micro avg.93.29%77.32%76.92% 81.99% 81.76%  75.07% 75.10% 82.15% 82.25% 81.83%         MethodSelf     85.97% MFS     73.44% Averaged     71.20% Bigger     74.25% All     75.86%data set. However, it is difficult to deem that the best classifier is that trained with only one training instance, which means that the degrees of confi- dence are not particularly trustworthy when there are few instances of the training data set.Moreover, Tables 4 and 5 reveal that the ac- curacies of LOO outperformed those of the three baselines except for the micro-averaged accura- cies of the whole data set and those when the tar- get data were the Q & A sites. We think that the LOO-bound score was effective for selecting the classifier because it weighted the number of the training data and therefore the score indicated how trustworthy the classifier was. However, the micro-averaged accuracy for the whole data set of LOO could not outperform that of Bigger because the micro-averaged accuracy of Bigger was higher than that of LOO when the target data were Q & A sites.The micro- and macro-averaged accuracies of Confidence*LOO (ty) or Confidence*LOO (in), on the other hand, were the best except for Self, i.e., the upper bound, for the whole data set, although the micro-averaged accuracy could not outper- form that of Bigger when the target data were Q & A sites. Although the differences between the accuracies of Confidence*LOO (ty) and Big- ger were not statistically significant, the differ- ences between the micro-averaged accuracies of Confidence*LOO (in) and Bigger, those of Con- fidence*LOO (ty) and All, and those of Confi- dence*LOO (in) and All were statistically signif- icant according to a chi-square test. The level of significance in the test was 0.05. We think that Confidence*LOO (ty) and Confidence*LOO (in)Macro avg.    Confidence (ty) Confidence (in) Confidence*LOO (ty) Confidence*LOO (in) LOO72.16% 72.66% 76.38% 76.35% 76.34% Table 4: Average accuracies of WSD for whole data setSecond, the same tables reveal that the accu- racies of Confidence (ty) and Confidence (in) are lower than those of the three baselines. This is different from the results obtained by Komiya and Okumura (Komiya and Okumura, 2012a) who found that it was more effective to select the method of domain adaptation, i.e., the training data set, using the degree of confidence for each instance. We think this is because the correct- ness of the degree of confidence decreased when there were few instances of the training data set. Since we randomly determined the number of the instances of the training data set, the classifiers were sometimes trained with a small number of in- stances and this affected the decline in accuracies. When the training data set included only one in- stance, for example, the degree of confidence was one, which was the highest value, because there was no other alternative word sense in the training
were the best because it selected the most suitable classifier for each target data. LOO returned the same score for any target data if the training data were the same, but Confidence returned the score for each combination of the training data set and each instance of the target data.Moreover, the macro-average accuracies of Confidence*LOO (ty), Confidence*LOO (in), and LOO outperformed those of the three baselines al- though the differences were not statistically sig- nificant because there were few samples for these. This indicated that these criteria could be used to select a better training data set even for target data with fewer instances.Next, we investigate which is better for domain adaptation, to determine the optimal training data set according to the word type or the instance. Ta- bles 4 and 5 revealed that the best method of do- main adaptation of each target corpus was Confi- dence*LOO (ty) or Confidence*LOO (in) except for the micro- and macro-accuracies of Q & A sites; the best methods were Bigger and LOO for the micro- and macro-accuracy of Q & A sites, respectively. The same tables also show that the differences amang accuracies of Confi- dence*LOO (ty), Confidence*LOO (in), and LOO are not so big. This indicates that the effect of Confidence*LOO mainly comes from LOO and the unit for selecting the optimal training data set, i.e., the word type or the instance, dose not affect the results so much although the effect of Confi- dence did improved the accuracies.However, although the differences are not so big, Table 5 indicates that the best methods varies according to the target corpus; the best method is Confidence*LOO (ty) for white papers and it is Confidence*LOO (in) for newswires. We think that it is associated with the balance of the senses in the target corpus. As Table 5 shows, the ratio of MFS for white papers is highest and that for newswires is the lowest. Therefore, we think that Confidence*LOO (ty) is the best for white papers because it is better than Confi- dence*LOO (in) when the senses are biased. Like- wise, we think that Confidence*LOO (in) is the best for newswires because it is better than Con- fidence*LOO (ty) when the senses are balanced. It indicates that the system should determine the op- timal training data set according to the word type when the senses of the target corpus are biased and it should determine them according to the instancewhen the senses of the target corpus are balanced. Finally, Table 4 demonstrates that Confi- dence*LOO (in) is the best for micro-averaged ac- curacy and Confidence*LOO (ty) is the best for macro-averaged accuracy for the whole data set. We think this is because the method that select training data set for each word type, i.e., Confi- dence*LOO (ty), improves the word type based accuracy, i.e., macro-averaged accuracy and the method that select training data set for each in- stance, i.e., Confidence*LOO (in), improves the instance based accuracy, i.e., micro-averaged ac-curacy.8 ConclusionThis paper described how to automatically select the training data set by using two criteria, the degree of confidence and the LOO-bound score when there were multiple domain source data for unsupervised domain adaptation in WSD. We se- lected a suitable training data set using the degree of confidence, the score based on a LOO-bound, and their product. The method with the product of two criteria demonstrated the best micro- and macro-averaged accuracies. We also investigated which was better for domain adaptation, to deter- mine the optimal training data set according to the word type or the instance. Although the differ- ences between accuracies of the units for selecting the training data set, i.e., the word type or the in- stance, were not so big, the experimental results indicated that the system should determine the op- timal training data set according to the word type when the senses of the target corpus are biased and it should determine them according to the in- stance when the senses of the target corpus are bal- anced. Finally, the differences between the micro- averaged accuracies of the proposed method, i.e., the method with the product of two criteria for the instances, and three baselines were significant ac- cording to a chi-square test.ReferencesEneko Agirre and Oier Lopez de Lacalle. 2008. On robustness and domain adaptation using svd for word sense disambiguation. In Proceedings of the 22nd International Conference on Computational Linguistics, pages 17–24.Eneko Agirre and Oier Lopez de Lacalle. 2009. Su- pervised domain adaption for wsd. In Proceedings of the 12th Conference of the European Chapter of
the Association of Computational Linguistics, pages 42–50.John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural coppe- spondence learning. In Proceedings of the 2006 Conference on Empirical Methods in Natural Lan- guage Processing, pages 120–128.Yee Seng Chan and Hwee Tou Ng. 2006. Estimat- ing class priors in domain adaptation for word sense disambiguation. In Proceedings of the 21st Interna- tional Conference on Computational Linguistics and 44th Annual Meeting of the Association for Compu- tational Linguistics, pages 89–96.Yee Seng Chan and Hwee Tou Ng. 2007. Domain adaptation with active learning for word sense dis- ambiguation. In Proceedings of the 45th Annual Meeting of the Association of Computational Lin- guistics, pages 49–56.Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu.edu.tw/ cjlin/libsvm.Hal Daume ́ III, Abhishek Kumar, and Avishek Saha. 2010. Frustratingly easy semi-supervised domain adaptation. In Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Pro- cessing, ACL 2010, pages 23–59.Hal Daume ́ III. 2007. Frustratingly easy domain adap- tation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 256–263.Keiko Harimoto, Yusuke Miyao, and Jun’ichi Tsu- jii. 2010. Kobunkaiseki no bunyatekiou ni okeru seido teika youin no bunseki oyobi bunyakan kyori no sokutei syuhou, in japanese. In Proceedings of NLP2010 (In Japanese), pages 27–30.Koichi Hashida, Hitoshi Isahara, Takenobu Tokunaga, Minako Hashimoto, Shiho Ogino, and Wakako Kashino. 1998. The rwc text databases. In Proceed- ings of the First International Conference on Lan- guage Resource and Evaluation, pages 457–461.Jing Jiang and ChengXiang Zhai. 2007. Instance weighting for domain adaptation in nlp. In Proceed- ings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 264–271.Kanako Komiya and Manabu Okumura. 2011. Auto- matic determination of a domain adaptation method for word sense disambiguation using decision tree learning. In Proceedings of the 5th International Joint Conference on Natural Language Processing, IJCNLP 2011, pages 1107–1115.Kanako Komiya and Manabu Okumura. 2012a. Au- tomatic domain adaptation for word sense disam- biguation based on comparison of multiple classi- fiers. In PACLIC 2012, pages 77–85.Kanako Komiya and Manabu Okumura. 2012b. Au- tomatic selection of domain adaptation method for wsd using decision tree learning. Journal of NLP (In Japanese), 19(3):143–166.Kikuo Maekawa. 2008. Balanced corpus of con- temporary written japanese. In Proceedings of the 6th Workshop on Asian Language Resources (ALR), pages 101–102.David McClosky, Eugene Charniak, and Mark John- son. 2010. Automatic domain adaptation for pars- ing. In Proceedings of the 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 28–36.National Institute for Japanese Language and Linguis- tics. 1964. Bunruigoihyo. Shuuei Shuppan, In Japanese.Roberto Navigli. 2009. Word sense disambiguation: A survey. ACM Comput. Surv., 41(2):1–69.Minoru Nishio, Etsutaro Iwabuchi, and Shizuo Mizu- tani. 1994. Iwanami Kokugo Jiten Dai Go Han. Iwanami Publisher, In Japanese.Rajat Raina, Alexis Battle, Honglak Lee, Benjamin Packer, and Andrew Y. Ng. 2007. Self-taught learn- ing: Transfer learning from unlabeled data. In ICML ’07: Proceedings of the 24th international confer- ence on Machine learning, pages 759–766.Gokhan Tur. 2009. Co-adaptation: Adaptive co- training for semi-supervised learning. In Proceed- ings of the IEEE International Conference on Acous- tics, Speech and Signal Processing, 2009. ICASSP 2009., pages 3721–3724.Vincent van Asch and Walter Daelemans. 2010. Us- ing domain similarity for performance estimation. In Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, ACL 2010, pages 31–36.V. Vapnik and O. Chapelle. 2000. Bounds on error ex- pectation for support vector machines. Neural Com- put., 12(9):2013–2036.Erheng Zhong, Wei Fan, Jing Peng, Kun Zhang, Jiang- tao Ren, Deepak Turaga, and Olivier Verscheure. 2009. Cross domain distribution adaptation via ker- nel mapping. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1027–1036.