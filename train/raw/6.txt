1AbstractUsing natural languages to query relational databases is of widespread interest. We present an efficient implementation for this purpose. It is based on the PRECISE (Popescu et al., 2003) model of natural language inter- face to databases, which translates the natu- ral language query into an SQL query by im- plementing a high-level abstraction of a graph and identifying the information available in the explicit and the implicit sense. The model uses the idea that named entities can be clas- sified into value-tokens (entries in tuple) and attribute-tokens (column names) and it then creates a graph on the basis of their relation- ships and uses the max-flow algorithm. It works well for the widely prevalent ‘semanti- cally tractable’ questions. PRECISE is sound and complete for such questions. We have also incorporated some rule-based heuristics into the system. It can be easily adapted to other relational databases.Motivation and IntroductionSecond AuthorAffiliation / Address line 1 Affiliation / Address line 2 Affiliation / Address line 3 email@domainsystems and decided to make an open-source soft- ware for this purpose, which can be easily adapted to other relational databases. For example, one might populate a relational database with the results of an online survey and use our system to query it in a nat- ural language.An effective question answering engine for re- lational database is an application of Natural Lan- guage Interface to Databases (NLIDB) and factoid question answering. Factoid questions are those questions that have some facts as answers. Ex- amples include, “What is the population of Great Britain”, “How many vowels are there in the English language”. We first describe the scope of question answering and NLIDB systems and some related work in brief, followed by a description of archi- tecture of our system and a qualitative performance evaluation.2 Question Answering and NLIDBWith the advent of Internet and easier modes of in- formation sharing in various forms, it has become imperative to provide a method for retrieving rele- vant data for various tasks. Users may spend hours searching over the Internet as search engines do not comprehend natural language queries well enough so far. Even more so, long natural language queries often lead to bizarre (if any) results. Therefore, bet- ter QA systems satisfy a wide variety of needs. Per- sonal applications include systems such as Siri1 and Cortana2. Even for enterprises, the ability to answer1 en.wikipedia.org/wiki/Siri2 en.wikipedia.org/wiki/Microsoft_CortanaAn Open Source Question Answering Interface for Relational DatabasesOur university, like many other institutes, does not have any internal search portal on which a naive user can search and find information about various students and professors of the university. The only option left is to use Google custom search on the institute website. Our initial motivation was to use the student database and build a question-answering engine that can be hosted on the institute website to permit naive users to fire queries in natural lan- guage. While developing such a system, we came to realize that there is a widespread need for such 
long natural language queries concerning project al- lotments and geographical launch information etc. can save a lot of desk work.Databases contain a collection of related data, stored in a systematic way to model a part of the world. In order to extract information from a database, one needs to formulate a query in such a way that the computer will understand and produce the desired output. However, writing queries which computer can understand requires technical skill and can be tough for those who lack a background in Computer Science or programming. The easiest way for people to obtain information is by asking questions in a natural language. Natural language interfaces to databases (NLIDB) are systems that translate a natural language sentence into a database query (Nihalani et al., 2011).3 Related WorkThere have been a lot of attempts to create NLIDB systems in the past. The system LUNAR (Woods et al., 1972) answers questions about samples of rocks brought back from the moon. The LAD- DER (Sacerdoti, 1977) system was designed as a natural language interface to a database of infor- mation about US Navy ships. The system CHAT- 80 (Warren and Pereira, 1982) was one of the most referenced NLP systems in the eighties. The sys- tem was implemented in Prolog. In recent times, NALIX (Li et al., 2005) was developed for querying extensible markup language (XML) database with Schema-Free XQuery as the database query lan- guage. The idea was to use keyword search for databases. Word Alignment-based Semantic Pars- ing, or WASP (Wong and Mooney, 2006), is de- signed to address the broader goal of constructing “a complete, formal, symbolic, meaningful representa- tion of a natural language sentence”. It can also be applied to the NLIDB domain. Predicate logic (Pro- log) was used as the formal query language.A lot of question answering systems have also been developed. AnswerBus (Zheng, 2002) is an open-domain question answering system based on sentence level Web information retrieval. It ac- cepts natural language questions in English, Ger- man, French, Spanish, Italian and Portuguese and provides answers in English. IBM’s Watson (Fer-rucci, 2012) used DeepQA approach to win the fa- mous “Jeopardy” challenge.Many NLIDB-type question answering sys- tems for relational databases are based on tem- plates (Stratica et al., 2003). In contrast, out system is based on a graphical representation and then bi- partite matching on that representation (Popescu et al., 2003).4 System ArchitectureThis section provides an overview of our system, which is built in Python. It is divided into three parts:• Preprocessing• Real time processing• Result extraction and display4.1 PreprocessingThis phase includes the preparation of database and the lexicon table.4.1.1 Preparation of DatabaseIf the database is not available already, then data can be obtained from users by carrying out an on- line survey and recording the responses in a csv file (comma separated values). The system then auto- matically builds a relational database from the csv file. For this purpose, a small Java API has been created. Once, the database is available, it is then loaded into the memory and the information from the database is stored in a two dimensional array for building up the lexicon table. The database used by us for testing consists of a single relation or table named “student”. It consists of the following fields or attributes: Name, Roll Number, Branch, Year, Degree, Date of Birth, Home Town, Research In- terests, Sport, Internship Company and Placement Company.4.1.2 Building the Lexicon TableThe lexicon table stores the domain related infor- mation of the database. In our system, it stores vari- ous words that will map to corresponding attributes in the table of the database. It also includes the list of wh-words that are compatible with a particular attribute. The coverage of the lexicon table can be
 Figure 1: System Architecture
increased by using the WordNet (Miller, 1995) to find words that are semantically similar to the given attributes. In our system, we did not use the Word- net and instead used a rule-based approach to fill the entries of the lexicon table.4.2 Real Time ProcessingThis phase includes processing of the input natu- ral language query and the intermediate SQL query generation.4.2.1 Query TokenizationThis module takes the input from the user. The input query is tokenized by using NLTK3 tok- enizer. These tokens are used for further process- ing. For example, if the query is “Where does Ralph live?”, then the tokenized query is [‘Where’, ‘does’, ‘Ralph’, ‘live’].4.2.2 StemmingStemming reduces inflected (or sometimes de- rived) words to their word stem, base or root form, which is generally a written word form. The stem need not be identical to the morphological root of the word. It is usually sufficient that related words map to the same stem. In our system, the tokenized words are converted to lower case and then stemmed by us- ing the Porter Stemmer (Porter, 1980) from NLTK. Stemming is done for extracting the attribute tokens. In this step, we also implement some heuristics. For extracting the value tokens, first they are extracted without stemming the query words and then they are extracted with stemming.4.2.3 Extraction of Value TokensThis module extracts the value tokens from the tokenized query. The subarrays of contiguous words of tokenized query are generated in the decreasing order of length. Thus, the subarray with the largest size is first matched with all the elements of the database.Sub-string matching: When a contigous subar- ray of tokens is matched with the matching values in the corresponding attributes, then the elements of that attribute are also split in the subarrays of de- creasing length. For example, if the subarray gener- ated from the query is “John” and the element with3Natural Language Toolkit: http://www.nltk.orgwhich it is being matched is “John Parker”. Then first the string “John Parker” is split into subarrays like “John Parker”, “John”, and “Parker”. These substrings are then matched with the query subarray.Strict matching: In this case, the value tokens are not split and the complete attribute value elements are matched with the subarray. If it is present, then that value token is considered.Removing ambiguities: There might be ambi- guities in the database. For example there may be two names “John Parker” and “John Lever” in the database. In this case “John” matches with both the names. To resolve this ambiguity, we have made our system user interactive. It matches the subarray with all the elements of the database and the candidate el- ements are stored in an array. These candidate ele- ments are displayed to the user and the user chooses one of the options. The chosen element is then added to the value tokens list.Adding question words: The question words like “who”, “what”, “where” etc. are also added to the value tokens. For example, in the query “where does Ralph live and which sport does he play”, the token “Ralph” matches with the element in the “Name” attribute. Wh-words are also added to the value tokens. Hence, the final value tokens obtained are “Ralph”, “where” and “which”.4.2.4 Attribute Token ExtractionThis module extracts the attribute tokens from the tokenized query. The subarrays of contiguous words of tokenized query are generated in the decreasing order of length. The subarray is searched in the lex- icon table generated in the pre-processing step. The matched term and the attribute are appended to the attribute tokens.4.2.5 Graph Creation and MatchingThis containes two modules : Graph genera- tor and matcher. The graph generator creates the flow network by the method described for the PRE- CISE (Popescu et al., 2003) system. The matcher tries to solve the problem of finding the maximum matching of the tokenization of query elements to database fields by reducing to a graph matching problem. This module embodies the core inno- vation in PRECISE and identifies proper match- ing attributes for ambiguous natural language value 
tokens. This is done by implementing Edmonds Karp maximum flow algorithm (Edmonds and Karp, 1972) on the graph denoting the semantic relation- ships.4.2.6 Intermediate SQL Query GenerationThe query generator takes the database elements selected by the matcher and weaves them into a well- formed SQL query. In the case of single-relation queries, this process is straightforward. The SE- LECT portion of the query contains the database el- ements paired with the wh-words, the WHERE por- tion contains a conjunction of attributes and their values, and the FROM portion contains the relevant relation name for the attributes in WHERE.From the maximum flow of the graph, the edges contributing to the maximum flow are found out. From these edges, we store the attributes to which value tokens should be matched. The column names to which question words like ‘who’, ‘where’ etc. are mapped are filled in the SELECT section and the columns to which other value tokens map are filled in the WHERE section as a pair of attributes and the corresponding value token. Thus the template of our query generation is:SELECT [column names (attributes)] FROM stu- dent WHERE [attribute = attribute value]For example, if the query is “where does Ralph live and which sport does he play”, then in the max flow path, the value token “where” is mapped to Home, “Ralph” is mapped to “Name”, “which” is mapped to “Sport”. Thus, the SELECT section con- tains “Home” and “Sport” and the WHERE section contains “Name = Ralph”.Final SQL query obtained from the above natural language query is:SELECT Home, Sport FROM student WHERE Name = Ralph.4.3 Result Extraction and DisplayThis consists of two modules: The query validator and query executor.4.3.1 Query ValidatorThe query validator examines the intermediate SQL query formed from the query generation mod- ule. The raw query can contain a lot of extra spaces.Therefore, it is first stripped of leading and trailing spaces. Types of intermediate queries are:Empty SELECT and Empty WHERE: This happens in the case when the input query is not re- lated to the database (e.g. ‘Why and when was the Sun upset?’) or is not a meaningful sentence. In such a case, this query should not be executed on the database to prevent runtime errors. Instead, the user must to be prompted to rephrase the query.Empty SELECT and Non-empty WHERE:This happens when the query contains the database value token (element), but it does not contain im- plicit or explicit database attribute. The desired in- formation does not map to any database attribute. Sufficient information in not available to answer such a query, even if the the query is valid. Such a case is handled by replacing the empty SE- LECT with the SELECT * clause. For exam- ple, for the query ‘Who is Ralph’, the raw query generated is “SELECT FROM student WHERE Name=‘Ralph”’. Since, there was no column at- tribute in the original query, there was no SELECT clause in the generated SQL query. The query val- idator modifies this query as: “SELECT * from STUDENT WHERE Name = ‘Ralph”’.Non-empty SELECT and Empty WHERE:This happens when the value-token given in the original query is absent in the database, but the corresponding attribute token gets matched with the database attribute (implicit or explicit). Since the query is valid, the user should receive the re- lated information about the asked database elements. Therefore, the empty WHERE clause is dropped from the query and only the SELECT clause is re- tained. For example, for the query “Who are the stu- dents that dance?”, since there is no database value element “dance” or no attribute matching “dance” in the database, there will be no mapping for the WHERE clause, but “students” will be mapped with the database attribute “Name”. As a result, the re- sultant query will be “SELECT Name from STU- DENT”. As with the previous case, the user will also be shown an appropriate message.Non-empty SELECT and Non-empty WHERE: In this case, the query is valid and is returned as it is.
4.3.2 Query ExecutorThe valid SQL query is executed on the database and the results are displayed on the display screen of the interactive interface.4.4 Interactive Graphical User InterfaceAn interactive graphical user interface has been de- veloped over the system to ease the process of asking natural language queries for naive users. The user interface is built using Python, Flask, JavaScript, Ajax and uses Bootstrap and JQuery libraries.An HTML page is created as a welcome page. The user asks the question in the text-box and the results are displayed on the display screen. In case of ambiguity, the user is shown a drop-down menu to select one of the elements to resolve the ambiguity.5 Performance and Error AnalysisThe system can effectively process and handle the following kinds of queries.It can handle and answer direct and straightfor- ward questions of any length. For example, “Who are the people who study computer science and what is their date of birth and in which city do they live and what is the sport played by them and what is their area of research and in which year are they studying?”It correctly generates the following SQL query for the previously described student database: SELECT Name, Year, Date, Home, Interest, Sport FROM stu- dent WHERE Branch = “computer science”. The order of the asked entities in the question does not matter. For example, the system produces same re- sults for these queries:1) Who all play cricket and study computer sci- ence? 2) Who all study computer science and play cricket?Resultant query: SELECT Name FROM student WHERE Branch = “computer science” AND Sport = “cricket”It handles queries when more than one database value elements are given and the single person (who) is asked. For example:Who studies computer science, plays poker, lives in london and is in 4th year?Resultant query: SELECT Name FROM student WHERE Branch = “computer science” AND Sport= “poker” AND Home = “london” AND Year = “4” It can handle queries in which more than one enti- ties are asked and more than one value-elements aregiven to identity those entities about a person:Who in 4 year plays poker. Where does he live and what is his area of interest?Resultant query: SELECT Name, Home, Interest FROM student WHERE Year = “4” AND Sport = ‘poker”It can handle contextual information in a single query statement:Where does Ralph Wilson live and what is his fa- vorite sport?Resultant query: SELECT Home, Sport FROM student WHERE Name = “Ralph Wilson”It can handle multiple different queries about a single person asked in the form of separate ques- tions:Where does Ralph live? Which sport does Ralph play?Resultant query: SELECT Home, Sport FROM student WHERE Name = “Ralph”It also answers indirect queries from the database:Who is Ralph?Resultant query: SELECT * FROM student WHERE Name = “Ralph”Since this is not a factoid question, a message is also displayed, saying “Query cannot be directly an- swered using the database ... Displaying all the re- lated information”.The system is able to answer the question below as it uses the stemmer to break the words into the root words, which are then used for finding matching tokens:Who are the cricketers in Paris?Resultant query: SELECT Name FROM student WHERE Sport = “cricket” AND Home = “paris”The system also handles the case when complete value of database element is not entered in the query:Where does Sam live?Resultant query: SELECT Home FROM student WHERE Name = “Sam Wilson”If the asked token matches to unique element of database, then that element is used to build the query, otherwise the user is asked to resolve the am- biguity.Ambiguity resolver using the interactive inter- face: Whenever the query involves some tokens that
come in many attributes of the database (columns), it results in ambiguity. In such cases, the user is shown the ambiguous entity and the possible options to re- solve that entity. The user then selects the appropri- ate entity.Effective exception handling: The system also deals with the cases when the query is not well formed. It adds the obvious information if pos- sible or deletes the unnecessary elements. It also replies to the user with messages like “Bad Query/Insufficient information” in case queries are not well formed and thus prevents runtime errors.Easy adaptability: The system can be easily adapted for other relation databases with minimal code changes as it does not use much domain spe- cific information other than the rules for building the lexicon table. So, if rules for lexicon tables can be supplied through a configuration file, this system can used for other databases also.The system cannot handle certain kinds of queries. It cannot handle the case when the query is asked about two different value elements of the same attribute in the same query:Which sport is played by Ralph. What is the area of interest of Sam?Here, “Name” is the common attribute and “Ralph” and “Sam” are the two value tokens of this attribute.It currently cannot handle multiple relations in the database. Aggregation functions are alo not handled. It also does not do context processing between dif- ferent queries.6 ConclusionThis question answering system that we have devel- oped for relational databases works well for ‘seman- tically tractable’ natural language queries. It is based on the PRCISE system (Popescu et al., 2003) with the addition of some heuristics. It worked well on our university database. It was able to form correct SQL queries for a majority of questions that can be asked on a relational database. Such questions are widely prevalent. The system is efficient in terms of response time as it is able to process long verbose queries and generate the answer within a second. It has an interactive interface and some kinds of ambi- guities are resolved by prompting the user. We havebuilt this system as an open source software and such a software is not currently available in the public do- main.7 Future WorkIn future, we would like to extend the system to deal with a wider set of queries. The system should be able to find independent queries from the given larger query. It should be able to execute those queries in parallel and combine the results. This will also enable answering queries like “Which sport is played by Ralph? What is Sam’s area of interest?”. We plan to extend the system to work with multiple (normalized) relations in a single database. It can be done by calculating the flow for the relations also along with attributes and values. The system should be developed to include aggregation function pro- cessing. This would enable answering queries like “Which student has the highest CGPA in the com- puter science department”. A more difficult exten- sion will be to include context information process- ing between different queries, which can be an area of further research.ReferencesJack Edmonds and Richard M Karp. 1972. Theoret- ical improvements in algorithmic efficiency for net- work flow problems. Journal of the ACM (JACM), 19(2):248–264.D. A. Ferrucci. 2012. Introduction to ”this is watson”. IBM J. Res. Dev., 56(3):235–249, May.Yunyao Li, Huahai Yang, and HV Jagadish. 2005. Nalix: an interactive natural language interface for querying xml. In Proceedings of the 2005 ACM SIGMOD in- ternational conference on Management of data, pages 900–902. ACM.George A. Miller. 1995. Wordnet: A lexical database for english. COMMUNICATIONS OF THE ACM, 38:39– 41.Mrs. Neelu Nihalani, Dr. Sanjay Silakari, and Dr. Ma- hesh Motwani. 2011. Natural language interface for database: A brief review. International Journal of Computer Science Issues, 8(2).Ana-Maria Popescu, Oren Etzioni, and Henry Kautz. 2003. Towards a theory of natural language inter- faces to databases. In Proceedings of the 8th Interna- tional Conference on Intelligent User Interfaces, IUI ’03, pages 327–327. ACM.
Martin F Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130–137.Earl D Sacerdoti. 1977. Language access to distributed data with error recovery. In Proceedings of the 5th in- ternational joint conference on Artificial intelligence- Volume 1, pages 196–202. Morgan Kaufmann Publish- ers Inc.Niculae Stratica, Leila Kosseim, and Bipin C. Desai. 2003. NLIDB templates for semantic parsing. In Nat- ural Language Processing and Information Systems, 8th International Conference on Applications of Natu- ral Language to Information Systems, June 2003, Burg (Spreewald), Germany, pages 235–241.David HD Warren and Fernando CN Pereira. 1982. An efficient easily adaptable system for interpreting natu- ral language queries. Computational Linguistics, 8(3- 4):110–122.Yuk Wah Wong and Raymond J Mooney. 2006. Learning for semantic parsing with statistical machine transla- tion. In Proceedings of the main conference on Human Language Technology Conference of the North Ameri- can Chapter of the Association of Computational Lin- guistics, pages 439–446. Association for Computa- tional Linguistics.William A Woods, Ronald M Kaplan, Bonnie Nash- Webber, and Manned Spacecraft Center. 1972. The lunar sciences natural language information system: Final report. Bolt Beranek and Newman.Zhiping Zheng. 2002. Answerbus question answer- ing system. In Proceedings of the Second Interna- tional Conference on Human Language Technology Research, HLT ’02, pages 399–404, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.