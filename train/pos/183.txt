An_DT Unsupervised_JJ Text_NN Classification_NNP Method_NNP Based_VBD On_IN LDA_NNP Abstract_NNP easier_JJR to_TO collect_VB a_DT large_JJ number_NN of_IN unmarked_JJ samples_NNS ,_, however_RB ,_, access_NN to_TO a_DT large_JJ number_NN of_IN marked_JJ samples_NNS is_VBZ relatively_RB difficult_JJ and_CC labeled_VBD the_DT samples_NNS collected_VBN usually_RB spend_VBP a_DT lot_NN of_IN time_NN and_CC energy_NN ._.
In_IN fact_NN ,_, in_IN the_DT real_JJ world_NN application_NN problems_NNS ,_, there_EX are_VBP usually_RB a_DT lot_NN of_IN unmarked_JJ samples_NNS ,_, but_CC marked_VBD sample_NN is_VBZ relatively_RB small_JJ ,_, especially_RB in_IN text_NN classification_NN task_NN ,_, if_IN we_PRP only_RB use_VBP the_DT tag_NN samples_NNS ,_, there_EX will_MD be_VB a_DT big_JJ problem_NN hard_JJ to_TO resolve_VB ._.
In_IN this_DT paper_NN ,_, we_PRP propose_VBP an_DT improved_VBN unsupervised_JJ text_NN classification_NN algorithm_NN based_VBN on_IN LDA_NNP -LRB-_-LRB- Latent_NNP Dirichlet_NNP Allocation_NNP -RRB-_-RRB- which_WDT does_VBZ not_RB need_VB a_DT large_JJ number_NN of_IN documents_NNS labeled_VBN by_IN humans_NNS ._.
The_DT experimental_JJ results_NNS on_IN datasets_NNS prove_VBP the_DT effectiveness_NN of_IN our_PRP$ approach_NN ._.
2_CD Related_JJ Work_NN Blei_NNP -LRB-_-LRB- 2003_CD -RRB-_-RRB- used_VBN LDA_NNP topics_NNS as_IN features_NNS in_IN text_NN classification_NN ,_, but_CC they_PRP use_VBP labeled_VBN documents_NNS while_IN learning_VBG a_DT classifier_NN ._.
sLDA_NNP -LRB-_-LRB- Blei_NNP and_CC McAuliffe_NNP et_FW al._FW ,_, 2007_CD -RRB-_-RRB- ,_, DiscLDA_NNP -LRB-_-LRB- Lacoste-Julien_NNP et_FW al._FW ,_, 2008_CD -RRB-_-RRB- and_CC MedLDA_NNP -LRB-_-LRB- Zhu_NNP et_FW al._FW ,_, 2009_CD -RRB-_-RRB- are_VBP few_JJ extensions_NNS of_IN LDA_NNP which_WDT model_NN both_DT class_NN labels_NNS and_CC words_NNS in_IN the_DT documents_NNS ._.
These_DT models_NNS can_MD be_VB used_VBN for_IN text_NN classification_NN ,_, but_CC they_PRP need_VBP expensive_JJ labeled_VBN documents_NNS ._.
Nigam_NNP -LRB-_-LRB- 2000_CD -RRB-_-RRB- proposed_VBN semi-supervised_JJ approaches_NNS for_IN document_NN classification_NN based_VBN on_IN labeled_VBN and_CC unlabeled_JJ datasets_NNS ._.
McCallum_NNP and_CC Nigam_NNP -LRB-_-LRB- 1999_CD -RRB-_-RRB- proposed_VBD a_DT semi-supervised_JJ approach_NN based_VBN on_IN labeling_VBG of_IN keywords_NNS ._.
In_IN keyword_NN based_VBN approaches_NNS ,_, finding_VBG right_NN set_NN of_IN keywords_NNS is_VBZ a_DT challenge_NN ._.
However_RB ,_, these_DT algorithms_NNS are_VBP sensitive_JJ to_TO initial_JJ labeled_VBN documents_NNS and_CC hyper-parameters_NNS of_IN the_DT algorithm_NN ._.
Liu_NNP -LRB-_-LRB- 2004_CD -RRB-_-RRB- proposed_VBD a_DT weakly_RB supervised_VBN classification_NN method_NN based_VBN on_IN labeling_VBG words_NNS ._.
It_PRP Text_NN classification_NN is_VBZ one_CD of_IN the_DT important_JJ application_NN in_IN the_DT field_NN of_IN natural_JJ language_NN processing_NN ,_, in_IN which_WDT supervised_JJ classification_NN algorithm_NN requires_VBZ a_DT lot_NN of_IN manpower_NN and_CC time_NN to_TO annotation_NN data_NNS ._.
In_IN this_DT paper_NN ,_, we_PRP propose_VBP an_DT unsupervised_JJ text_NN classification_NN algorithm_NN on_IN the_DT basis_NN of_IN LDA_NNP ,_, which_WDT no_RB longer_RB need_VB to_TO ask_VB an_DT annotator_NN to_TO assign_VB one_CD class_NN label_NN to_TO every_DT topic_NN ,_, but_CC rather_RB determine_VB which_WDT class_NN label_NN the_DT topic_NN belongs_VBZ to_TO by_IN computing_VBG the_DT distances_NNS between_IN this_DT topic_NN with_IN all_PDT the_DT classes_NNS ._.
Besides_IN ,_, we_PRP maintain_VBP the_DT word_NN order_NN and_CC introduce_VB the_DT bigram_NN grammar_NN ._.
Experimental_JJ results_NNS show_VBP that_IN when_WRB this_DT approach_NN reduces_VBZ manpower_NN and_CC material_NN resources_NNS ,_, we_PRP also_RB obtain_VB better_JJR effect_NN ._.
1_CD Introduction_NNP With_IN the_DT development_NN of_IN science_NN and_CC technology_NN ,_, large_JJ amount_NN of_IN information_NN in_IN machine-readable_JJ form_NN ,_, people_NNS also_RB more_RBR and_CC more_RBR dependent_JJ on_IN the_DT information_NN resources_NNS ._.
The_DT text_NN information_NN has_VBZ been_VBN occupies_VBZ an_DT important_JJ position_NN ._.
How_WRB can_MD we_PRP find_VB the_DT most_RBS useful_JJ information_NN in_IN such_JJ complex_JJ text_NN information_NN for_IN us_PRP has_VBZ always_RB been_VBN the_DT major_JJ problems_NNS that_WDT need_VBP to_TO be_VB solved_VBN in_IN information_NN processing_NN ._.
How_WRB to_TO realize_VB the_DT classification_NN of_IN information_NN ,_, especially_RB the_DT effective_JJ classification_NN of_IN text_NN information_NN is_VBZ an_DT important_JJ branch_NN of_IN information_NN processing_NN research_NN field_NN ._.
It_PRP promots_VBZ the_DT rapid_JJ development_NN and_CC extensive_JJ application_NN of_IN automatic_JJ text_NN classification_NN technology_NN ._.
In_IN the_DT traditional_JJ supervised_JJ text_NN classification_NN algorithm_NN ,_, it_PRP almost_RB needs_VBZ a_DT large_JJ amount_NN of_IN labeled_VBN corpus_NN ._.
With_IN the_DT rapid_JJ development_NN of_IN data_NNS collection_NN and_CC storage_NN technology_NN ,_, it_PRP much_RB combines_VBZ clustering_NN and_CC feature_NN selection_NN and_CC then_RB labels_VBZ a_DT set_NN of_IN representative_JJ words_NNS for_IN each_DT class_NN through_IN ranking_VBG the_DT words_NNS in_IN the_DT unlabeled_JJ documents_NNS according_VBG to_TO their_PRP$ importance_NN ._.
Druck_NNP -LRB-_-LRB- 2008_CD -RRB-_-RRB- ,_, using_VBG the_DT generalized_VBN expectations_NNS criterion_NN ,_, puted_VBN forward_RB a_DT weakly_RB supervised_VBN learning_VBG method_NN based_VBN on_IN feature_NN tagged_VBN ._.
An_DT important_JJ limitation_NN of_IN these_DT algorithms_NNS is_VBZ coming_VBG up_RP with_IN a_DT small_JJ set_NN of_IN words_NNS that_WDT should_MD be_VB presented_VBN to_TO the_DT annotators_NNS for_IN labeling_VBG ._.
Also_RB a_DT human_JJ annotator_NN may_MD discard_VB or_CC mislabel_VB a_DT polysemous_JJ word_NN ,_, which_WDT may_MD affect_VB the_DT performance_NN of_IN a_DT text_NN classifier_NN ._.
An_DT approach_NN that_WDT is_VBZ not_RB demanding_VBG in_IN terms_NNS of_IN labeled_VBN documents_NNS is_VBZ ClassifyLDA_NNP -LRB-_-LRB- Hingmire_NNP et_FW al._FW ,_, 2013_CD -RRB-_-RRB- ._.
In_IN this_DT approach_NN ,_, a_DT topic_NN model_NN on_IN a_DT given_VBN set_NN of_IN unlabeled_JJ training_NN documents_NNS is_VBZ constructed_VBN using_VBG LDA_NNP ,_, then_RB an_DT annotator_NN assigns_VBZ a_DT class_NN label_NN to_TO some_DT topics_NNS based_VBN on_IN their_PRP$ most_RBS probable_JJ words_NNS ._.
These_DT labeled_VBN topics_NNS are_VBP used_VBN to_TO create_VB a_DT new_JJ topic_NN model_NN such_JJ that_IN in_IN the_DT new_JJ model_NN topics_NNS are_VBP better_RBR aligned_VBN to_TO class_NN labels_NNS ._.
A_DT class_NN label_NN is_VBZ assigned_VBN to_TO a_DT test_NN document_NN on_IN the_DT basis_NN of_IN its_PRP$ most_RBS prominent_JJ topics_NNS ._.
In_IN this_DT paper_NN ,_, we_PRP propose_VBP a_DT new_JJ improved_VBN algorithm_NN based_VBN on_IN LDA_NNP which_WDT does_VBZ not_RB need_VB labeled_VBN documents_NNS ._.
In_IN this_DT algorithm_NN ,_, a_DT topic_NN model_NN on_IN a_DT given_VBN set_NN of_IN unlabeled_JJ training_NN documents_NNS is_VBZ constructed_VBN using_VBG LDA_NNP and_CC a_DT set_NN of_IN artificial_JJ representative_NN words_NNS -LRB-_-LRB- Hingmire_NNP et_FW al._FW ,_, 2014_CD -RRB-_-RRB- is_VBZ created_VBN for_IN each_DT class_NN label_NN ,_, then_RB translate_VB the_DT most_RBS probable_JJ words_NNS of_IN each_DT topic_NN and_CC the_DT artificial_JJ representative_NN words_NNS of_IN each_DT class_NN label_NN into_IN vectors_NNS ._.
When_WRB calculating_VBG the_DT distance_NN between_IN the_DT topic_NN and_CC each_DT class_NN label_NN ,_, we_PRP first_RB calculate_VBP the_DT average_JJ distance_NN between_IN each_DT probable_JJ word_NN of_IN this_DT topic_NN between_IN every_DT representative_NN word_NN of_IN some_DT class_NN label_NN ,_, then_RB the_DT sum_NN of_IN all_PDT the_DT average_JJ distance_NN between_IN each_DT prominent_JJ word_NN of_IN the_DT topic_NN and_CC all_DT representative_JJ words_NNS of_IN this_DT class_NN label_NN is_VBZ the_DT distance_NN between_IN the_DT topic_NN and_CC the_DT class_NN label_NN ._.
After_IN all_PDT the_DT distance_NN between_IN the_DT topic_NN and_CC all_PDT the_DT class_NN labels_NNS calculated_VBN ,_, we_PRP assign_VBP the_DT topic_NN to_TO the_DT class_NN label_NN which_WDT is_VBZ closest_JJS to_TO it_PRP in_IN distance_NN ._.
When_WRB assigning_VBG the_DT topic_NN to_TO the_DT class_NN label_NN ,_, we_PRP do_VBP not_RB need_VB a_DT human_JJ annotator_NN to_TO do_VB this_DT directly_RB ,_, it_PRP can_MD avoid_VB the_DT negligence_NN which_WDT may_MD be_VB made_VBN by_IN the_DT human_JJ annotator_NN ._.
And_CC on_IN the_DT premise_NN of_IN keeping_VBG the_DT word_NN order_NN in_IN document_NN ,_, we_PRP introduce_VBP the_DT bigram_NN grammar_NN ._.
We_PRP use_VBP a_DT random_JJ bigram_NN status_NN variable_JJ which_WDT denotes_VBZ whether_IN a_DT bigram_NN grammar_NN could_MD be_VB formed_VBN between_IN the_DT current_JJ word_NN and_CC the_DT previous_JJ one_CD ._.
If_IN they_PRP could_MD ,_, the_DT previous_JJ word_NN will_MD directly_RB impact_VB on_IN choosing_VBG a_DT topic_NN for_IN the_DT current_JJ word_NN ._.
And_CC if_IN not_RB ,_, choosing_VBG a_DT topic_NN for_IN the_DT current_JJ word_NN is_VBZ independent_JJ ._.
3_CD LDA_NNP LDA_NNP is_VBZ an_DT unsupervised_JJ generative_JJ probabilistic_JJ model_NN for_IN collections_NNS of_IN discrete_JJ data_NNS such_JJ as_IN text_NN documents_NNS ._.
It_PRP is_VBZ widely_RB used_VBN to_TO discover_VB latent_NN semantic_JJ structure_NN of_IN a_DT document_NN collection_NN by_IN modeling_NN words_NNS in_IN the_DT documents_NNS ._.
LDA_NNP models_NNS each_DT of_IN the_DT documents_NNS as_IN a_DT mixture_NN over_IN some_DT latent_NN topics_NNS ,_, each_DT of_IN which_WDT describes_VBZ a_DT multinomial_JJ distribution_NN over_IN a_DT word_NN vocabulary_NN ._.
The_DT generative_JJ process_NN of_IN LDA_NNP can_MD be_VB described_VBN as_IN follows_VBZ :_: 1_LS ._.
for_IN each_DT topic_NN t_NN ,_, draw_VB a_DT distribution_NN over_IN words_NNS :_: Φt_JJ ~_NN Dirichlet_NNP -LRB-_-LRB- β_FW -RRB-_-RRB- 2_CD ._.
for_IN each_DT document_NN d_SYM ε_FW D_NNP a._FW draw_VB a_DT vector_NN of_IN topic_NN proportions_NNS :_: θd_JJ ~_NN Dirichlet_NNP -LRB-_-LRB- α_FW -RRB-_-RRB- b._NN for_IN each_DT word_NN w_NN at_IN position_NN n_NN in_IN d_LS i._FW draw_VB a_DT topic_NN assignment_NN :_: zd_VB ,_, n_VB ~_SYM Multinomial_NNP -LRB-_-LRB- θd_VBN -RRB-_-RRB- ii_FW ._.
draw_VB a_DT word_NN :_: wd_VB ,_, n_VB ~_SYM Multinomial_NNP -LRB-_-LRB- Φz_NNP -RRB-_-RRB- Where_WRB ,_, Φt_NNP is_VBZ the_DT word_NN probabilities_NNS for_IN topic_NN t_NN ,_, θd_NN is_VBZ the_DT topic_NN probability_NN distribution_NN ,_, zd_NN ,_, n_NN is_VBZ topic_NN assignment_NN and_CC wd_NN ,_, n_NN is_VBZ word_NN assignment_NN for_IN nth_JJ word_NN position_NN in_IN document_NN d_LS respectively_RB ._.
α_NN and_CC β_NN are_VBP topic_NN and_CC word_NN Dirichlet_NNP priors_NNS ._.
The_DT key_JJ problem_NN in_IN LDA_NNP is_VBZ posterior_JJ inference_NN ._.
It_PRP involves_VBZ the_DT inference_NN of_IN the_DT hidden_JJ topic_NN structure_NN given_VBN the_DT observed_JJ documents_NNS ._.
Direct_NNP and_CC exact_JJ estimation_NN of_IN posterior_NN inference_NN is_VBZ intractable_JJ ._.
In_IN this_DT paper_NN we_PRP estimate_VBP approximate_JJ posterior_NN inference_NN using_VBG collapsed_VBD Gibbs_NNP sampling_NN -LRB-_-LRB- Griffiths_NNP and_CC Steyvers_NNP et_FW al._FW ,_, 2004_CD -RRB-_-RRB- ._.
The_DT Gibbs_NNP sampling_NN equation_NN used_VBN to_TO update_VB the_DT assignmentofatopicttothewordwεWatthe_NNP position_NN n_NN in_IN document_NN d_LS ,_, conditioned_VBN on_IN α_NN and_CC β_NN is_VBZ :_: ∝_NN -LRB-_-LRB- 1_CD -RRB-_-RRB- vεW_NN -LRB-_-LRB- nv_FW ,_, t_VB +_SYM βv_FW -RRB-_-RRB- −_SYM 1_CD Where_WRB ,_, is_VBZ the_DT count_NN of_IN the_DT word_NN w_NN assigned_VBN to_TO the_DT topic_NN t_NN ,_, ,_, is_VBZ the_DT count_NN of_IN the_DT topic_NN t_NN assigned_VBN to_TO words_NNS in_IN the_DT document_NN d_LS and_CC W_NNP is_VBZ the_DT vocabulary_NN of_IN the_DT corpus_NN ._.
We_PRP use_VBP a_DT subscript_JJ d_LS ,_, ¬_CD n_NN to_TO denote_VB the_DT current_JJ token_JJ ,_, ,_, is_VBZ ignored_VBN in_IN the_DT Gibbs_NNP sampling_NN update_VBP ._.
After_IN performing_VBG collapsed_VBD Gibbs_NNP sampling_VBG using_VBG equation_NN 1_CD ,_, probability_NN of_IN the_DT word_NN w_NN assigned_VBN to_TO the_DT topic_NN t_NN and_CC the_DT probability_NN of_IN the_DT topic_NN t_NN assigned_VBN to_TO the_DT document_NN d_LS is_VBZ estimated_VBN as_IN :_: knowledge_NN from_IN the_DT text_NN ._.
The_DT words_NNS of_IN a_DT document_NN are_VBP independent_JJ of_IN each_DT other_JJ ,_, assigning_VBG a_DT topic_NN for_IN the_DT current_JJ word_NN do_VBP not_RB depend_VB on_IN the_DT previous_JJ word_NN and_CC the_DT current_JJ word_NN do_VBP not_RB affect_VB assigning_VBG a_DT topic_NN for_IN the_DT next_JJ word_NN ._.
The_DT generative_JJ process_NN of_IN each_DT word_NN is_VBZ a_DT independent_JJ ._.
In_IN this_DT paper_NN ,_, the_DT improved_VBN algorithm_NN VB-LDA_NN considers_VBZ the_DT word_NN order_NN ,_, not_RB a_DT pure_JJ bag-of-words_NNS model_NN ._.
When_WRB assigning_VBG a_DT topic_NN for_IN the_DT word_NN of_IN a_DT document_NN ,_, we_PRP introduce_VBP the_DT bigram_NN grammar_NN ._.
We_PRP introduce_VBP a_DT bigram_NN status_NN variable_JJ x_LS -LRB-_-LRB- X._NNP Wang_NNP and_CC A._NNP McCallum_NNP et_FW al._FW ,_, 2007_CD -RRB-_-RRB- between_IN two_CD words_NNS which_WDT are_VBP adjacent_JJ ._.
If_IN x_LS =_SYM 1_CD ,_, they_PRP form_VBP a_DT bigram_NN grammar_NN ,_, if_IN x_LS =_SYM 0_CD ,_, they_PRP do_VBP not_RB ._.
What_WP it_PRP is_VBZ different_JJ from_IN LDA_NNP is_VBZ that_IN the_DT production_NN of_IN every_DT word_NN of_IN a_DT document_NN is_VBZ not_RB independent_JJ ,_, it_PRP may_MD depend_VB on_IN the_DT topic_NN probability_NN distribution_NN of_IN the_DT previous_JJ word_NN or_CC affect_VB the_DT generative_JJ process_NN of_IN next_JJ word_NN ._.
Therefor_RB ,_, the_DT generation_NN of_IN a_DT word_NN is_VBZ not_RB only_RB dependent_JJ on_IN the_DT topic_NN probability_NN distribution_NN ,_, but_CC also_RB the_DT random_JJ Bernoulli_NNP distribution_NN which_WDT decides_VBZ the_DT current_JJ word_NN and_CC the_DT previous_JJ one_CD whether_IN to_TO form_VB a_DT bigram_NN grammar_NN or_CC a_DT unigram_NN grammar_NN ._.
The_DT graphical_JJ model_NN presentation_NN of_IN the_DT model_NN which_WDT is_VBZ introduced_VBN the_DT bigram_NN grammar_NN is_VBZ shown_VBN in_IN Figure_NN 1_CD ._.
Where_WRB T_NNP is_VBZ the_DT number_NN of_IN topics_NNS in_IN the_DT documents_NNS ,_, W_NNP is_VBZ the_DT number_NN of_IN unique_JJ words_NNS ,_, θ_NN is_VBZ the_DT multinomial_JJ distribution_NN of_IN topics_NNS in_IN a_DT document_NN ,_, zi_FW is_VBZ the_DT topic_NN associated_VBN with_IN the_DT ith_NN token_JJ in_IN a_DT document_NN ,_, xi_NN is_VBZ the_DT bigram_NN status_NN between_IN the_DT -LRB-_-LRB- i_FW −_FW 1_LS -RRB-_-RRB- th_JJ token_JJ and_CC ith_NN token_JJ in_IN a_DT document_NN ,_, wi_NN is_VBZ the_DT ith_NN token_JJ in_IN a_DT document_NN ,_, Φ_NNP is_VBZ the_DT multinomial_JJ unigram_NN distribution_NN of_IN words_NNS ._.
φ_NN is_VBZ the_DT binomial_JJ -LRB-_-LRB- Bernoulli_NNP -RRB-_-RRB- distribution_NN of_IN bigram_NN status_NN variables_NNS x_LS ,_, σ_NN is_VBZ the_DT multinomial_JJ bigram_NN distribution_NN of_IN words_NNS ,_, α_FW 、_FW β_FW 、_FW γ_FW 、_FW δ_FW are_VBP the_DT Dirichlet_NNP priors_NNS of_IN θ_FW 、_FW Φ_FW 、_FW φ_FW 、_FW σ_FW respectively_RB ._.
The_DT generative_JJ process_NN of_IN this_DT model_NN can_MD be_VB described_VBN as_IN follows_VBZ :_: 1_LS ._.
draw_NN Discrete_NNP distributions_NNS Φz_VBP from_IN a_DT Dirichlet_NNP prior_JJ β_NN for_IN each_DT topic_NN z_SYM :_: P_NN zd_NN ,_, n_NN =_SYM t_FW zd_FW ,_, −_CD n_NN ,_, wd_NN ,_, n_NN =_SYM w_FW ,_, αt_NN ,_, βw_FW nw_FW ,_, t_VB +_SYM βw_SYM −_SYM 1_CD ×_CD -LRB-_-LRB- nt_NN ,_, d_LS +_SYM αt_SYM −_SYM 1_LS -RRB-_-RRB- Φw_NNP ,_, t_NN =_SYM nw_FW ,_, t_FW +_FW βw_FW vεW_FW -LRB-_-LRB- nv_NN ,_, t_NN +_SYM βw_FW -RRB-_-RRB- θ_FW =_SYM nt_NN ,_, d_LS +_SYM αt_FW t_FW ,_, d_LS Ti_NNP =_SYM 1_CD -LRB-_-LRB- n_NN i_FW ,_, d_LS +_SYM α_FW i_FW -RRB-_-RRB- 4_CD VB-LDA_NNP -LRB-_-LRB- 2_LS -RRB-_-RRB- -LRB-_-LRB- 3_LS -RRB-_-RRB- In_IN this_DT paper_NN ,_, we_PRP introduce_VBP the_DT word_NN vector_NN and_CC the_DT bigram_NN grammar_NN based_VBN on_IN LDA_NNP ,_, so_IN the_DT algorithm_NN is_VBZ called_VBN VB-LDA_NNP ._.
4.1_CD Bigram_NNP grammar_NN The_DT LDA_NNP does_VBZ not_RB take_VB the_DT word_NN order_NN into_IN account_NN ,_, it_PRP is_VBZ a_DT typical_JJ bag-of-words_NNS model_NN ._.
Due_JJ to_TO not_RB considering_VBG the_DT structural_JJ information_NN inherent_JJ in_IN the_DT text_NN ,_, it_PRP could_MD not_RB help_VB tap_VB deep_JJ Figure_NN 1_CD ._.
the_DT topic_NN model_NN with_IN bigram_JJ grammar_NN Φ_NNP ~_NNP Dirichlet_NNP -LRB-_-LRB- β_FW -RRB-_-RRB- z_SYM 2_CD ._.
draw_NN Bernoulli_NNP distributions_NNS φzw_VBP from_IN a_DT Beta_JJ prior_JJ γ_NN for_IN each_DT topic_NN z_SYM and_CC each_DT word_NN w_NN :_: φzw_VB ~_NN Beta_NN -LRB-_-LRB- γ_FW -RRB-_-RRB- 3_LS ._.
draw_NN Discrete_NNP distributions_NNS σzw_VBP from_IN a_DT Dirichlet_NNP prior_JJ δ_NN for_IN each_DT topic_NN z_SYM and_CC each_DT word_NN w_NN :_: σzw_VB ~_SYM Dirichlet_NNP -LRB-_-LRB- δ_FW -RRB-_-RRB- 4_LS ._.
for_IN each_DT document_NN d_LS :_: ×_FW a._FW draw_VB a_DT Discrete_JJ distribution_NN θd_NN from_IN a_DT Dirichlet_NNP prior_JJ α_NN :_: i_FW i_FW −_FW 1_CD i_FW −_FW 1_CD i_FW n_FW z_SYM di_FW w_FW di_FW +_FW β_FW w_FW d_SYM −_SYM 1_CD i_FW Wv_FW =_SYM 1_CD n_NN z_SYM di_FW v_FW +_FW β_FW v_FW −_FW 1_LS mzdwd_FW wd_FW +_FW δwd_FW −_FW 1_CD i_FW i_FW −_FW 1_CD i_FW i_FW Wv_FW =_SYM 1_CD m_NN z_SYM d_LS w_FW d_LS +_SYM δ_FW v_FW −_FW 1_CD i_FW i_FW −_FW 1_CD i_FW i_FW f_LS x_LS di_FW =_SYM 0_CD -LRB-_-LRB- 4_LS -RRB-_-RRB- i_FW f_LS x_LS di_FW =_SYM 1_CD -LRB-_-LRB- 5_CD -RRB-_-RRB- θd_FW ~_FW Dirichlet_NNP -LRB-_-LRB- α_FW -RRB-_-RRB- b._NN for_IN each_DT word_NN wd_NN in_IN document_NN d_LS :_: i_FW i._FW draw_NN xid_NN from_IN Bernoulli_NNP φzd_NNP wd_VBD i_FW −_FW 1_CD i_FW −_FW 1_CD xid_NN ~_NN Bernoulli_NNP -LRB-_-LRB- φzd_FW wd_FW -RRB-_-RRB- i_FW −_FW 1_CD i_FW −_FW 1_CD ii_FW ._.
draw_NN zid_NN from_IN Discrete_NNP θd_NN :_: zid_NN ~_NN Dirichlet_NNP -LRB-_-LRB- θd_VBN -RRB-_-RRB- :_: where_WRB nzw_NN represents_VBZ how_WRB many_JJ times_NNS word_NN w_NN is_VBZ assigned_VBN into_IN topic_NN z_SYM as_IN a_DT unigram_NN ,_, mzwv_NN represents_VBZ how_WRB many_JJ times_NNS word_NN v_NN is_VBZ assigned_VBN to_TO topic_NN z_SYM as_IN the_DT 2th_JJ term_NN of_IN a_DT bigram_NN given_VBN the_DT previous_JJ word_NN w_NN ,_, pzwk_NN denotes_NNS how_WRB many_JJ times_NNS the_DT status_NN variable_JJ x_LS =_SYM k_NN -LRB-_-LRB- 0_CD or_CC 1_CD -RRB-_-RRB- given_VBN the_DT previous_JJ word_NN w_NN and_CC the_DT previous_JJ word_NN 's_POS topic_NN z_SYM ,_, and_CC qdz_NN represents_VBZ how_WRB many_JJ times_NNS a_DT word_NN is_VBZ assigned_VBN to_TO topic_NN z_SYM in_IN document_NN d._NN Note_VB all_DT counts_NNS here_RB do_VBP include_VB the_DT assignment_NN of_IN the_DT token_JJ being_VBG visited_VBN ._.
Simple_NN manipulations_NNS give_VBP us_PRP the_DT posterior_NN estimates_NNS of_IN θ_NN 、_CD Φ_NNP 、_CD φ_NN and_CC σ_NN as_IN follows_VBZ :_: θd_VB =_SYM qdz_FW +_FW αz_FW -LRB-_-LRB- 6_CD -RRB-_-RRB- Φ_NNP =_SYM nzw_FW +_FW βw_FW -LRB-_-LRB- 7_CD -RRB-_-RRB- z_SYM Tt_NNP =_SYM 1_CD -LRB-_-LRB- q_NN d_LS t_FW +_FW α_FW t_FW -RRB-_-RRB- z_SYM w_FW Wv_FW =_SYM 1_CD -LRB-_-LRB- n_NN z_SYM v_FW +_FW β_FW w_FW -RRB-_-RRB- iii_FW ._.
draw_NN wid_NN from_IN xid_JJ =_SYM 1_CD :_: Discrete_JJ σz_NN d_LS w_FW d_LS i_FW i_FW −_FW 1_CD if_IN wid_NN ~_NN Multinomial_NNP -LRB-_-LRB- σzdwd_VBN -RRB-_-RRB- i_FW i_FW −_FW 1_CD wid_NN ~_NN Multinomial_NNP -LRB-_-LRB- Φzdi_NNP -RRB-_-RRB- We_PRP use_VBP Gibbs_NNP sampling_VBG to_TO conduct_VB approximate_JJ inference_NN in_IN this_DT paper_NN ._.
To_TO reduce_VB the_DT uncertainty_NN introduced_VBN by_IN θ_NN 、_CD Φ_NNP 、_CD φ_NN and_CC σ_NN ,_, we_PRP could_MD integrate_VB them_PRP out_RP with_IN no_DT trouble_NN because_IN of_IN the_DT conjugate_JJ prior_JJ setting_NN in_IN our_PRP$ model_NN ._.
Starting_VBG from_IN the_DT joint_JJ distribution_NN P_NNP -LRB-_-LRB- w_NN ,_, z_SYM ,_, x_LS |_SYM α_SYM 、_FW β_FW 、_FW γ_FW 、_FW δ_FW -RRB-_-RRB- ,_, we_PRP can_MD work_VB out_RP the_DT conditional_JJ probabilities_NNS P_NNP -LRB-_-LRB- zd_JJ ,_, xd_JJ |_NN zd_NN ,_, i_FW i_FW −_FW i_FW xd_FW ,_, w_NN ,_, α_FW 、_FW β_FW 、_FW γ_FW 、_FW δ_FW -RRB-_-RRB- conveniently_RB using_VBG Bayes_NNP rule_NN ,_, −_CD i_FW where_WRB zd_JJ denotes_NNS the_DT topic_NN assignments_NNS for_IN all_DT −_FW i_FW word_NN tokens_NNS except_IN word_NN wd_NN ,_, and_CC xd_NN represents_VBZ i_FW −_FW i_FW the_DT bigram_NN status_NN for_IN all_DT tokens_NNS except_IN word_NN wid_NN ._.
During_IN Gibbs_NNP sampling_NN ,_, we_PRP draw_VBP the_DT topic_NN assignment_NN zid_NN and_CC the_DT bigram_NN status_NN xid_NN iteratively_RB for_IN each_DT word_NN token_JJ wid_NN according_VBG to_TO the_DT following_VBG conditional_JJ probability_NN distribution_NN :_: p_NN -LRB-_-LRB- zd_JJ ,_, xd_JJ |_NN zd_NN ,_, xd_NN ,_, w_NN ,_, α_FW 、_FW β_FW 、_FW γ_FW 、_FW δ_FW -RRB-_-RRB- i_FW i_FW −_FW i_FW −_FW i_FW ∝_FW -LRB-_-LRB- γxd_FW +_FW pzd_FW wd_FW x_SYM −_SYM 1_LS -RRB-_-RRB- -LRB-_-LRB- αzdi_FW +_FW qdzd_FW −_FW 1_LS -RRB-_-RRB- else_RB draw_VB wid_NN from_IN Discrete_NNP Φzdi_NNP :_: φ_VB =_SYM γk_FW +_FW pzwk_FW zwk_FW 1k_FW =_SYM 0_CD -LRB-_-LRB- γk_FW +_FW pzwk_FW -RRB-_-RRB- δ_FW =_SYM δv_FW +_FW mzwv_FW zwv_FW Wv_FW =_SYM 1_CD -LRB-_-LRB- δv_FW +_FW mzwv_FW -RRB-_-RRB- -LRB-_-LRB- 8_CD -RRB-_-RRB- -LRB-_-LRB- 9_CD -RRB-_-RRB- class_NN label_NN c._NN dt_NN ,_, c_NN =_SYM Ri_FW =_SYM 1di_FW ,_, p_NN -LRB-_-LRB- R_NN is_VBZ the_DT number_NN of_IN the_DT prominent_JJ words_NNS of_IN a_DT topic_NN -RRB-_-RRB- Then_RB we_PRP can_MD calculate_VB the_DT distance_NN of_IN the_DT topic_NN t_NN and_CC each_DT class_NN label_NN ,_, choosing_VBG one_CD which_WDT is_VBZ closest_JJS to_TO the_DT topic_NN t_NN for_IN it_PRP ._.
In_IN this_DT paper_NN ,_, the_DT algorithm_NN can_MD be_VB described_VBN as_IN follows_VBZ :_: Input_NNP :_: the_DT Dirichlet_NNP priors_NNS α_SYM 、_FW β_FW 、_FW γ_FW 、_FW δ_FW ,_, the_DT number_NN of_IN topics_NNS T_NNP ,_, the_DT document_NN corpus_VBZ D_NNP and_CC the_DT max_NN iteration_NN M_NNP 4.2_CD Assign_NNP topic_NN '_'' class_NN label_NN In_IN the_DT LDA_NNP model_NN ,_, after_IN performing_VBG collapsed_VBD Gibbs_NNP sampling_NN ,_, we_PRP can_MD get_VB the_DT probable_JJ words_NNS of_IN each_DT topic_NN by_IN the_DT multinomial_JJ distribution_NN of_IN the_DT topic_NN over_IN the_DT word_NN vocabulary_NN ._.
Then_RB the_DT human_JJ annotator_NN can_MD assign_VB the_DT class_NN label_NN for_IN each_DT topic_NN with_IN its_PRP$ probable_JJ words_NNS ._.
While_IN in_IN this_DT paper_NN ,_, we_PRP do_VBP not_RB need_VB the_DT human_JJ annotator_NN to_TO do_VB this_DT ,_, because_IN the_DT human_JJ annotator_NN may_MD make_VB some_DT carelessness_NN which_WDT may_MD affect_VB the_DT performance_NN of_IN the_DT algorithm_NN ._.
We_PRP first_RB create_VBP a_DT set_NN of_IN artificial_JJ representative_NN words_NNS for_IN each_DT class_NN label_NN ,_, then_RB get_VB the_DT prominent_JJ words_NNS through_IN performing_VBG collapsed_VBD Gibbs_NNP sampling_NN ._.
And_CC we_PRP use_VBP the_DT word2vec_CD -LRB-_-LRB- Tomas_NNP Mikolov_NNP et_FW al._FW ,_, 2013_CD -RRB-_-RRB- to_TO translate_VB the_DT words_NNS in_IN the_DT documents_NNS into_IN word_NN vectors_NNS ,_, then_RB we_PRP find_VBP out_RP the_DT word_NN vectors_NNS of_IN the_DT artificial_JJ representative_NN words_NNS for_IN the_DT class_NN labels_NNS and_CC the_DT prominent_JJ words_NNS for_IN the_DT topics_NNS respectively_RB ._.
We_PRP introduce_VBP Vit_NNP to_TO represent_VB the_DT word_NN vector_NN of_IN the_DT ith_NN prominent_JJ word_NN of_IN the_DT topic_NN t_NN and_CC Vjc_NNP to_TO represent_VB the_DT word_NN vector_NN of_IN the_DT jth_NN representative_NN word_NN of_IN the_DT class_NN label_NN c_NN ._.
We_PRP use_VBP di_FW ,_, j_NN to_TO denote_VB the_DT geometric_JJ distance_NN between_IN the_DT word_NN vector_NN Vit_NNP and_CC Vjc_NNP ,_, and_CC dt_NN ,_, c_NN to_TO denote_VB the_DT distance_NN between_IN the_DT topic_NN t_NN and_CC the_DT class_NN label_NN c_NN ._.
Their_PRP$ distance_NN is_VBZ calculated_VBN as_IN follows_VBZ :_: 1_LS ._.
calculate_VB the_DT average_JJ distance_NN di_FW ,_, p_NN between_IN the_DT ith_NN prominent_JJ word_NN of_IN the_DT topic_NN t_NN and_CC each_DT representative_NN word_NN of_IN the_DT class_NN label_NN c_NN :_: di_FW ,_, p_VB =_SYM Nj_NNP =_SYM 1_CD di_FW ,_, j_VBP N_NNP -LRB-_-LRB- N_NNP is_VBZ the_DT number_NN of_IN the_DT representative_JJ words_NNS of_IN a_DT class_NN label_NN -RRB-_-RRB- 2_CD ._.
the_DT sum_NN of_IN the_DT average_JJ distance_NN between_IN each_DT prominent_JJ word_NN of_IN the_DT topic_NN t_NN and_CC the_DT class_NN label_NN c_NN is_VBZ the_DT distance_NN dt_NN ,_, c_NN between_IN the_DT topic_NN t_NN and_CC the_DT 1_CD ._.
2_LS ._.
3_LS ._.
4_LS ._.
5_CD ._.
6_CD ._.
7_CD ._.
8_CD ._.
9_CD ._.
10_CD ._.
11_CD ._.
12_CD ._.
13_CD ._.
14_CD ._.
15_CD ._.
16_CD ._.
17_CD ._.
18_CD ._.
19_CD ._.
20_CD ._.
Initialize_VB count_NN variables_NNS in_IN Equation_NN 4_CD and_CC 5_CD to_TO 0_CD ;_: Initialize_VB the_DT topic_NN assignments_NNS for_IN all_PDT the_DT words_NNS in_IN the_DT corpus_NN D_NNP for_IN iter_NN from_IN 1to_CD M_NNP do_VB foreach_VB document_NN d_LS in_IN the_DT corpus_NN D_NNP do_VBP foreach_JJ word_NN i_FW in_IN the_DT document_NN d_LS do_VBP Exclude_VB word_NN i_FW and_CC its_PRP$ assigned_VBN topic_NN from_IN the_DT count_NN variables_NNS ;_: calculate_VB the_DT xi_NNS for_IN the_DT word_NN i_FW and_CC the_DT word_NN -LRB-_-LRB- i-1_JJ -RRB-_-RRB- using_VBG Equation_NN 8_CD ;_: if_IN -LRB-_-LRB- xi_FW =_SYM =_SYM 1_LS -RRB-_-RRB- then_RB sample_NN new_JJ topic_NN for_IN word_NN i_FW using_VBG Equation_NN 5_CD ;_: update_VB qdz_NN ,_, pzw_NN 1_CD ,_, mzwv_NN using_VBG the_DT new_JJ topic_NN z_SYM for_IN word_NN i_FW ;_: end_NN if_IN -LRB-_-LRB- xi_FW =_SYM =_SYM 0_CD -RRB-_-RRB- then_RB sample_NN new_JJ topic_NN for_IN word_NN i_FW using_VBG Equation_NN 4_CD ;_: update_VB qdz_NN ,_, pzw_NN 0_CD ,_, nzw_NN using_VBG the_DT new_JJ topic_NN z_SYM for_IN word_NN i_FW ;_: end_NN end_NN Update_NNP the_DT posterior_NN estimates_VBZ for_IN θdz_NN using_VBG Equation_NN 6_CD ;_: end_NN Update_NNP the_DT posterior_NN estimates_VBZ for_IN Φzw_NNP φzwk_NN and_CC δzwv_NN using_VBG Equation_NNP 7,8_CD and_CC 9_CD ;_: end_NN 21_CD ._.
Get_VB each_DT document_NN '_'' mixture_NN distribution_NN θ_NN over_IN T_NNP latent_NN topics_NNS and_CC each_DT topic_NN '_'' prominent_JJ words_NNS ,_, then_RB we_PRP create_VBP a_DT set_NN of_IN artificial_JJ representative_NN words_NNS for_IN each_DT class_NN label_NN ._.
22_CD ._.
Translate_VB the_DT prominent_JJ words_NNS of_IN a_DT topic_NN and_CC the_DT representative_JJ words_NNS of_IN a_DT class_NN label_NN into_IN word_NN vectors_NNS respectively_RB using_VBG word2vec_CD -LRB-_-LRB- Tomas_NNP Mikolov_NNP et_FW al._FW ,_, 2013_CD -RRB-_-RRB- ._.
23_CD ._.
Assign_VB a_DT class_NN label_NN for_IN the_DT topic_NN t_NN -LRB-_-LRB- t_SYM ∈_SYM -LSB-_SYM 1_CD ,_, T_NNP -RSB-_NNP -RRB-_-RRB- by_IN calculating_VBG the_DT distance_NN between_IN the_DT topic_NN t_NN and_CC each_DT class_NN label_NN ._.
24_CD ._.
Corresponding_VBG to_TO the_DT topic_NN proportions_NNS θd_VBP in_IN document_NN d_LS ,_, we_PRP can_MD find_VB out_RP the_DT topic_NN z_SYM which_WDT has_VBZ the_DT largest_JJS proportion_NN in_IN document_NN ,_, so_IN the_DT class_NN label_NN c_NN which_WDT is_VBZ assigned_VBN to_TO the_DT topic_NN z_SYM is_VBZ what_WP the_DT document_NN d_LS belongs_VBZ to_TO ._.
2_LS ._.
SRAA_NNP :_: Simulated/Real/Aviation_NN /_CD Auto_NN UseNet_NNP data_NNS 2_CD :_: This_DT dataset_NN contains_VBZ 73,218_CD UseNet_NNP articles_NNS from_IN four_CD discussion_NN groups_NNS ,_, for_IN simulated_JJ auto_NN racing_NN -LRB-_-LRB- sim_auto_JJ -RRB-_-RRB- ,_, simulated_JJ aviation_NN -LRB-_-LRB- sim_aviation_NN -RRB-_-RRB- ,_, real_JJ autos_NNS -LRB-_-LRB- real_auto_JJ -RRB-_-RRB- ,_, real_JJ aviation_NN -LRB-_-LRB- real_aviation_NN -RRB-_-RRB- ._.
Following_VBG are_VBP the_DT three_CD classification_NN tasks_NNS associated_VBN with_IN this_DT dataset_NN ._.
1_LS ._.
sim_auto_JJ vs_NNS sim_aviation_NN vs_VBZ rea_lauto_JJ vs_NNS real_aviation_NN 2_CD ._.
auto_NN -LRB-_-LRB- sim_auto_JJ +_NN real_auto_NN -RRB-_-RRB- vs_NNS aviation_NN -LRB-_-LRB- sim_aviation_JJ +_NN real_aviation_NN -RRB-_-RRB- 3_LS ._.
simulated_JJ -LRB-_-LRB- sim_auto_JJ +_NN sim_aviation_NN -RRB-_-RRB- vs_FW real_JJ -LRB-_-LRB- real_auto_JJ +_NN real_aviation_NN -RRB-_-RRB- 3_LS ._.
WebKB_NNP :_: The_DT WebKB_NNP dataset_NN contains_VBZ 8145_CD web_NN pages_NNS gathered_VBN from_IN university_NN computer_NN science_NN departments_NNS ._.
The_DT task_NN is_VBZ to_TO classify_VB the_DT webpages_NNS as_IN student_NN ,_, course_NN ,_, faculty_NN or_CC project_NN ._.
We_PRP randomly_RB split_VBD SRAA_NNP and_CC WebKB_NNP datasets_VBZ such_JJ that_IN 80_CD %_NN is_VBZ used_VBN as_IN training_NN data_NNS and_CC remaining_VBG 20_CD %_NN is_VBZ used_VBN as_IN test_NN data_NNS ._.
We_PRP preprocess_VBP these_DT datasets_NNS by_IN removing_VBG HTML_NNP tags_NNS and_CC stop-words_NNS ._.
For_IN various_JJ subsets_NNS of_IN the_DT 20Newsgroups_NNS and_CC WebKB_NNP datasets_VBZ discussed_VBN above_IN ,_, we_PRP choose_VBP number_NN of_IN topics_NNS as_IN twice_RB the_DT number_NN of_IN classes_NNS ._.
For_IN SRAA_NNP dataset_NN we_PRP infer_VBP 8_CD topics_NNS on_IN the_DT training_NN dataset_NN and_CC label_NN these_DT 8_CD topics_NNS for_IN all_PDT the_DT three_CD classification_NN tasks_NNS ._.
The_DT Dirichlet_NNP priorsα_NN 、_NN β_SYM 、_FW γ_FW 、_FW δ_FW were_VBD chosen_VBN to_TO be_VB 50/T_JJ ,_, 0.01_CD ,_, 0.1_CD and_CC 0.01_CD respectively_RB ._.
The_DT number_NN of_IN the_DT topic_NN '_'' prominent_JJ words_NNS and_CC the_DT class_NN '_'' representative_JJ words_NNS were_VBD all_DT set_VBN to_TO be_VB 20_CD ._.
We_PRP can_MD obtain_VB the_DT representative_NN words_NNS of_IN a_DT class_NN label_NN as_IN follows_VBZ :_: setting_VBG the_DT number_NN of_IN topics_NNS to_TO be_VB 1_CD ,_, after_IN performing_VBG collapsed_VBD Gibbs_NNP sampling_NN ,_, the_DT topic_NN '_'' prominent_JJ words_NNS are_VBP this_DT class_NN '_'' representative_JJ words_NNS ._.
5.2_CD Results_NNS and_CC analysis_NN Table_NNP 1_CD shows_VBZ the_DT experimental_JJ results_NNS ._.
We_PRP can_MD observe_VB that_IN ,_, VB-LDA_NNP almost_RB performs_VBZ better_JJR 5_CD ._.
Experimental_JJ Evaluation_NN We_PRP determine_VBP the_DT effectiveness_NN of_IN our_PRP$ algorithm_NN in_IN relation_NN to_TO ClassifyLDA_NNP -LRB-_-LRB- Hingmire_NNP et_FW al._FW ,_, 2013_CD -RRB-_-RRB- algorithm_NN on_IN the_DT same_JJ experimental_JJ datasets_NNS ._.
We_PRP evaluate_VBP and_CC compare_VBP our_PRP$ text_NN classification_NN algorithm_NN by_IN computing_VBG Macro_NNP averaged_VBD F1_CD ._.
As_IN the_DT inference_NN of_IN LDA_NNP is_VBZ approximate_JJ ,_, we_PRP repeat_VBP all_PDT the_DT experiments_NNS for_IN each_DT dataset_NN ten_CD times_NNS and_CC report_NN average_JJ Macro-F1_NN ._.
We_PRP also_RB learn_VBP supervised_JJ SVM_NNP classifier_NN for_IN each_DT dataset_NN using_VBG topics_NNS as_IN features_NNS and_CC report_NN average_JJ Macro-F1_NN ._.
5.1_CD Datasets_NNPS We_PRP evaluate_VBP the_DT effectiveness_NN of_IN VB-LDA_NNP and_CC ClassifyLDA_NNP and_CC SVM_NNP on_IN following_VBG three_CD real_JJ world_NN text_NN classification_NN datasets_NNS ._.
1_LS ._.
20Newsgroup_NNP :_: This_DT dataset_NN contains_VBZ messages_NNS across_IN twenty_CD newsgroups_NNS ._.
In_IN our_PRP$ experiments_NNS ,_, we_PRP use_VBP bydate_JJ version_NN of_IN the_DT 20Newsgroup_JJ dataset_NN ._.
This_DT version_NN of_IN the_DT dataset_NN is_VBZ divided_VBN into_IN training_NN -LRB-_-LRB- 60_CD %_NN -RRB-_-RRB- and_CC test_NN -LRB-_-LRB- 40_CD %_NN -RRB-_-RRB- datasets_NNS which_WDT are_VBP grouped_VBN into_IN 6_CD major_JJ categories_NNS ._.
We_PRP construct_VBP classifiers_NNS on_IN training_NN datasets_NNS and_CC evaluate_VB them_PRP on_IN test_NN datasets_NNS ._.
Dataset_NNP Text_NNP classification_NN -LRB-_-LRB- Macro-F1_NNP -RRB-_-RRB- #_# Topics_NNPS ClassifyLDA_NNP VB-LDA_NNP SVM_NNP 20Newsgroups_NNS comp_VBP vs_NNS politics_NNS 4_CD 0.962_CD 0.985_CD 0.983_CD religion_NN vs_NNS sports_NNS 4_CD 0.899_CD 0.903_CD 0.907_CD politics_NNS vs_FW religion_NN 4_CD 0.875_CD 0.889_CD 0.891_CD comp_NN vs_NNS religion_NN vs_VBZ sports_NNS 6_CD 0.908_CD 0.935_CD 0.939_CD comp_NN vs_NNS religion_NN vs_VBZ politics_NNS 6_CD 0.884_CD 0.929_CD 0.944_CD comp_NN vs_NNS religion_NN vs_VBZ sports_NNS vs_NNS politics_NNS 8_CD 0.835_CD 0.888_CD 0.912_CD SRAA_NNP sim_auto_JJ vs_NNS sim_aviation_NN vs_VBZ real_auto_JJ vs_NNS real_aviation_NN 8_CD 0.747_CD 0.768_CD 0.813_CD auto_NN vs_NNS aviation_NN 8_CD 0.918_CD 0.930_CD 0.933_CD simulated_JJ vs_NNS real_JJ 8_CD 0.916_CD 0.921_CD 0.928_CD WebKB_NNP WebKB_NNP 8_CD 0.653_CD 0.687_CD 0.725_CD Table_NNP 1_CD :_: Experimental_JJ results_NNS of_IN document_NN classification_NN on_IN various_JJ datasets_NNS For_IN the_DT religion_NN vs_VBZ sports_NNS dataset_NN and_CC the_DT simulated_JJ vs_NNS real_JJ dataset_NN VB-LDA_NNP and_CC ClassifyLDA_NNP almost_RB have_VBP the_DT same_JJ performance_NN ._.
What_WP is_VBZ more_JJR ,_, the_DT increase_NN of_IN Macro-F1_NN from_IN ClassifyLDA_NNP to_TO VB-LDA_NNP for_IN comp_NN vs_NNS religion_NN vs_VBZ politics_NNS is_VBZ more_JJR than_IN for_IN comp_NN vs_NNS politics_NNS ,_, religion_NN vs_NNS sports_NNS and_CC politics_NNS vs_FW religion_NN ,_, and_CC the_DT increase_NN of_IN Macro-F1_NN from_IN ClassifyLDA_NNP to_TO VB-LDA_NNP for_IN comp_NN vs_NNS religion_NN vs_VBZ sports_NNS vs_NNS politics_NNS is_VBZ more_JJR than_IN for_IN comp_NN vs_NNS religion_NN vs_VBZ politics_NNS and_CC comp_NN vs_NNS religion_NN vs_VBZ sports_NNS ._.
Therefor_RB we_PRP can_MD see_VB that_IN our_PRP$ algorithm_NN performs_VBZ better_JJR when_WRB the_DT classes_NNS is_VBZ more_RBR ._.
We_PRP can_MD also_RB observe_VB that_IN ,_, performance_NN of_IN VB-LDA_NNP is_VBZ close_JJ to_TO supervised_JJ SVM_NNP in_IN the_DT 10_CD subsets_NNS ._.
That_DT is_VBZ to_TO say_VB ,_, VB-LDA_NNP can_MD perform_VB well_RB without_IN labeled_VBN documents_NNS ._.
Table_NNP 2_CD shows_VBZ the_DT topic_NN '_'' most_RBS prominent_JJ words_NNS and_CC the_DT class_NN which_WDT the_DT topic_NN belongs_VBZ to_TO in_IN the_DT politics_NNS vs_VBP religion_NN subset_NN ._.
We_PRP can_MD observe_VB that_IN ,_, a_DT human_JJ annotator_NN is_VBZ very_RB hard_JJ to_TO assign_VB the_DT class_NN politics_NNS or_CC religion_NN to_TO the_DT topic_NN 1_CD ,_, but_CC in_IN this_DT paper_NN ,_, we_PRP can_MD accurately_RB calculate_VB the_DT distance_NN between_IN the_DT topic_NN 1_CD and_CC the_DT class_NN politics_NNS and_CC religion_NN which_WDT are_VBP 3.49_CD and_CC 3.47_CD ,_, so_RB the_DT class_NN religion_NN is_VBZ assigned_VBN for_IN the_DT topic_NN 1_CD ._.
Table_NNP 3_CD shows_VBZ the_DT distance_NN between_IN comp_NN ,_, politics_NNS ,_, religion_NN and_CC sports_NNS ._.
We_PRP can_MD observe_VB that_IN ,_, the_DT distance_NN between_IN comp_NN and_CC politics_NNS is_VBZ clearly_RB longer_JJR than_IN religion_NN and_CC sports_NNS ,_, politics_NNS and_CC religion_NN ,_, so_IN it_PRP is_VBZ relatively_RB easy_JJ to_TO distinguish_VB between_IN class_NN comp_NN and_CC politics_NNS and_CC their_PRP$ Macro-F1_NNP is_VBZ higher_JJR than_IN the_DT other_JJ two_CD susets_NNS ._.
We_PRP also_RB observe_VBP that_IN the_DT distance_NN between_IN religion_NN and_CC sports_NNS is_VBZ longer_JJR than_IN politics_NNS and_CC religion_NN and_CC the_DT Macro-F1_NNP in_IN religion_NN vs_NNS sports_NNS subset_NN is_VBZ a_DT little_RB higher_JJR than_IN the_DT other_JJ subset_NN ._.
ID_NNP Most_JJS prominent_JJ words_NNS in_IN the_DT topic_NN Class_NN -LRB-_-LRB- politics_NNS /_VBP religion_NN -RRB-_-RRB- 0_CD people_NNS gun_NN know_VBP government_NN president_NN file_NN guns_NNS fire_VBP state_NN weapons_NNS politics_NNS 1_CD people_NNS know_VBP make_VB evidence_NN argument_NN well_RB things_NNS question_VBP wrong_JJ moral_JJ religion_NN 2_CD god_NN jesus_NN christian_JJ bible_JJ people_NNS church_NN christians_NNS time_NN life_NN know_VBP religion_NN 3_CD israel_NN turkish_JJ people_NNS armenian_JJ war_NN israeli_NNS armenians_NNS government_NN turks_NNS turkey_NN politics_NNS Table_NNP 2_CD :_: Topic_NNP assigning_VBG in_IN the_DT politics_NNS vs_VBP religion_NN subset_NN class_NN distance_NN comp_NN politics_NNS 5.72_CD religion_NN sports_NNS 4.83_CD politics_NNS religion_NN 4.38_CD Table_NNP 3_CD :_: the_DT distance_NN between_IN each_DT class_NN 6_CD ._.
Conclusions_NNS In_IN this_DT paper_NN we_PRP propose_VBP a_DT novel_NN text_NN classification_NN algorithm_NN based_VBN on_IN LDA_NNP ._.
In_IN this_DT algorithm_NN ,_, when_WRB assigning_VBG the_DT class_NN label_NN to_TO the_DT topic_NN ,_, without_IN a_DT human_JJ annotator_NN to_TO do_VB this_DT ,_, we_PRP use_VBP the_DT tool_NN word2vec_CD to_TO translate_VB the_DT topic_NN '_'' prominent_JJ words_NNS and_CC the_DT class_NN '_'' representative_JJ words_NNS into_IN vectors_NNS ,_, then_RB calculate_VBP the_DT distance_NN between_IN topic_NN t_NN and_CC each_DT of_IN class_NN label_NN to_TO find_VB out_RP the_DT class_NN label_NN which_WDT the_DT topic_NN t_NN belongs_VBZ to_TO ._.
We_PRP also_RB keep_VBP the_DT word_NN order_NN in_IN a_DT document_NN and_CC introduce_VB the_DT bigram_NN grammar_NN which_WDT needs_VBZ a_DT bigram_NN status_NN variable_JJ x_LS to_TO denote_VB whether_IN the_DT two_CD words_NNS adjacent_JJ to_TO be_VB a_DT bigram_NN or_CC not_RB ._.
This_DT algorithm_NN reduces_VBZ the_DT need_NN to_TO label_VB a_DT large_JJ collection_NN of_IN documents_NNS ._.
The_DT results_NNS in_IN experiments_NNS shows_VBZ that_IN the_DT approach_NN can_MD yield_VB performance_NN comparable_JJ to_TO entirely_RB supervised_JJ method_NN SVM_NNP ._.
In_IN this_DT paper_NN ,_, the_DT datasets_NNS in_IN our_PRP$ experiments_NNS is_VBZ English_NNP ,_, in_IN the_DT future_NN ,_, we_PRP will_MD do_VB some_DT research_NN in_IN Chinese_JJ datasets_NNS ._.
References_NNS David_NNP M._NNP Blei_NNP ,_, Andrew_NNP Y._NNP Ng_NNP ,_, and_CC Michael_NNP I._NNP Jordan_NNP ._.
2003_CD ._.
Latent_NN Dirichlet_NNP Allocation_NNP ._.
The_DT Journal_NNP of_IN Machine_NNP Learning_NNP Research_NNP ,_, 3:993_CD --_: 1022_CD ,_, March_NNP ._.
David_NNP M._NNP Blei_NNP and_CC Jon_NNP D._NNP McAuliffe_NNP ._.
2007_CD ._.
Supervised_VBN Topic_NNP Models_NNPS ._.
In_IN NIPS_NNP ._.
Simon_NNP Lacoste-Julien_NNP ,_, Fei_NNP Sha_NNP ,_, and_CC Michael_NNP I._NNP Jordan_NNP ._.
2008_CD ._.
DiscLDA_NNP :_: Discriminative_JJ Learning_NNP for_IN Dimensionality_NNP Reduction_NNP and_CC Classification_NNP ._.
In_IN NIPS_NNP ._.
Jun_NNP Zhu_NNP ,_, Amr_NNP Ahmed_NNP ,_, and_CC Eric_NNP P._NNP Xing_NNP ._.
2009_CD ._.
MedLDA_NNP :_: Maximum_NNP Margin_NN Supervised_VBN Topic_NNP Models_NNPS for_IN Regression_NNP and_CC Classification_NNP ._.
In_IN ICML_NNP ,_, pages_NNS 1257_CD --_: 1264_CD ._.
Kamal_NNP Nigam_NNP ,_, Andrew_NNP Kachites_NNP McCallum_NNP ,_, Sebastian_NNP Thrun_NNP ,_, and_CC Tom_NNP Mitchell_NNP ._.
2000_CD ._.
Text_NN Classification_NN from_IN Labeled_VBN and_CC Unlabeled_NNP Documents_NNS using_VBG EM_NNP ._.
Machine_NN Learning-Special_JJ issue_NN on_IN information_NN retrieval_NN ,_, 39_CD -LRB-_-LRB- 2-3_CD -RRB-_-RRB- ,_, May-June_NNP ._.
A._NN Mccallum_NNP and_CC K._NNP Nigam_NNP ._.
Text_NN classi_cation_NN by_IN bootstrapping_VBG with_IN keywords_NNS ,_, EM_NNP and_CC shrinkage_NN ._.
In_IN ACL-99_NNP Workshop_NNP for_IN Unsupervised_NNP Learning_NNP in_IN Natural_NNP Language_NNP Processing_NNP ,_, pages_NNS 52-58_CD ,_, 1999_CD ._.
Bing_NNP Liu_NNP ,_, Xiaoli_NNP Li_NNP ,_, Wee_NNP Sun_NNP Lee_NNP ,_, and_CC Philip_NNP S._NNP Yu_NNP ._.
2004_CD ._.
Text_NN Classification_NN by_IN Labeling_VBG Words_NNS ._.
In_IN Proceedings_NNP of_IN the_DT 19th_JJ national_JJ conference_NN on_IN Artifical_JJ intelligence_NN ,_, pages_NNS 425_CD --_: 430_CD ._.
Gregory_NNP Druck_NNP ,_, Gideon_NNP Mann_NNP ,_, and_CC Andrew_NNP McCallum_NNP ._.
2008_CD ._.
Learning_NNP from_IN Labeled_NNP Features_VBZ using_VBG Generalized_NNP Expectation_NNP criteria_NNS ._.
In_IN SIGIR_NNP ,_, pages_NNS 595_CD --_: 602_CD ._.
Swapnil_NNP Hingmire_NNP ,_, Sandeep_NNP Chougule_NNP ,_, Girish_NNP K._NNP Palshikar_NNP ,_, and_CC Sutanu_NNP Chakraborti_NNP ._.
2013_CD ._.
Document_NNP Classification_NNP by_IN Topic_NNP Labeling_VBG ._.
In_IN SIGIR_NNP ,_, pages_NNS 877_CD --_: 880_CD ._.
Swapnil_NNP Hingmire_NNP ,_, Sutanu_NNP Chakraborti.Sprinkling_NNP Topics_NNPS for_IN Weakly_NNP Supervised_NNP Text_NNP Classification_NNP -LSB-_NNP J_NNP -RSB-_NNP ._.
In_IN ACL_NNP ,2014_CD :_: 55-66_CD ._.
T._NNP L._NNP Gri_ths_NNP and_CC M._NNP Steyvers_NNP ._.
Finding_VBG scientic_JJ topics_NNS ._.
PNAS_NNP ,_, 101_CD -LRB-_-LRB- suppl_NN ._.
1_LS -RRB-_-RRB- :5228_CD -5235_CD ,_, April_NNP 2004_CD ._.
X._NNP Wang_NNP ,_, A._NN McCallum_NNP ,_, and_CC X._NNP Wei_NNP ._.
Topical_JJ N-Grams_NNS :_: Phrase_NN and_CC topic_NN discovery_NN ,_, with_IN an_DT application_NN to_TO Information_NNP Retrieval_NNP ._.
In_IN Proc_NNP ._.
of_IN ICDM_NNP ,_, pages_NNS 697_CD --_: 702_CD ,_, 2007_CD ._.
Tomas_NNP Mikolov_NNP ,_, Kai_NNP Chen_NNP ,_, Greg_NNP Corrado_NNP ,_, Jeffrey_NNP Dean_NNP ._.
Efficient_JJ Estimation_NN of_IN Word_NNP Representations_NNPS in_IN Vector_NNP Space.arXiv_NNP :1301.3781_CD ,_, 2013_CD ._.
