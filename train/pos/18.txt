Unsupervised_JJ and_CC Lightly_RB Supervised_VBN Part-of-Speech_NNP Tagging_NNP Using_VBG Recurrent_NNP Neural_NNP Networks_NNP Abstract_NNP In_IN this_DT paper_NN ,_, we_PRP propose_VBP a_DT novel_NN approach_NN to_TO induce_VB automatically_RB a_DT Part-Of-Speech_NNP -LRB-_-LRB- POS_NNP -RRB-_-RRB- tagger_NN for_IN resource-poor_JJ languages_NNS -LRB-_-LRB- languages_NNS that_WDT have_VBP no_DT labeled_VBN training_NN data_NNS -RRB-_-RRB- ._.
This_DT ap_SYM -_: proach_NN is_VBZ based_VBN on_IN cross-language_JJ projection_NN of_IN linguistic_JJ annotations_NNS from_IN parallel_JJ cor_NN -_: pora_NN without_IN the_DT use_NN of_IN word_NN alignment_NN infor_NN -_: mation_NN ._.
Our_PRP$ approach_NN does_VBZ not_RB assume_VB any_DT knowledge_NN about_IN foreign_JJ languages_NNS ,_, making_VBG it_PRP applicable_JJ to_TO a_DT wide_JJ range_NN of_IN resource-poor_JJ languages_NNS ._.
We_PRP use_VBP Recurrent_JJ Neural_NNP Net_NN -_: works_NNS -LRB-_-LRB- RNNs_NNS -RRB-_-RRB- as_IN multilingual_JJ analysis_NN tool_NN ._.
Our_PRP$ approach_NN combined_VBN with_IN a_DT basic_JJ cross_NN -_: lingual_NN projection_NN method_NN -LRB-_-LRB- using_VBG word_NN align_SYM -_: ment_NN information_NN -RRB-_-RRB- achieves_VBZ comparable_JJ re_SYM -_: sults_NNS to_TO the_DT state-of-the-art_JJ ._.
We_PRP also_RB use_VBP our_PRP$ approach_NN in_IN a_DT weakly_RB supervised_VBN context_NN ,_, and_CC it_PRP shows_VBZ an_DT excellent_JJ potential_NN for_IN very_RB low_JJ -_: resource_NN settings_NNS -LRB-_-LRB- less_JJR than_IN 1k_CD training_NN utter_JJ -_: ances_NNS -RRB-_-RRB- ._.
1_CD Introduction_NNP Nowadays_NNP ,_, Natural_NNP Language_NNP Processing_NNP -LRB-_-LRB- NLP_NNP -RRB-_-RRB- tools_NNS -LRB-_-LRB- part-of-speech_JJ tagger_NN ,_, sense_NN tagger_NN ,_, syntactic_JJ parser_NN ,_, named_VBN entity_NN recognizer_NN ,_, semantic_JJ role_NN la_NNP -_: beler_NN ,_, etc._FW -RRB-_-RRB- with_IN the_DT best_JJS performance_NN are_VBP those_DT built_VBN using_VBG supervised_JJ learning_VBG approaches_NNS for_IN resource_NN -_: rich_JJ languages_NNS -LRB-_-LRB- where_WRB manually_RB annotated_VBN corpora_NN are_VBP available_JJ -RRB-_-RRB- such_JJ as_IN English_NNP ,_, French_NNP ,_, German_NNP ,_, Chi_NNP -_: nese_NN and_CC Arabic_NNP ._.
However_RB ,_, for_IN a_DT large_JJ number_NN of_IN resource-poor_JJ languages_NNS ,_, annotated_JJ corpora_NN do_VBP not_RB exist_VB ._.
Their_PRP$ manual_NN construction_NN is_VBZ labor_NN intensive_JJ and_CC very_RB expensive_JJ ,_, making_VBG supervised_JJ approaches_NNS not_RB feasible_JJ ._.
The_DT availability_NN of_IN parallel_JJ corpora_NN has_VBZ recently_RB led_VBN to_TO several_JJ strands_NNS of_IN research_NN work_NN exploring_VBG the_DT use_NN of_IN unsupervised_JJ approaches_NNS based_VBN on_IN lin_NN -_: guistic_JJ annotations_NNS projection_NN from_IN the_DT -LRB-_-LRB- resource_NN -_: rich_JJ -RRB-_-RRB- source_NN language_NN to_TO the_DT -LRB-_-LRB- under-resourced_JJ -RRB-_-RRB- tar_NN -_: get_VB language_NN ._.
The_DT goal_NN of_IN cross-language_JJ projec_NN -_: tion_NN is_VBZ ,_, on_IN the_DT one_CD hand_NN ,_, to_TO provide_VB all_DT languages_NNS with_IN linguistic_JJ annotations_NNS ,_, and_CC on_IN the_DT other_JJ hand_NN ,_, to_TO automatically_RB induce_VB NLP_NNP tools_NNS for_IN these_DT lan_SYM -_: guages_NNS ._.
Unfortunately_RB ,_, the_DT state-of-the-art_JJ in_IN un_NN -_: supervised_JJ methods_NNS ,_, is_VBZ still_RB quite_RB far_RB from_IN super_JJ -_: vised_VBN learning_VBG approaches_NNS ._.
For_IN example_NN ,_, Petrov_NNP et_FW al._FW -LRB-_-LRB- 2012_CD -RRB-_-RRB- obtained_VBD an_DT average_JJ accuracy_NN of_IN 95.2_CD %_NN for_IN 22_CD resource-rich_JJ languages_NNS supervised_VBD POS_NNP tag_NN -_: gers_NNS ,_, while_IN the_DT state-of-the-art_JJ in_IN the_DT unsupervised_JJ POS_NNP taggers_NNS achieved_VBN by_IN Das_NNP and_CC Petrov_NNP -LRB-_-LRB- 2011_CD -RRB-_-RRB- and_CC Duong_NNP et_FW al._FW -LRB-_-LRB- 2013_CD -RRB-_-RRB- with_IN an_DT average_JJ accuracy_NN reaches_VBZ only_RB 83.4_CD %_NN on_IN 8_CD European_JJ languages_NNS ._.
Sec_SYM -_: tion_NN 2_CD presents_VBZ a_DT brief_JJ overview_NN of_IN related_JJ work_NN ._.
In_IN this_DT paper_NN ,_, we_PRP first_RB adapt_VBP a_DT similar_JJ method_NN than_IN the_DT one_CD of_IN Duong_NNP et_FW al._FW -LRB-_-LRB- 2013_CD -RRB-_-RRB- 1_CD ,_, to_TO build_VB an_DT unsu_JJ -_: pervised_VBN POS_NNP tagger_NN based_VBN on_IN a_DT simple_JJ cross-lingual_JJ projection_NN -LRB-_-LRB- Section_NN 3.1_CD -RRB-_-RRB- ._.
Next_JJ ,_, we_PRP explore_VBP the_DT possi_NNS -_: bility_NN of_IN using_VBG a_DT recurrent_JJ neural_JJ network_NN -LRB-_-LRB- RNN_NNP -RRB-_-RRB- to_TO induce_VB multilingual_JJ NLP_NNP tools_NNS ,_, without_IN using_VBG word_NN alignment_NN information_NN ._.
To_TO show_VB the_DT potential_NN of_IN our_PRP$ approach_NN ,_, we_PRP firstly_RB investigate_VBP POS_NNP tagging_NN ._.
In_IN our_PRP$ approach_NN ,_, a_DT parallel_JJ corpus_NN between_IN a_DT resource-rich_JJ language_NN -LRB-_-LRB- having_VBG a_DT POS_NNP tagger_NN -RRB-_-RRB- and_CC a_DT lower-resourced_JJ language_NN is_VBZ used_VBN to_TO extract_VB a_DT com_NN -_: mon_NN words_NNS representation_NN -LRB-_-LRB- cross-lingual_JJ words_NNS rep_NN -_: resentation_NN -RRB-_-RRB- based_VBN only_RB on_IN sentence_NN level_NN alignment_NN ._.
This_DT representation_NN is_VBZ used_VBN with_IN the_DT source_NN side_NN of_IN the_DT parallel_JJ corpus_NN -LRB-_-LRB- tagged_VBN corpus_NN -RRB-_-RRB- to_TO learn_VB a_DT neural_JJ network_NN POS_NNP tagger_NN for_IN the_DT source_NN language_NN ._.
No_DT 1We_JJ did_VBD not_RB use_VB incremental_JJ training_NN -LRB-_-LRB- as_IN Duong_NNP et_FW al._FW -LRB-_-LRB- 2013_CD -RRB-_-RRB- did_VBD -RRB-_-RRB- ._.
word_NN alignment_NN information_NN is_VBZ needed_VBN in_IN our_PRP$ ap_SYM -_: proach_NN ._.
Based_VBN on_IN this_DT common_JJ representation_NN of_IN source_NN and_CC target_NN words_NNS ,_, this_DT neural_JJ network_NN POS_NNP tagger_NN can_MD also_RB be_VB used_VBN to_TO tag_VB target_NN language_NN text_NN -LRB-_-LRB- Section_NN 3.2_CD -RRB-_-RRB- ._.
We_PRP assume_VBP that_IN these_DT two_CD models_NNS -LRB-_-LRB- baseline_NN cross_NN -_: lingual_NN projection_NN and_CC RNN_NNP -RRB-_-RRB- are_VBP complementary_JJ to_TO each_DT other_JJ -LRB-_-LRB- one_CD relies_VBZ on_IN word-alignment_NN informa_NN -_: tion_NN while_IN the_DT other_JJ does_VBZ not_RB -RRB-_-RRB- ,_, and_CC the_DT performance_NN can_MD be_VB further_JJ improved_VBN by_IN combining_VBG them_PRP -LRB-_-LRB- linear_JJ combination_NN presented_VBN in_IN Section_NN 3.3_CD -RRB-_-RRB- ._.
This_DT unsu_JJ -_: pervised_VBD RNN_NNP model_NN ,_, obtained_VBN without_IN any_DT target_NN language_NN annotated_VBN data_NNS ,_, can_MD be_VB easily_RB adapted_VBN in_IN a_DT weakly_RB supervised_VBN manner_NN -LRB-_-LRB- if_IN a_DT small_JJ amount_NN of_IN an_DT -_: notated_JJ target_NN data_NNS is_VBZ available_JJ -RRB-_-RRB- in_IN order_NN to_TO take_VB into_IN account_NN the_DT target_NN language_NN specificity_NN -LRB-_-LRB- Section_NN 4_CD -RRB-_-RRB- ._.
To_TO evaluate_VB our_PRP$ approach_NN ,_, we_PRP conducted_VBD an_DT exper_NN -_: iment_NN ,_, which_WDT consists_VBZ of_IN two_CD parts_NNS ._.
First_RB ,_, using_VBG only_RB parallel_JJ corpora_NN ,_, we_PRP evaluate_VBP our_PRP$ unsupervised_JJ ap_SYM -_: proach_NN for_IN 4_CD languages_NNS :_: French_JJ ,_, German_JJ ,_, Greek_JJ and_CC Spanish_JJ ._.
Secondly_RB ,_, the_DT performance_NN of_IN our_PRP$ approach_NN is_VBZ evaluated_VBN for_IN German_JJ in_IN a_DT weakly_RB supervised_VBN con_NN -_: text_NN ,_, using_VBG several_JJ amounts_NNS of_IN target_NN adaptation_NN data_NNS -LRB-_-LRB- Section_NN 5_CD -RRB-_-RRB- ._.
Finally_RB ,_, Section_NNP 6_CD concludes_VBZ our_PRP$ study_NN and_CC presents_VBZ our_PRP$ future_JJ work_NN ._.
2_CD Related_JJ Work_NN Several_JJ studies_NNS have_VBP used_VBN cross-lingual_JJ projection_NN to_TO transfer_VB linguistic_JJ annotations_NNS from_IN a_DT resource_NN -_: rich_JJ language_NN to_TO a_DT resource-poor_JJ language_NN in_IN order_NN to_TO train_VB NLP_NNP tools_NNS for_IN the_DT target_NN language_NN ._.
The_DT pro-_JJ jection_NN approach_NN has_VBZ been_VBN successfully_RB used_VBN to_TO trans_NNS -_: fer_VB several_JJ linguistic_JJ annotations_NNS between_IN languages_NNS ._.
Examples_NNS include_VBP POS_NNP -LRB-_-LRB- Yarowsky_NNP et_FW al._FW ,_, 2001_CD ;_: Das_NNP and_CC Petrov_NNP ,_, 2011_CD ;_: Duong_NNP et_FW al._FW ,_, 2013_CD -RRB-_-RRB- ,_, named_VBN entity_NN -LRB-_-LRB- Kim_NNP and_CC Lee_NNP ,_, 2012_CD -RRB-_-RRB- ,_, syntactic_JJ constituent_NN -LRB-_-LRB- Jiang_NNP et_FW al._FW ,_, 2011_CD -RRB-_-RRB- ,_, word_NN senses_NNS -LRB-_-LRB- Bentivogli_NNP et_FW al._FW ,_, 2004_CD ;_: Van_NNP der_NNP Plas_NNP and_CC Apidianaki_NNP ,_, 2014_CD -RRB-_-RRB- ,_, and_CC semantic_JJ role_NN labeling_VBG -LRB-_-LRB- Pado_NNP ́_CD ,_, 2007_CD ;_: Annesi_NNP and_CC Basili_NNP ,_, 2010_CD -RRB-_-RRB- ._.
In_IN these_DT approaches_NNS ,_, the_DT source_NN language_NN is_VBZ tagged_VBN ,_, and_CC tags_NNS are_VBP projected_VBN from_IN the_DT source_NN lan_NN -_: guage_NN to_TO the_DT target_NN language_NN through_IN the_DT use_NN of_IN word_NN alignments_NNS in_IN parallel_JJ corpora_NN ._.
Then_RB ,_, these_DT par_NN -_: tial_JJ noisy_JJ annotations_NNS can_MD be_VB used_VBN in_IN conjunction_NN with_IN robust_JJ learning_NN algorithms_NNS to_TO build_VB unsuper_JJ -_: vised_VBN NLP_NNP tools_NNS ._.
One_CD limitation_NN of_IN these_DT approaches_NNS is_VBZ due_JJ to_TO the_DT poor_JJ accuracy_NN of_IN word-alignment_NN algo_NN -_: rithms_NNS ,_, and_CC also_RB to_TO the_DT weak_JJ or_CC incomplete_JJ inherent_JJ match_NN between_IN the_DT two_CD sides_NNS of_IN a_DT bilingual_JJ corpus_NN -LRB-_-LRB- the_DT alignment_NN is_VBZ not_RB only_RB a_DT one-to-one_JJ mapping_NN ,_, it_PRP can_MD also_RB be_VB one-to-many_JJ ,_, many-to-one_NN ,_, many-to_NN -_: many_JJ or_CC some_DT words_NNS can_MD remain_VB unaligned_JJ -RRB-_-RRB- ._.
To_TO deal_VB with_IN these_DT limitations_NNS ,_, recent_JJ studies_NNS have_VBP pro-_JJ posed_VBN to_TO combine_VB projected_VBN labels_NNS with_IN partially_RB su_SYM -_: pervised_JJ monolingual_JJ information_NN in_IN order_NN to_TO filter_NN out_RP invalid_JJ label_NN sequences_NNS ._.
For_IN example_NN ,_, Li_NNP et_FW al._FW -LRB-_-LRB- 2012_CD -RRB-_-RRB- ,_, Ta_NNP ̈ckstro_NN ̈m_NN et_FW al._FW -LRB-_-LRB- 2013b_JJ -RRB-_-RRB- and_CC Wisniewski_NNP et_FW al._FW -LRB-_-LRB- 2014_CD -RRB-_-RRB- have_VBP proposed_VBN to_TO improve_VB projection_NN per_IN -_: formance_NN by_IN using_VBG a_DT dictionary_NN of_IN valid_JJ tags_NNS for_IN each_DT word_NN -LRB-_-LRB- coming_VBG from_IN Wiktionary_JJ 2_CD -RRB-_-RRB- ._.
In_IN another_DT vein_NN ,_, various_JJ studies_NNS based_VBN on_IN cross_NN -_: lingual_NN representation_NN learning_VBG methods_NNS have_VBP pro-_JJ posed_VBN to_TO avoid_VB using_VBG such_JJ pre-processed_JJ and_CC noisy_JJ alignments_NNS for_IN label_NN projection_NN ._.
First_RB ,_, these_DT approaches_NNS learn_VBP language-independent_JJ features_NNS ,_, across_IN many_JJ different_JJ languages_NNS -LRB-_-LRB- Al-Rfou_NNP et_FW al._FW ,_, 2013_CD -RRB-_-RRB- ._.
Then_RB ,_, the_DT induced_JJ representation_NN space_NN is_VBZ used_VBN to_TO train_VB NLP_NNP tools_NNS by_IN exploiting_VBG labeled_VBN data_NNS from_IN the_DT source_NN language_NN and_CC apply_VB them_PRP in_IN the_DT tar_NN -_: get_VB language_NN ._.
To_TO induce_VB interlingual_JJ features_NNS ,_, sev_SYM -_: eral_JJ resources_NNS have_VBP been_VBN used_VBN ,_, including_VBG bilingual_JJ lexicon_NN -LRB-_-LRB- Durrett_NNP et_FW al._FW ,_, 2012_CD ;_: Gouws_NNP and_CC Søgaard_NNP ,_, 2015a_CD -RRB-_-RRB- and_CC parallel_JJ corpora_NN -LRB-_-LRB- Ta_NNP ̈ckstro_NN ̈m_NN et_FW al._FW ,_, 2013a_CD ;_: Gouws_NNP et_FW al._FW ,_, 2015b_JJ -RRB-_-RRB- ._.
Cross-lingual_JJ repre_NN -_: sentation_NN learning_VBG have_VBP achieved_VBN good_JJ results_NNS in_IN dif_NN -_: ferent_JJ NLP_NNP applications_NNS such_JJ as_IN cross-language_JJ POS_NNP tagging_NN and_CC cross-language_JJ super_JJ sense_NN -LRB-_-LRB- SuS_NNP -RRB-_-RRB- tag_NN -_: ging_NN -LRB-_-LRB- Gouws_NNP and_CC Søgaard_NNP ,_, 2015a_CD -RRB-_-RRB- ,_, cross-language_JJ named_VBN entity_NN recognition_NN -LRB-_-LRB- Ta_NNP ̈ckstro_NN ̈m_NN et_FW al._FW ,_, 2012_CD -RRB-_-RRB- ,_, cross-lingual_JJ document_NN classification_NN and_CC lexical_JJ translation_NN task_NN -LRB-_-LRB- Gouws_NNP et_FW al._FW ,_, 2015b_JJ -RRB-_-RRB- ,_, cross_VB lan_SYM -_: guage_NN dependency_NN parsing_NN -LRB-_-LRB- Durrett_NNP et_FW al._FW ,_, 2012_CD ;_: Ta_NNP ̈ckstro_NN ̈m_NN et_FW al._FW ,_, 2013a_CD ;_: Xiao_NNP and_CC Guo_NNP ,_, 2014_CD -RRB-_-RRB- and_CC cross_VB language_NN semantic_JJ role_NN labeling_VBG -LRB-_-LRB- Titov_NNP and_CC Klementiev_NNP ,_, 2012_CD -RRB-_-RRB- ._.
Our_PRP$ approach_NN described_VBD in_IN next_JJ section_NN ,_, is_VBZ inspired_VBN by_IN these_DT works_NNS since_IN we_PRP also_RB try_VBP to_TO learn_VB a_DT common_JJ language-independent_JJ feature_NN space_NN ._.
Our_PRP$ common_JJ -LRB-_-LRB- multilingual_JJ -RRB-_-RRB- representation_NN is_VBZ based_VBN on_IN the_DT occurrence_NN of_IN source_NN and_CC target_NN words_NNS in_IN a_DT parallel_JJ corpus_NN ._.
Using_VBG this_DT representation_NN ,_, we_PRP learn_VBP a_DT cross-lingual_JJ POS_NNP tagger_NN -LRB-_-LRB- multilingual_JJ POS_NNP tagger_NN if_IN a_DT multilingual_JJ parallel_NN corpus_NN is_VBZ used_VBN -RRB-_-RRB- based_VBN on_IN a_DT recurrent_JJ neural_JJ network_NN -LRB-_-LRB- RNN_NNP -RRB-_-RRB- on_IN the_DT source_NN 2_CD http://www.wiktionary.org/_NN Figure_NN 1_CD :_: Architecture_NNP of_IN the_DT recurrent_JJ neural_JJ network_NN ._.
labeled_VBN text_NN and_CC apply_VB it_PRP to_TO tag_VB target_NN language_NN text_NN ._.
We_PRP also_RB show_VBP that_IN the_DT architecture_NN proposed_VBN is_VBZ well_RB suited_VBN for_IN lightly_RB supervised_VBN training_NN -LRB-_-LRB- adaptation_NN -RRB-_-RRB- ._.
Finally_RB ,_, several_JJ works_NNS have_VBP investigated_VBN how_WRB to_TO apply_VB neural_JJ networks_NNS to_TO NLP_NNP applications_NNS -LRB-_-LRB- Ben_NNP -_: gio_NN et_FW al._FW ,_, 2006_CD ;_: Collobert_NNP and_CC Weston_NNP ,_, 2008_CD ;_: Col_NNP -_: lobert_NN et_FW al._FW ,_, 2011_CD ;_: Henderson_NNP ,_, 2004_CD ;_: Mikolov_NNP et_FW al._FW ,_, 2010_CD ;_: Federici_NNP and_CC Pirrelli_NNP ,_, 1993_CD -RRB-_-RRB- ._.
While_IN Fed_NNP -_: erici_FW and_CC Pirrelli_NNP -LRB-_-LRB- 1993_CD -RRB-_-RRB- was_VBD one_CD of_IN the_DT earliest_JJS at_IN -_: tempts_VBZ to_TO develop_VB a_DT part-of-speech_JJ tagger_NN based_VBN on_IN a_DT special_JJ type_NN of_IN neural_JJ network_NN ,_, Bengio_NNP et_FW al._FW -LRB-_-LRB- 2006_CD -RRB-_-RRB- and_CC Mikolov_NNP et_FW al._FW -LRB-_-LRB- 2010_CD -RRB-_-RRB- applied_VBD neural_JJ net_NN -_: workstobuildlanguagemodels_NNS ._.
CollobertandWe_NNP -_: ston_NN -LRB-_-LRB- 2008_CD -RRB-_-RRB- and_CC Collobert_NNP et_FW al._FW -LRB-_-LRB- 2011_CD -RRB-_-RRB- employed_VBN a_DT deep_JJ learning_NN framework_NN for_IN multi-task_JJ learning_NN including_VBG part-of-speech_NN tagging_NN ,_, chunking_NN ,_, named_VBN -_: entity_NN recognition_NN ,_, language_NN modelling_NN and_CC seman_NN -_: ticrole-labeling_NN ._.
Henderson_NNP -LRB-_-LRB- 2004_CD -RRB-_-RRB- proposedtrain_NN -_: ing_NN methods_NNS for_IN learning_VBG a_DT statistical_JJ parser_NN based_VBN on_IN neural_JJ network_NN ._.
3_CD Unsupervised_JJ Approach_NNP Overview_NNP To_TO avoid_VB projecting_VBG label_NN information_NN from_IN deter_VB -_: ministic_JJ and_CC error-prone_JJ word_NN alignments_NNS ,_, we_PRP pro-_JJ pose_VBP to_TO represent_VB the_DT bilingual_JJ word_NN alignment_NN in_IN -_: formation_NN intrinsically_RB in_IN a_DT neural_JJ network_NN architec_NN -_: ture_NN ._.
The_DT idea_NN consists_VBZ in_IN implementing_VBG a_DT neural_JJ net_NN -_: work_NN as_IN a_DT cross-lingual_JJ POS_NNP tagger_NN and_CC show_NN that_IN ,_, in_IN combination_NN with_IN a_DT simple_JJ cross-lingual_JJ projection_NN method_NN ,_, this_DT achieves_VBZ comparable_JJ results_NNS to_TO state-of_VB -_: the-art_NN unsupervised_JJ POS_NNP taggers_NNS ._.
Our_PRP$ approach_NN is_VBZ the_DT following_NN :_: we_PRP assume_VBP that_IN we_PRP have_VBP a_DT POS_NNP tagger_NN in_IN the_DT source_NN language_NN and_CC a_DT par_NN -_: allel_IN corpus_NN ._.
The_DT key_JJ idea_NN is_VBZ to_TO learn_VB a_DT bilingual_JJ neu_NN -_: ral_NN network_NN POS_NNP tagger_NN on_IN the_DT pre-annotated_JJ source_NN side_NN of_IN the_DT parallel_JJ corpus_NN ,_, and_CC to_TO use_VB it_PRP for_IN tagging_VBG target_NN text_NN ._.
Before_IN describing_VBG our_PRP$ bilingual_JJ neural_JJ network_NN POS_NNP tagger_NN ,_, we_PRP present_VBP the_DT simple_JJ cross_NN -_: lingual_NN projection_NN method_NN ,_, considered_VBN as_IN our_PRP$ base_NN -_: line_NN in_IN this_DT work_NN ._.
3.1_CD Unsupervised_JJ POS_NNP Tagger_NNP Based_VBD on_IN a_DT Simple_NN Cross-lingual_JJ Projection_NNP Our_PRP$ simple_JJ POS_NNP tagger_NN -LRB-_-LRB- described_VBN by_IN Algorithm_NNP 1_CD -RRB-_-RRB- is_VBZ close_JJ to_TO the_DT approach_NN introduced_VBN in_IN Yarowsky_NNP et_FW al._FW -LRB-_-LRB- 2001_CD -RRB-_-RRB- ._.
These_DT authors_NNS were_VBD the_DT first_JJ to_TO use_VB automatic_JJ word_NN alignments_NNS -LRB-_-LRB- from_IN a_DT bilingual_JJ parallel_NN corpus_NN -RRB-_-RRB- to_TO project_VB annotations_NNS from_IN a_DT source_NN language_NN to_TO a_DT target_NN language_NN ,_, to_TO build_VB unsupervised_JJ POS_NNP taggers_NNS ._.
The_DT algorithm_NN is_VBZ shortly_RB recalled_VBN below_IN ._.
Algorithm_NNP 1_CD :_: Simple_NN POS_NNP Tagger_NNP 1_CD :_: Tag_NN source_NN side_NN of_IN the_DT parallel_JJ corpus_NN ._.
2_CD :_: Word_NN align_IN the_DT parallel_JJ corpus_NN with_IN Giza_NNP +_CD +_NN -LRB-_-LRB- Och_NNP and_CC Ney_NNP ,_, 2000_CD -RRB-_-RRB- or_CC other_JJ word_NN alignment_NN tools_NNS ._.
3_CD :_: Project_NNP tags_NNS directly_RB for_IN 1-to-1_JJ alignments_NNS ._.
4_CD :_: For_IN many-to-one_NN mappings_NNS project_VBP the_DT tag_NN of_IN the_DT middle_JJ word_NN ._.
5_CD :_: The_DT unaligned_JJ words_NNS -LRB-_-LRB- target_NN -RRB-_-RRB- are_VBP tagged_VBN with_IN their_PRP$ most_RBS frequent_JJ associated_VBN tag_NN in_IN the_DT corpus_NN ._.
6_CD :_: Learn_VB POS_NNP tagger_NN on_IN target_NN side_NN of_IN the_DT bi-text_NN with_IN ,_, for_IN instance_NN ,_, TNT_NNP tagger_NN -LRB-_-LRB- Brants_NNPS ,_, 2000_CD -RRB-_-RRB- ._.
3.2_CD Unsupervised_JJ POS_NNP Tagger_NNP Based_VBD on_IN Recurrent_NNP Neural_NNP Network_NNP There_EX are_VBP two_CD major_JJ architectures_NNS of_IN neural_JJ net_NN -_: works_NNS :_: Feedforward_NNP -LRB-_-LRB- Bengio_NNP et_FW al._FW ,_, 2006_CD -RRB-_-RRB- and_CC Re_NNP -_: current_JJ Neural_NNP Networks_NNP -LRB-_-LRB- RNN_NNP -RRB-_-RRB- -LRB-_-LRB- Mikolov_NNP et_FW al._FW ,_, 2010_CD -RRB-_-RRB- ._.
Sundermeyer_NNP et_FW al._FW -LRB-_-LRB- 2013_CD -RRB-_-RRB- showed_VBD that_IN language_NN models_NNS based_VBN on_IN recurrent_JJ architecture_NN achieve_VB better_JJR performance_NN than_IN language_NN models_NNS based_VBN on_IN feedforward_JJ architecture_NN ._.
This_DT is_VBZ due_JJ to_TO the_DT fact_NN that_IN recurrent_JJ neural_JJ networks_NNS do_VBP not_RB use_VB a_DT context_NN of_IN limited_JJ size_NN ._.
This_DT property_NN led_VBD us_PRP to_TO use_VB ,_, in_IN our_PRP$ experiments_NNS ,_, a_DT simple_JJ recurrent_JJ architecture_NN -LRB-_-LRB- Elman_NNP ,_, 1990_CD -RRB-_-RRB- ._.
In_IN this_DT section_NN ,_, we_PRP describe_VBP in_IN detail_NN our_PRP$ method_NN for_IN building_VBG an_DT unsupervised_JJ POS_NNP tagger_NN for_IN a_DT target_NN language_NN based_VBN on_IN a_DT recurrent_JJ neural_JJ network_NN ._.
3.2.1_CD Model_NNP description_NN The_DT RNN_NNP consists_VBZ of_IN at_IN least_JJS three_CD layers_NNS :_: input_NN layer_NN in_IN time_NN t_NN is_VBZ x_LS -LRB-_-LRB- t_VBN -RRB-_-RRB- ,_, hidden_VBN layer_NN h_NN -LRB-_-LRB- t_VBN -RRB-_-RRB- -LRB-_-LRB- also_RB called_VBN context_NN layer_NN -RRB-_-RRB- ,_, and_CC output_NN layer_NN is_VBZ denoted_VBN as_IN y_NN -LRB-_-LRB- t_VBN -RRB-_-RRB- ._.
All_DT neurons_NNS of_IN the_DT input_NN layer_NN are_VBP connected_VBN to_TO ev_SYM -_: ery_NN neuron_NN of_IN hidden_JJ layer_NN by_IN weight_NN matrix_NN U_NNP and_CC W_NNP ._.
The_DT weight_NN matrix_NN V_NNP connects_VBZ all_DT neurons_NNS of_IN the_DT hidden_JJ layer_NN to_TO every_DT neuron_NN of_IN output_NN layer_NN ,_, as_IN it_PRP can_MD be_VB seen_VBN in_IN Figure_NN 1_CD ._.
In_IN our_PRP$ RNN_NNP POS_NNP tagger_NN ,_, the_DT input_NN layer_NN is_VBZ formed_VBN by_IN concatenating_VBG vector_NN representing_VBG current_JJ word_NN w_NN ,_, and_CC the_DT copy_NN of_IN the_DT hidden_JJ layer_NN at_IN previous_JJ time_NN ._.
We_PRP start_VBP by_IN associating_VBG to_TO each_DT word_NN in_IN both_DT the_DT source_NN and_CC the_DT target_NN vocabularies_NNS a_DT common_JJ vector_NN representation_NN ,_, namely_RB Vwi_NNP ,_, i_FW =_SYM 1_CD ,_, ..._: ,_, N_NNP ,_, where_WRB N_NNP is_VBZ the_DT number_NN of_IN parallel_JJ sentences_NNS -LRB-_-LRB- bi-sentences_NNS in_IN the_DT parallel_JJ corpus_NN -RRB-_-RRB- ._.
If_IN w_NN appears_VBZ in_IN i-th_JJ bi-sentence_NN of_IN the_DT parallel_JJ corpus_NN then_RB Vwi_NNP =_SYM 1_CD ._.
Therefore_RB ,_, all_DT input_NN neurons_NNS corresponding_JJ to_TO current_JJ word_NN w_NN are_VBP set_VBN to_TO 0_CD except_IN those_DT that_WDT correspond_VBP to_TO bi-sentences_JJ containing_VBG w_NN ,_, which_WDT are_VBP set_VBN to_TO 1_CD ._.
The_DT idea_NN is_VBZ that_IN ,_, in_IN general_JJ ,_, a_DT source_NN word_NN and_CC its_PRP$ target_NN translation_NN appear_VBP together_RB in_IN the_DT same_JJ bi-sentences_NNS and_CC their_PRP$ vector_NN representations_NNS are_VBP close.We_NNP can_MD then_RB use_VB the_DT RNN_NNP POS_NNP tagger_NN ,_, initially_RB trained_VBN on_IN source_NN side_NN ,_, to_TO tag_VB the_DT target_NN side_NN -LRB-_-LRB- because_IN of_IN our_PRP$ common_JJ vector_NN representation_NN -RRB-_-RRB- ._.
We_PRP also_RB use_VBP two_CD hidden_JJ layers_NNS -LRB-_-LRB- our_PRP$ prelimi_NN -_: nary_PDT experiments_NNS have_VBP shown_VBN better_JJR performance_NN than_IN one_CD hidden_JJ layer_NN -RRB-_-RRB- ,_, with_IN variable_JJ sizes_NNS -LRB-_-LRB- usually_RB 80-1024_CD neurons_NNS -RRB-_-RRB- and_CC sigmoid_JJ activation_NN function_NN ._.
These_DT hidden_JJ layers_NNS inherently_RB capture_VBP word_NN align_SYM -_: ment_NN information_NN ._.
The_DT output_NN layer_NN of_IN our_PRP$ model_NN contains_VBZ 12_CD neurons_NNS ,_, this_DT number_NN is_VBZ determined_VBN by_IN the_DT POS_NNP tagset_NN size_NN ._.
To_TO deal_VB with_IN the_DT potential_JJ mis_SYM -_: match_NN in_IN the_DT POS_NNP tagsets_NNS of_IN source_NN and_CC target_NN lan_NN -_: guages_NNS ,_, we_PRP adopted_VBD the_DT Petrov_NNP et_FW al._FW -LRB-_-LRB- 2012_CD -RRB-_-RRB- uni_SYM -_: versal_JJ tagset_NN -LRB-_-LRB- 12_CD tags_NNS common_JJ for_IN most_JJS languages_NNS -RRB-_-RRB- :_: NOUN_NN -LRB-_-LRB- nouns_NNS -RRB-_-RRB- ,_, VERB_NN -LRB-_-LRB- verbs_NNS -RRB-_-RRB- ,_, ADJ_NNP -LRB-_-LRB- adjectives_NNS -RRB-_-RRB- ,_, ADV_NNP -LRB-_-LRB- adverbs_NNS -RRB-_-RRB- ,_, PRON_NNP -LRB-_-LRB- pronouns_NNS -RRB-_-RRB- ,_, DET_NNP -LRB-_-LRB- determin_SYM -_: ers_NNS and_CC articles_NNS -RRB-_-RRB- ,_, ADP_NNP -LRB-_-LRB- prepositions_NNS and_CC postpo_NN -_: sitions_NNS -RRB-_-RRB- ,_, NUM_NNP -LRB-_-LRB- numerals_NNS -RRB-_-RRB- ,_, CONJ_NNP -LRB-_-LRB- conjunctions_NNS -RRB-_-RRB- ,_, PRT_NNP -LRB-_-LRB- particles_NNS -RRB-_-RRB- ,_, ._.
-LRB-_-LRB- punctuation_NN marks_NNS -RRB-_-RRB- and_CC X_NNP -LRB-_-LRB- all_DT other_JJ categories_NNS ,_, e.g._FW ,_, foreign_JJ words_NNS ,_, abbreviations_NNS -RRB-_-RRB- ._.
Therefore_RB ,_, each_DT output_NN neuron_NN corresponds_VBZ to_TO one_CD POS_NNP tag_NN in_IN the_DT tagset_NN ._.
The_DT softmax_JJ activation_NN func_NN -_: tion_NN is_VBZ used_VBN to_TO normalize_VB the_DT values_NNS of_IN output_NN neu_NN -_: rons_NNS to_TO sum_VB up_RP to_TO 1_CD ._.
Finally_RB ,_, the_DT current_JJ word_NN w_NN -LRB-_-LRB- in_IN input_NN -RRB-_-RRB- is_VBZ tagged_VBN with_IN most_JJS probable_JJ output_NN tag_NN ._.
3.2.2_CD Training_VBG the_DT model_NN The_DT first_JJ step_NN in_IN our_PRP$ approach_NN is_VBZ to_TO train_VB the_DT neural_JJ network_NN ,_, given_VBN a_DT parallel_JJ corpus_NN -LRB-_-LRB- training_NN corpus_NN -RRB-_-RRB- ,_, and_CC a_DT validation_NN corpus_NN -LRB-_-LRB- different_JJ from_IN train_NN data_NNS -RRB-_-RRB- in_IN the_DT source_NN language_NN ._.
In_IN typical_JJ applications_NNS ,_, the_DT source_NN language_NN is_VBZ a_DT resource-rich_JJ language_NN -LRB-_-LRB- which_WDT already_RB has_VBZ an_DT efficient_JJ POS_NNP tagger_NN -RRB-_-RRB- ._.
Before_IN train_NN -_: ing_VBG the_DT model_NN ,_, the_DT following_VBG pre-processing_JJ steps_NNS are_VBP performed_VBN :_: •_CD Source_NN side_NN of_IN training_NN corpus_NN and_CC validation_NN corpus_NN are_VBP annotated_VBN -LRB-_-LRB- using_VBG the_DT available_JJ super_JJ -_: vised_VBN POS_NNP tagger_NN -RRB-_-RRB- ._.
•_NNP Using_VBG a_DT parallel_JJ corpus_NN ,_, we_PRP build_VBP the_DT common_JJ vector_NN representations_NNS for_IN source_NN and_CC target_NN side_NN words_NNS ._.
Then_RB ,_, the_DT neural_JJ network_NN is_VBZ trained_VBN through_IN sev_SYM -_: eral_JJ epochs_NNS ._.
Algorithm_NNP 2_CD below_IN describes_VBZ one_CD train_NN -_: ing_VBG epoch_NN ._.
Algorithm_NNP 2_CD :_: Training_VBG RNN_NNP POS_NNP Tagger_NNP 1_CD :_: Initialize_VB weights_NNS with_IN Normal_JJ distribution_NN ._.
2_CD :_: Set_NNP time_NN counter_JJ t_NN =_SYM 0_CD ,_, and_CC initialize_VB state_NN of_IN the_DT neurons_NNS in_IN the_DT hidden_JJ layer_NN h_NN -LRB-_-LRB- t_VBN -RRB-_-RRB- to_TO 1_CD ._.
3_CD :_: Increase_VB time_NN counter_JJ t_NN by_IN 1_CD ._.
4_CD :_: Push_VB at_IN the_DT input_NN layer_NN w_NN -LRB-_-LRB- t_VBN -RRB-_-RRB- the_DT vector_NN representa_NN -_: tion_NN of_IN the_DT current_JJ -LRB-_-LRB- source_NN -RRB-_-RRB- word_NN of_IN training_NN corpus_NN ._.
5_CD :_: Copy_VB the_DT state_NN of_IN the_DT hidden_JJ layer_NN h_NN -LRB-_-LRB- t-1_JJ -RRB-_-RRB- to_TO the_DT input_NN layer_NN ._.
6_CD :_: Perform_VB a_DT forward_RB pass_VB to_TO obtain_VB the_DT predicted_VBN output_NN y_NN -LRB-_-LRB- t_VBN -RRB-_-RRB- ._.
7_CD :_: Compute_VB the_DT gradient_NN of_IN the_DT error_NN in_IN the_DT output_NN layer_NN eo_NN -LRB-_-LRB- t_VBN -RRB-_-RRB- =_SYM d_LS -LRB-_-LRB- t_VBN -RRB-_-RRB- −_FW y_FW -LRB-_-LRB- t_VBN -RRB-_-RRB- -LRB-_-LRB- difference_NN between_IN the_DT predicted_VBN y_NN -LRB-_-LRB- t_VBN -RRB-_-RRB- and_CC the_DT desired_VBN output_NN d_LS -LRB-_-LRB- t_VBN -RRB-_-RRB- -RRB-_-RRB- ._.
8_CD :_: Propagate_VB the_DT error_NN back_RB through_IN the_DT network_NN and_CC update_VB weights_NNS with_IN stochastic_JJ gradient_NN descent_NN us_PRP -_: ing_VBG Back-Propagation_NNP -LRB-_-LRB- BP_NNP -RRB-_-RRB- and_CC Back-Propagation_NNP -_: through-time_NN -LRB-_-LRB- BPTT_NNP -RRB-_-RRB- -LRB-_-LRB- Rumelhartet_NNP al._NNP ,_, 1985_CD -RRB-_-RRB- ._.
9_CD :_: If_IN not_RB all_DT training_NN inputs_NNS were_VBD processed_VBN ,_, go_VB to_TO 3_CD ._.
After_IN each_DT epoch_NN ,_, the_DT neural_JJ network_NN is_VBZ used_VBN to_TO tag_VB the_DT validation_NN corpus_NN ,_, then_RB the_DT result_NN is_VBZ compared_VBN with_IN the_DT result_NN of_IN the_DT supervised_JJ POS_NNP tagger_NN ,_, to_TO cal_SYM -_: culate_VB the_DT per-token_JJ accuracy_NN ._.
If_IN the_DT per-token_JJ accu_NN -_: racy_JJ increases_NNS ,_, training_NN continues_VBZ in_IN the_DT new_JJ epoch_NN ._.
Otherwise_RB ,_, the_DT learning_NN rate_NN is_VBZ halved_VBN at_IN the_DT start_NN of_IN the_DT new_JJ epoch_NN ._.
After_IN that_DT ,_, if_IN the_DT per-token_JJ accu_NN -_: racy_JJ does_VBZ not_RB increase_VB anymore_RB ,_, training_NN is_VBZ stopped_VBN to_TO prevent_VB over-fitting_JJ ._.
Generally_RB convergence_NN takes_VBZ 5_CD --_: 10_CD epochs_NNS ,_, starting_VBG with_IN a_DT learning_NN rate_NN α_FW =_SYM 0.1_CD ._.
After_IN learning_VBG the_DT model_NN ,_, step_VB 2_CD simply_RB consists_VBZ in_IN using_VBG the_DT trained_JJ model_NN as_IN a_DT target_NN language_NN POS_NNP tagger_NN -LRB-_-LRB- using_VBG our_PRP$ common_JJ vector_NN representation_NN -RRB-_-RRB- ._.
It_PRP is_VBZ important_JJ to_TO note_VB that_IN if_IN we_PRP train_VBP on_IN a_DT multilingual_JJ parallel_JJ corpus_NN with_IN N_NNP languages_NNS -LRB-_-LRB- N_NNP >_CD 2_CD -RRB-_-RRB- ,_, the_DT same_JJ trained_JJ model_NN will_MD be_VB able_JJ to_TO tag_VB all_PDT the_DT N_NNP languages_NNS ._.
Hence_RB ,_, our_PRP$ approach_NN assumes_VBZ that_IN the_DT word_NN or_CC -_: der_NN in_IN both_DT source_NN and_CC target_NN languages_NNS are_VBP simi_SYM -_: lar_NN ._.
In_IN some_DT languages_NNS such_JJ as_IN English_NNP and_CC French_NNP ,_, word_NN order_NN for_IN contexts_NNS containing_VBG nouns_NNS could_MD be_VB reversed_VBN most_JJS of_IN the_DT time_NN ._.
For_IN example_NN ,_, the_DT Euro_NNP -_: pean_NN Commission_NNP would_MD be_VB translated_VBN into_IN la_DT Com_NNP -_: mission_NN europenne_NN ._.
In_IN order_NN to_TO deal_VB with_IN the_DT word_NN order_NN constraints_NNS ,_, we_PRP combined_VBD the_DT RNN_NNP model_NN with_IN the_DT cross-lingual_JJ projection_NN model_NN ,_, and_CC we_PRP also_RB pro-_JJ pose_VBP Light_NN Supervision_NNP -LRB-_-LRB- adaptation_NN -RRB-_-RRB- of_IN RNN_NNP model_NN where_WRB a_DT few_JJ amount_NN of_IN target_NN data_NNS will_MD help_VB to_TO learn_VB the_DT word_NN order_NN -LRB-_-LRB- and_CC consequently_RB POS_NNP order_NN -RRB-_-RRB- in_IN the_DT target_NN language_NN ._.
3.3_CD Combining_VBG Simple_NN Cross-lingual_JJ Projection_NNP and_CC RNN_NNP Models_NNPS Since_IN the_DT simple_JJ cross-lingual_JJ projection_NN model_NN M1_CD and_CC RNN_NNP model_NN M2_CD use_NN different_JJ strategies_NNS for_IN POS_NNP tagging_VBG -LRB-_-LRB- TNT_NNP is_VBZ based_VBN on_IN Markov_NNP models_NNS while_IN RNN_NNP is_VBZ a_DT neural_JJ network_NN -RRB-_-RRB- ,_, we_PRP assume_VBP that_IN these_DT two_CD models_NNS are_VBP complementary_JJ ._.
In_IN addition_NN ,_, model_NN M2_CD does_VBZ not_RB implement_VB any_DT out-of-vocabulary_JJ -LRB-_-LRB- OOV_NNP -RRB-_-RRB- words_NNS processing_VBG yet_RB ._.
So_RB ,_, to_TO keep_VB the_DT benefits_NNS of_IN each_DT approach_NN ,_, we_PRP explore_VBP how_WRB to_TO combine_VB them_PRP with_IN linear_JJ interpolation_NN ._.
Formally_RB ,_, the_DT probability_NN to_TO tag_VB a_DT given_VBN word_NN w_NN is_VBZ computed_VBN as_IN PM12_NN -LRB-_-LRB- t_FW |_FW w_FW -RRB-_-RRB- =_SYM -LRB-_-LRB- μPM1_JJ -LRB-_-LRB- t_FW |_FW w_FW ,_, CM1_NNP -RRB-_-RRB- +_NN -LRB-_-LRB- 1_CD −_CD μ_NN -RRB-_-RRB- PM2_NN -LRB-_-LRB- t_FW |_FW w_FW ,_, CM2_NNP -RRB-_-RRB- -RRB-_-RRB- -LRB-_-LRB- 1_LS -RRB-_-RRB- where_WRB ,_, CM_NNP 1_CD and_CC CM_NNP 2_CD are_VBP ,_, respectively_RB the_DT context_NN of_IN w_NN considered_VBN by_IN M1_NNP and_CC M2_NNP ._.
The_DT relative_JJ impor_NN -_: tance_NN of_IN each_DT model_NN is_VBZ adjusted_VBN through_IN the_DT interpo_NN -_: lation_NN parameter_NN μ_NN ._.
The_DT word_NN w_NN is_VBZ tagged_VBN with_IN the_DT most_RBS probable_JJ tag_NN ,_, using_VBG the_DT function_NN f_LS described_VBN as_IN 4_CD f_LS -LRB-_-LRB- w_NN -RRB-_-RRB- =_SYM arg_FW max_FW -LRB-_-LRB- PM12_NN -LRB-_-LRB- t_FW |_FW w_FW -RRB-_-RRB- -RRB-_-RRB- -LRB-_-LRB- 2_CD -RRB-_-RRB- t_NN Light_NN Supervision_NNP -LRB-_-LRB- adaptation_NN -RRB-_-RRB- of_IN RNN_NNP model_NN While_IN the_DT unsupervised_JJ RNN_NNP model_NN described_VBD in_IN the_DT previous_JJ section_NN has_VBZ not_RB seen_VBN any_DT annotated_JJ data_NNS in_IN the_DT target_NN language_NN ,_, we_PRP also_RB consider_VBP the_DT use_NN of_IN a_DT small_JJ amount_NN of_IN adaptation_NN data_NNS -LRB-_-LRB- manually_RB an_DT -_: notated_VBN in_IN target_NN language_NN -RRB-_-RRB- in_IN order_NN to_TO capture_VB tar_NN -_: get_VB language_NN specificity_NN ._.
Such_JJ an_DT adaptation_NN is_VBZ per_IN -_: formed_VBN on_IN top_NN of_IN the_DT unsupervised_JJ RNN_NNP model_NN with_IN -_: out_RP retraining_VBG the_DT full_JJ model_NN ._.
The_DT full_JJ process_NN is_VBZ the_DT following_NN -LRB-_-LRB- steps_NNS 1_CD and_CC 2_CD correspond_VBP to_TO the_DT unsuper_JJ -_: vised_VBN case_NN -RRB-_-RRB- :_: 1_LS ._.
Each_DT word_NN in_IN the_DT parallel_JJ corpus_NN is_VBZ represented_VBN by_IN a_DT binary_JJ occurrence_NN vector_NN -LRB-_-LRB- same_JJ initial_JJ common_JJ vector_NN representation_NN -RRB-_-RRB- ._.
2_LS ._.
The_DT source_NN side_NN of_IN the_DT parallel_JJ corpus_NN -LRB-_-LRB- using_VBG the_DT available_JJ supervised_JJ POS_NNP tagger_NN -RRB-_-RRB- and_CC common_JJ vector_NN representation_NN of_IN words_NNS are_VBP combined_VBN to_TO train_VB the_DT RNN_NNP -LRB-_-LRB- Algorithm_NNP 2_CD -RRB-_-RRB- ._.
3_LS ._.
The_DT RNN_NNP trained_VBN is_VBZ adapted_VBN in_IN a_DT light_JJ supervi_NN -_: sion_NN manner_NN ,_, using_VBG a_DT small_JJ monolingual_JJ target_NN corpus_NN -LRB-_-LRB- manually_RB annotated_VBN -RRB-_-RRB- and_CC the_DT common_JJ vector_NN representation_NN of_IN words_NNS -LRB-_-LRB- extracted_VBN from_IN the_DT initial_JJ parallel_NN corpus_NN -RRB-_-RRB- ._.
Such_JJ an_DT approach_NN is_VBZ particularly_RB suited_VBN for_IN an_DT iter_NN -_: ative_JJ scenario_NN where_WRB a_DT user_NN would_MD post-edit_VB -LRB-_-LRB- correct_JJ -RRB-_-RRB- the_DT unsupervised_JJ POS-tagger_JJ output_NN in_IN order_NN to_TO pro-_JJ duce_NN rapidly_RB adaptation_NN data_NNS in_IN the_DT training_NN language_NN -LRB-_-LRB- light_JJ supervision_NN -RRB-_-RRB- ._.
5_CD 5.1_CD Experiments_NNS and_CC Results_NNS Data_NNS and_CC tools_NNS Initially_RB ,_, we_PRP applied_VBD our_PRP$ method_NN to_TO the_DT English_NNP --_: French_JJ language_NN pair_NN ._.
French_NNP was_VBD considered_VBN as_IN the_DT target_NN language_NN here_RB ._.
French_NNP is_VBZ certainly_RB not_RB a_DT resource-poor_JJ language_NN ,_, but_CC it_PRP was_VBD used_VBN as_IN if_IN no_DT tagger_NN was_VBD available_JJ -LRB-_-LRB- in_IN fact_NN ,_, TreeTagger_NNP -LRB-_-LRB- Schmid_NNP ,_, 1995_CD -RRB-_-RRB- ,_, a_DT supervised_JJ POS_NNP tagger_NN exists_VBZ for_IN this_DT lan_NN -_: guage_NN and_CC helps_VBZ us_PRP to_TO obtain_VB a_DT ground_NN truth_NN for_IN Model_NNP Lang_NNP ._.
French_JJ German_JJ Greek_JJ Spanish_JJ All_DT words_NNS OOV_NNP All_NNP words_NNS OOV_NNP All_NNP words_NNS OOV_NNP All_NNP words_NNS OOV_NNP Simple_NN Projection_NNP 80.3_CD %_NN 77.1_CD %_NN 78.9_CD %_NN 73_CD %_NN 77.5_CD %_NN 72.8_CD %_NN 80_CD %_NN 79.7_CD %_NN RNN-640-160_NN 78.5_CD %_NN 70_CD %_NN 76.1_CD %_NN 76.4_CD %_NN 75.7_CD %_NN 70.7_CD %_NN 78.8_CD %_NN 72.6_CD %_NN Projection_NNP +_CD RNN_NNP 84.5_CD %_NN 78.8_CD %_NN 81.5_CD %_NN 77_CD %_NN 78.3_CD %_NN 74.6_CD %_NN 83.6_CD %_NN 81.2_CD %_NN -LRB-_-LRB- Das_NNP ,_, 2011_CD -RRB-_-RRB- --_: --_: 82.8_CD %_NN --_: 82.5_CD %_NN --_: 84.2_CD %_NN --_: -LRB-_-LRB- Duong_NNP ,_, 2013_CD -RRB-_-RRB- --_: --_: 85.4_CD %_NN --_: 80.4_CD %_NN --_: 83.3_CD %_NN --_: -LRB-_-LRB- Gouws_NNP ,_, 2015a_CD -RRB-_-RRB- --_: --_: 84.8_CD %_NN --_: --_: --_: 82.6_CD %_NN --_: Table_NNP 1_CD :_: Unsupervised_JJ model_NN :_: token-level_JJ POS_NNP tagging_VBG accuracy_NN for_IN Simple_NN Projection_NNP ,_, RNN_NNP 4_CD ,_, Projection_NNP +_CD RNN_NNP and_CC methods_NNS of_IN Das_NNP &_CC Petrov_NNP -LRB-_-LRB- 2011_CD -RRB-_-RRB- ,_, Duong_NNP et_NNP al_NNP -LRB-_-LRB- 2013_CD -RRB-_-RRB- and_CC Gouws_NNP &_CC Søgaard_NNP -LRB-_-LRB- 2015_CD -RRB-_-RRB- ._.
evaluation_NN -RRB-_-RRB- ._.
To_TO train_VB the_DT RNN_NNP POS_NNP tagger_NN ,_, we_PRP used_VBD a_DT training_NN set_NN of_IN 10_CD ,_, 000_CD parallel_JJ sentences_NNS ex_FW -_: tracted_VBN from_IN the_DT ARCADE_NNP II_NNP English_NNP --_: French_JJ cor_NN -_: pus_NN -LRB-_-LRB- Veronis_NNP et_FW al._FW ,_, 2008_CD -RRB-_-RRB- ._.
Our_PRP$ validation_NN corpus_NN contains_VBZ 1000_CD English_JJ sentences_NNS -LRB-_-LRB- these_DT sentences_NNS are_VBP not_RB in_IN the_DT train_NN set_NN -RRB-_-RRB- extracted_VBN from_IN the_DT AR_NNP -_: CADE_NNP II_NNP English_NNP corpus_NN ._.
The_DT test_NN corpus_NN is_VBZ also_RB extracted_VBN from_IN the_DT ARCADE_NNP II_NNP corpus_VBZ ,_, and_CC it_PRP con_RB -_: tains_NNS 1000_CD French_JJ sentences_NNS -LRB-_-LRB- which_WDT are_VBP obviously_RB different_JJ from_IN the_DT train_NN set_NN -RRB-_-RRB- tagged_VBN with_IN the_DT French_JJ TreeTagger_NNP Toolkit_NNP -LRB-_-LRB- Schmid_NNP ,_, 1995_CD -RRB-_-RRB- and_CC manually_RB checked_VBD ._.
Encouraged_VBN by_IN the_DT results_NNS obtained_VBN on_IN the_DT English_NNP --_: French_JJ language_NN pair_NN ,_, and_CC in_IN order_NN to_TO con_VB -_: firm_NN our_PRP$ results_NNS ,_, we_PRP run_VBP additional_JJ experiments_NNS on_IN other_JJ languages_NNS ,_, we_PRP applied_VBD our_PRP$ method_NN to_TO build_VB RNN_NNP POS_NNP taggers_NNS for_IN three_CD more_JJR target_NN languages_NNS --_: German_JJ ,_, Greek_JJ and_CC Spanish_JJ --_: with_IN English_NNP as_IN the_DT source_NN language_NN ,_, in_IN order_NN to_TO compare_VB our_PRP$ re_SYM -_: sults_NNS with_IN those_DT of_IN -LRB-_-LRB- Das_NNP and_CC Petrov_NNP ,_, 2011_CD ;_: Duong_NNP et_FW al._FW ,_, 2013_CD ;_: Gouws_NNP and_CC Søgaard_NNP ,_, 2015a_CD -RRB-_-RRB- ._.
Our_PRP$ train_NN -_: ing_NN and_CC validation_NN -LRB-_-LRB- English_NNP -RRB-_-RRB- data_NNS extracted_VBN from_IN the_DT Europarl_NNP corpus_NN -LRB-_-LRB- Koehn_NNP ,_, 2005_CD -RRB-_-RRB- are_VBP a_DT subset_NN of_IN the_DT training_NN data_NNS of_IN -LRB-_-LRB- Das_NNP and_CC Petrov_NNP ,_, 2011_CD ;_: Duong_NNP et_FW al._FW ,_, 2013_CD -RRB-_-RRB- ._.
The_DT sizes_NNS of_IN the_DT data_NNS sets_NNS are_VBP :_: 65_CD ,_, 000_CD -LRB-_-LRB- train_NN -RRB-_-RRB- and_CC 10_CD ,_, 000_CD -LRB-_-LRB- dev_NN -RRB-_-RRB- bi-sentences_NNS ._.
For_IN testing_NN ,_, we_PRP used_VBD the_DT same_JJ test_NN corpora_NN -LRB-_-LRB- from_IN CoNLL_NNP shared_VBD tasks_NNS on_IN dependency_NN parsing_NN -LRB-_-LRB- Buchholz_NNP and_CC Marsi_NNP ,_, 2006_CD -RRB-_-RRB- -RRB-_-RRB- as_IN -LRB-_-LRB- Das_NNP and_CC Petrov_NNP ,_, 2011_CD ;_: Duong_NNP et_FW al._FW ,_, 2013_CD ;_: Gouws_NNP and_CC Søgaard_NNP ,_, 2015a_CD -RRB-_-RRB- ._.
The_DT evaluation_NN met_VBD -_: ric_NN -LRB-_-LRB- per-token_JJ accuracy_NN -RRB-_-RRB- and_CC the_DT Universal_NNP Tagset_NNP are_VBP the_DT same_JJ as_IN before_RB ._.
The_DT source_NN sides_NNS of_IN the_DT training_NN corpora_NN -LRB-_-LRB- ARCADE_NNP II_NNP and_CC Europarl_NNP -RRB-_-RRB- and_CC the_DT valida_NN -_: tion_NN corpora_NN are_VBP tagged_VBN with_IN the_DT English_NNP TreeTagger_NNP Toolkit_NNP ._.
Using_VBG the_DT matching_VBG provided_VBN by_IN Petrov_NNP et_NNP 4For_NNP RNN_NNP a_DT single_JJ system_NN is_VBZ used_VBN for_IN German_JJ ,_, Greek_JJ and_CC Spanish_JJ al._NN -LRB-_-LRB- 2012_CD -RRB-_-RRB- we_PRP map_VBP the_DT TreeTagger_NNP and_CC the_DT CoNLL_NNP tagsets_NNS to_TO a_DT common_JJ Universal_NNP Tagset_NNP ._.
In_IN order_NN to_TO build_VB our_PRP$ unsupervised_JJ tagger_NN based_VBN on_IN a_DT Simple_NN Cross-lingual_JJ Projection_NNP -LRB-_-LRB- Algorithm_NNP 1_CD -RRB-_-RRB- ,_, we_PRP tag_VBP the_DT target_NN side_NN of_IN the_DT training_NN corpus_NN ,_, with_IN tags_NNS projected_VBN from_IN English_JJ side_NN through_IN word_NN -_: alignments_NNS established_VBN by_IN GIZA_NNP +_CD +_NN ._.
After_IN tags_NNS pro-_JJ jection_NN we_PRP use_VBP TNT_NNP Tagger_NNP to_TO induce_VB a_DT target_NN lan_NN -_: guage_NN POS_NNP Tagger_NNP -LRB-_-LRB- see_VBP Algorithm_NNP 1_CD described_VBN in_IN Section_NN 3.1_CD -RRB-_-RRB- ._.
Also_RB ,_, our_PRP$ proposed_VBN approach_NN implements_VBZ Algo_NNP -_: rithm_NN 2_CD described_VBN before_RB ._.
We_PRP had_VBD to_TO slightly_RB mod_SYM -_: ify_NN the_DT Recurrent_NNP Neural_NNP Network_NNP Language_NNP Mod_NNP -_: eling_VBG Toolkit_NNP -LRB-_-LRB- RNNLM_NNP -RRB-_-RRB- provided_VBN by_IN Mikolov_NNP et_FW al._FW -LRB-_-LRB- 2011_CD -RRB-_-RRB- ,_, to_TO learn_VB our_PRP$ Recurrent_NNP Neural_NNP Network_NNP Based_VBD POS_NNP Tagger5_NNP ._.
The_DT modifications_NNS include_VBP :_: -LRB-_-LRB- 1_LS -RRB-_-RRB- building_VBG the_DT cross-lingual_JJ word_NN representations_NNS automatically_RB ;_: and_CC -LRB-_-LRB- 2_CD -RRB-_-RRB- learning_NN and_CC testing_NN models_NNS with_IN several_JJ hidden_JJ layers_NNS -LRB-_-LRB- common_JJ representation_NN as_IN input_NN and_CC universal_JJ POS_NNP tags_NNS as_IN output_NN -RRB-_-RRB- ._.
The_DT combined_VBN model_NN is_VBZ built_VBN for_IN each_DT considered_VBN language_NN using_VBG cross-validation_NN on_IN the_DT test_NN corpus_NN ._.
First_RB the_DT test_NN corpus_NN is_VBZ split_VBN into_IN 2_CD equal_JJ parts_NNS and_CC on_IN each_DT part_NN ,_, we_PRP estimate_VBP the_DT interpolation_NN parameter_NN μ_NN -LRB-_-LRB- Equation_NN 1_CD -RRB-_-RRB- which_WDT maximizes_VBZ the_DT per-token_JJ accu_NN -_: racy_JJ score_NN ._.
Then_RB each_DT part_NN of_IN test_NN corpus_NN is_VBZ tagged_VBN using_VBG the_DT combined_VBN model_NN tuned_VBN -LRB-_-LRB- Equation_NN 2_CD -RRB-_-RRB- on_IN the_DT other_JJ part_NN ,_, and_CC vice_NN versa_RB -LRB-_-LRB- standard_JJ cross-validation_NN procedure_NN -RRB-_-RRB- ._.
Finally_RB ,_, we_PRP investigate_VBP how_WRB the_DT performance_NN of_IN the_DT adapted_VBN model_NN changes_NNS according_VBG to_TO target_VB adap_SYM -_: tation_NN corpus_NN size_NN ._.
We_PRP choose_VBP German_JJ as_IN target_NN adaptation_NN language_NN ,_, because_IN we_PRP dispose_VBP of_IN a_DT large_JJ German_JJ annotated_JJ data_NN set_NN -LRB-_-LRB- from_IN CoNLL_NNP shared_VBD 5The_JJ modified_VBN source_NN code_NN is_VBZ Available_JJ from_IN the_DT following_VBG URL_NNP https://github.com/othman-zennaki/RNN__NNP POS_Tagger_NNP ._.
git_NN English_NNP a_DT precise_JJ breakdown_NN of_IN spending_NN French_JJ une_FW re_FW ́partition_FW pre_FW ́cise_FW des_FW de_FW ́penses_FW Simple_NN Projection_NNP une/DET_NNP re_VBD ́partition_NN /_CD NOUN_NNP pre_VB ́cise_JJ /_NN VERB_NN des/ADP_NN ..._: Projection_NNP +_CD RNN_NNP une/DET_NNP re_VBD ́partition_NN /_CD NOUN_NNP pre_VB ́cise_JJ /_NN ADJ_NNP des/ADP_NNP ..._: tasks_NNS on_IN dependency_NN parsing_NN -RRB-_-RRB- ._.
Then_RB ,_, we_PRP generate_VBP German_JJ adaptation_NN sets_NNS of_IN 7_CD different_JJ sizes_NNS -LRB-_-LRB- from_IN 100_CD to_TO 10_CD ,_, 000_CD utterances_NNS -RRB-_-RRB- ._.
Each_DT adaptation_NN set_NN is_VBZ used_VBN to_TO adapt_VB our_PRP$ unsupervised_JJ RNN_NNP POS_NNP tagger_NN ._.
As_IN contrastive_JJ experiments_NNS ,_, we_PRP also_RB learn_VBP supervised_JJ POS_NNP Taggers_NNPS based_VBN on_IN RNN_NNP ,_, TNT_NNP or_CC their_PRP$ linear_JJ combination_NN ._.
Table_NNP 2_CD :_: Improved_VBN tagged_VBN example_NN for_IN french_JJ target_NN lan_NN -_: guage_NN ._.
-LRB-_-LRB- VERB_NN -RRB-_-RRB- ,_, whereas_IN it_PRP is_VBZ an_DT adjective_NN -LRB-_-LRB- ADJ_NNP -RRB-_-RRB- in_IN this_DT par_NN -_: ticular_JJ context_NN ._.
We_PRP hypothesize_VBP that_IN the_DT context_NN in_IN -_: formation_NN is_VBZ better_RBR represented_VBN in_IN RNN_NNP ,_, because_IN of_IN the_DT recurrent_JJ connections_NNS ._.
In_IN case_NN of_IN word_NN order_NN divergence_NN ,_, we_PRP observed_VBD that_IN our_PRP$ model_NN can_MD still_RB handle_VB some_DT divergence_NN ,_, no_DT -_: tably_RB for_IN the_DT following_JJ cases_NNS :_: •_CD Obviously_RB if_IN the_DT current_JJ tag_NN word_NN is_VBZ unambigu_JJ -_: ous_NN -LRB-_-LRB- case_NN of_IN ADJ_NNP and_CC NOUN_NNP order_NN from_IN En_SYM -_: glish_NN to_TO French_JJ -_: see_VB table_NN 3_LS -RRB-_-RRB- ,_, then_RB the_DT context_NN -LRB-_-LRB- RNN_NNP history_NN -RRB-_-RRB- information_NN has_VBZ no_DT effect_NN ._.
•_CD When_WRB the_DT context_NN is_VBZ erroneous_JJ -LRB-_-LRB- due_JJ to_TO the_DT fact_NN that_IN word_NN order_NN for_IN the_DT target_NN test_NN corpus_NN is_VBZ different_JJ from_IN the_DT source_NN training_NN corpus_NN -RRB-_-RRB- ,_, the_DT right_JJ word_NN tag_NN can_MD be_VB recovered_VBN using_VBG the_DT com_NN -_: bination_NN -LRB-_-LRB- RNN+C_NNP ross-lingual_JJ projection_NN -_: see_VB table_NN 4_LS -RRB-_-RRB- ._.
Table_NNP 3_CD :_: Word_NN order_NN divergence_NN -_: unambiguous_JJ tag_NN word_NN -_: ._.
Table_NNP 4_CD :_: Word_NN order_NN divergence_NN -_: ambiguous_JJ tag_NN word_NN -_: ._.
5.2.2_CD Lightly_RB supervised_VBN model_NN In_IN table_NN 5_CD we_PRP report_VBP the_DT results_NNS obtained_VBN after_IN adaptation_NN with_IN a_DT gradually_RB increasing_VBG amount_NN of_IN 5.2_CD 5.2.1_CD Results_NNS and_CC discussion_NN Unsupervised_JJ model_NN In_IN table_NN 1_CD we_PRP report_VBP the_DT results_NNS obtained_VBN for_IN the_DT unsupervised_JJ approach_NN ._.
Preliminary_JJ RNN_NNP experi_NN -_: ments_NNS used_VBD one_CD hidden_VBN layer_NN ,_, but_CC we_PRP obtained_VBD lower_JJR performance_NN compared_VBN to_TO those_DT with_IN two_CD hidden_JJ lay_VBD -_: ers_NNS ._.
So_IN we_PRP report_VBP here_RB RNN_NNP accuracy_NN achieved_VBD us_PRP -_: ing_VBG two_CD hidden_JJ layers_NNS ,_, containing_VBG respectively_RB 640_CD and_CC 160_CD neurons_NNS -LRB-_-LRB- RNN-640-160_NN -RRB-_-RRB- ._.
As_IN shown_VBN in_IN the_DT table_NN ,_, this_DT accuracy_NN is_VBZ close_JJ to_TO that_DT of_IN the_DT simple_JJ pro-_JJ jection_NN tagger_NN ,_, the_DT difference_NN coming_VBG mostly_RB from_IN out-of-vocabulary_JJ -LRB-_-LRB- OOV_NNP -RRB-_-RRB- words_NNS ._.
As_IN OOV_NNP words_NNS are_VBP not_RB in_IN the_DT training_NN corpus_NN ,_, their_PRP$ vector_NN repre_NN -_: sentations_NNS are_VBP empty_JJ -LRB-_-LRB- they_PRP contain_VBP only_RB 0_CD -RRB-_-RRB- ,_, there_EX -_: fore_NN the_DT RNN_NNP model_NN uses_VBZ only_RB the_DT context_NN infor_NN -_: mation_NN ,_, which_WDT is_VBZ insufficient_JJ to_TO tag_VB correctly_RB the_DT OOV_NNP words_NNS in_IN the_DT test_NN corpus_NN ._.
We_PRP also_RB observe_VBP that_IN both_DT methods_NNS seem_VBP complementary_JJ since_IN the_DT best_JJS results_NNS are_VBP achieved_VBN using_VBG the_DT linearly_RB combined_VBN model_NN Projection_NNP +_CD RNN-640-160_NN ._.
It_PRP achieves_VBZ com_NN -_: parable_JJ results_NNS to_TO Das_NNP and_CC Petrov_NNP -LRB-_-LRB- 2011_CD -RRB-_-RRB- ,_, Duong_NNP et_FW al._FW -LRB-_-LRB- 2013_CD -RRB-_-RRB- -LRB-_-LRB- who_WP used_VBD the_DT full_JJ Europarl_NNP corpus_NN while_IN we_PRP used_VBD only_RB a_DT 65,000_CD subset_NN of_IN it_PRP -RRB-_-RRB- and_CC Gouws_NNP and_CC Søgaard_NNP -LRB-_-LRB- 2015a_JJ -RRB-_-RRB- -LRB-_-LRB- who_WP in_IN addition_NN used_VBN Wik_SYM -_: tionary_JJ and_CC Wikipedia_NNP -RRB-_-RRB- methods_NNS ._.
It_PRP is_VBZ also_RB impor_SYM -_: tant_NN to_TO note_VB that_IN a_DT single_JJ RNN_NNP tagger_NN applies_VBZ to_TO Ger_NNP -_: man_NN ,_, Greek_JJ and_CC Spanish_JJ ;_: so_RB this_DT is_VBZ a_DT truly_RB multi_NNS -_: lingual_NN POS_NNP tagger_NN !_.
Therefore_RB ,_, as_IN for_IN several_JJ other_JJ NLP_NNP tasks_NNS such_JJ as_IN language_NN modelling_NN or_CC machine_NN translation_NN -LRB-_-LRB- where_WRB standard_JJ and_CC NN-based_JJ models_NNS are_VBP combined_VBN in_IN a_DT log-linear_JJ model_NN -RRB-_-RRB- ,_, the_DT use_NN of_IN both_DT standard_JJ and_CC RNN-based_JJ approaches_NNS seems_VBZ neces_NNS -_: sary_JJ to_TO obtain_VB optimal_JJ performances_NNS ._.
In_IN order_NN to_TO know_VB in_IN what_WP respect_NN using_VBG RNN_NNP im_SYM -_: proves_VBZ combined_VBN model_NN accuracy_NN ,_, and_CC vice_NN versa_RB ,_, we_PRP analyzed_VBD the_DT French_JJ test_NN corpus_NN ._.
In_IN the_DT exam_NN -_: ple_NN provided_VBN in_IN table_NN 2_CD ,_, RNN_NNP information_NN helps_VBZ to_TO resolve_VB the_DT French_JJ word_NN ``_`` pre_VB ́cise_JJ ''_'' tag_NN ambiguity_NN :_: in_IN the_DT Simple_NN Projection_NNP model_NN it_PRP is_VBZ tagged_VBN as_IN a_DT verb_NN EN_NNP Supervised_NNP Treetagger_NNP ..._: other/ADJ_NNP specific/ADJ_NNP groups/NOUN_NNP ..._: FR_NNP Unsupervised_NNP RNN_NNP ..._: autres/ADJ_NNP groupes/NOUN_NNP spcifiques/ADJ_NNP ..._: EN_NNP Supervised_NNP Treetagger_NNP ..._: two/NUM_NNP local/ADJ_NNP groups/NOUN_NNP ..._: FR_NNP Unsupervised_NNP RNN_NNP ..._: deux/NUM_NNP groupes/NOUN_NNP locaux/NOUN_NNP ..._: Projection_NNP +_CD RNN_NNP ..._: deux/NUM_NNP groupes/NOUN_NNP locaux_VBD /_CD ADJ_NNP ..._: Model_NNP DE_NNP Corpus_NNP Size_NN 0_CD 100_CD 500_CD 1k_FW 2k_FW 5k_FW 7k_FW 10k_FW Unsupervised_JJ RNN_NNP +_NN DE_NNP Adaptation_NNP 76.1_CD %_NN 82.1_CD %_NN 87.3_CD %_NN 90.4_CD %_NN 90.7_CD %_NN 91.2_CD %_NN 91.4_CD %_NN 92.4_CD %_NN Supervised_VBN RNN_NNP DE_NNP only_RB --_: 71_CD %_NN 76.4_CD %_NN 82.1_CD %_NN 90.6_CD %_NN 93_CD %_NN 94.2_CD %_NN 95.2_CD %_NN Supervised_VBN TNT_NNP DE_NNP only_RB --_: 80.5_CD %_NN 86.5_CD %_NN 89_CD %_NN 92.2_CD %_NN 94.1_CD %_NN 95.3_CD %_NN 95.7_CD %_NN Supervised_VBN RNN_NNP +_CD Supervised_VBN TNT_NNP DE_NNP --_: 81_CD %_NN 86.7_CD %_NN 90.1_CD %_NN 94.2_CD %_NN 95.3_CD %_NN 95.7_CD %_NN 96_CD %_NN Table_NNP 5_CD :_: Lightly_RB supervised_VBN model_NN :_: effect_NN of_IN German_JJ adaptation_NN corpus_NNS -LRB-_-LRB- manually_RB annotated_VBN -RRB-_-RRB- size_NN on_IN method_NN de_IN -_: scribed_VBN in_IN Section_NN 4_CD -LRB-_-LRB- Unsupervised_JJ RNN_NNP +_NN DE_NNP Adaptation_NNP trained_VBN on_IN EN_NNP Europarl_NNP and_CC adapted_VBD to_TO German_NNP -RRB-_-RRB- ._.
Con_NN -_: trastive_JJ experiments_NNS with_IN German_JJ supervised_JJ POS_NNP taggers_NNS using_VBG same_JJ data_NNS -LRB-_-LRB- RNN_NNP ,_, TNT_NNP and_CC RNN+TNT_NNP -RRB-_-RRB- ._.
0_CD means_VBZ no_DT German_JJ corpus_NN used_VBN during_IN training_NN ._.
target_NN language_NN data_NNS annotated_VBN -LRB-_-LRB- from_IN 100_CD to_TO 10_CD ,_, 000_CD utterances_NNS -RRB-_-RRB- ._.
We_PRP focus_VBP on_IN German_JJ target_NN language_NN only_RB ._.
It_PRP is_VBZ compared_VBN with_IN two_CD supervised_JJ approaches_NNS based_VBN on_IN TNT_NNP or_CC RNN_NNP ._.
The_DT supervised_JJ approaches_NNS are_VBP trained_VBN on_IN the_DT adaptation_NN data_NNS only_RB ._.
For_IN super_JJ -_: vised_VBD RNN_NNP ,_, it_PRP is_VBZ important_JJ to_TO mention_VB that_IN the_DT input_NN vector_NN representation_NN has_VBZ a_DT different_JJ dimension_NN for_IN each_DT amount_NN of_IN adaptation_NN data_NNS -LRB-_-LRB- we_PRP recall_VBP that_IN the_DT vector_NN representation_NN is_VBZ Vwi_NNP ,_, i_FW =_SYM 1_CD ,_, ..._: ,_, N_NNP ,_, where_WRB N_NNP is_VBZ the_DT number_NN of_IN sentences_NNS ;_: and_CC N_NNP is_VBZ growing_VBG from_IN 100_CD to_TO 10_CD ,_, 000_CD -RRB-_-RRB- ._.
The_DT results_NNS show_VBP that_IN our_PRP$ adapta_NN -_: tion_NN ,_, on_IN top_NN of_IN the_DT unsupervised_JJ RNN_NNP is_VBZ efficient_JJ in_IN very_RB low_JJ resource_NN settings_NNS -LRB-_-LRB- <_CD 1000_CD target_NN language_NN utterances_NNS -RRB-_-RRB- ._.
When_WRB more_RBR data_NNS is_VBZ available_JJ -LRB-_-LRB- >_SYM 1000_CD utterances_NNS -RRB-_-RRB- ,_, the_DT supervised_JJ approaches_NNS start_VBP to_TO be_VB better_JJR -LRB-_-LRB- but_CC RNN_NNP and_CC TNT_NNP are_VBP still_RB complementary_JJ since_IN their_PRP$ combination_NN improves_VBZ the_DT tag_NN accuracy_NN -RRB-_-RRB- ._.
Figure_NN 2_CD :_: Accuracy_NNP on_IN OOV_NNP according_VBG to_TO German_JJ train_NN -_: ing_NN corpus_NN size_NN for_IN Unsupervised_JJ RNN_NNP +_NN DE_NNP Adaptation_NNP ,_, Supervised_VBN RNN_NNP DE_NNP and_CC Supervised_NNP TNT_NNP DE_NNP ._.
Figure_NN 2_CD details_NNS the_DT behavior_NN of_IN the_DT same_JJ meth_NN -_: ods_NNS for_IN OOV_NNP words_NNS ._.
We_PRP clearly_RB see_VBP the_DT limitation_NN of_IN the_DT Unsupervised_JJ RNN_NNP +_NN Adaptation_NN to_TO handle_VB OOV_NNP words_NNS ,_, since_IN the_DT input_NN vector_NN representation_NN is_VBZ the_DT same_JJ -LRB-_-LRB- comes_VBZ from_IN the_DT initial_JJ parallel_NN corpus_NN -RRB-_-RRB- and_CC does_VBZ not_RB evolve_VB as_IN more_JJR German_JJ adaptation_NN data_NNS is_VBZ available_JJ ._.
Better_RBR handling_VBG OOV_NNP words_NNS in_IN unsuper_JJ -_: vised_VBD RNN_NNP training_NN is_VBZ our_PRP$ priority_NN for_IN future_JJ works_NNS ._.
Finally_RB ,_, these_DT results_NNS show_VBP that_IN for_IN all_DT training_NN data_NNS sizes_NNS ,_, RNN_NNP brings_VBZ complementary_JJ information_NN on_IN top_NN of_IN a_DT more_RBR classical_JJ approach_NN such_JJ as_IN TNT_NNP ._.
6_CD Conclusion_NN In_IN this_DT paper_NN ,_, we_PRP have_VBP presented_VBN a_DT novel_NN approach_NN which_WDT uses_VBZ a_DT language-independent_JJ word_NN represen_NN -_: tation_NN -LRB-_-LRB- based_VBN only_RB on_IN word_NN occurrence_NN in_IN a_DT paral_NN -_: lel_NN corpus_NN -RRB-_-RRB- within_IN a_DT recurrent_JJ neural_JJ network_NN -LRB-_-LRB- RNN_NNP -RRB-_-RRB- to_TO build_VB multilingual_JJ POS_NNP tagger_NN ._.
Our_PRP$ method_NN in_IN -_: duces_NNS automatically_RB POS_NNP tags_NNS from_IN one_CD language_NN to_TO another_DT -LRB-_-LRB- or_CC several_JJ others_NNS -RRB-_-RRB- and_CC needs_VBZ only_RB a_DT paral_NN -_: lel_NN corpus_NN and_CC a_DT POS_NNP tagger_NN in_IN the_DT source_NN language_NN -LRB-_-LRB- without_IN using_VBG word_NN alignment_NN information_NN -RRB-_-RRB- ._.
We_PRP first_RB empirically_RB evaluated_VBD the_DT proposed_VBN ap_SYM -_: proach_NN on_IN two_CD unsupervised_JJ POS_NNP taggers_NNS based_VBN on_IN RNN_NNP :_: -LRB-_-LRB- 1_LS -RRB-_-RRB- English_NNP --_: French_JJ cross-lingual_JJ POS_NNP tag_NN -_: ger_NN ;_: and_CC -LRB-_-LRB- 2_LS -RRB-_-RRB- English_NNP --_: German_JJ --_: Greek_JJ --_: Spanish_JJ mul_NN -_: tilingual_JJ POS_NNP tagger_NN ._.
The_DT performance_NN of_IN the_DT second_JJ model_NN is_VBZ close_JJ to_TO state-of-the-art_JJ with_IN only_RB a_DT subset_NN -LRB-_-LRB- 65_CD ,_, 000_CD -RRB-_-RRB- of_IN Europarl_NNP corpus_NN used_VBN ._.
Additionally_RB ,_, when_WRB a_DT small_JJ amount_NN of_IN super_JJ -_: vised_VBN data_NNS is_VBZ available_JJ ,_, the_DT experimental_JJ results_NNS demonstrated_VBD the_DT effectiveness_NN of_IN our_PRP$ method_NN in_IN a_DT weakly_RB supervised_VBN context_NN -LRB-_-LRB- especially_RB for_IN very-low_JJ -_: resourced_JJ settings_NNS -RRB-_-RRB- ._.
Although_IN our_PRP$ initial_JJ experiments_NNS are_VBP positive_JJ ,_, we_PRP believe_VBP they_PRP can_MD be_VB improved_VBN in_IN a_DT number_NN of_IN ways_NNS ._.
In_IN future_JJ work_NN ,_, we_PRP plan_VBP ,_, on_IN the_DT one_CD hand_NN ,_, to_TO bet_VB -_: ter_NN manage_VB OOV_NNP representation_NN -LRB-_-LRB- for_IN instance_NN using_VBG Cross-lingual_JJ Word_NN Embeddings_NNS -RRB-_-RRB- ,_, and_CC ,_, on_IN the_DT other_JJ hand_NN ,_, to_TO consider_VB more_JJR complex_JJ tasks_NNS such_JJ as_IN word_NN senses_NNS projection_NN or_CC semantic_JJ role_NN labels_NNS projection_NN ._.
References_NNS R._NNP Al-Rfou_NNP ,_, B._NNP Perozzi_NNP and_CC S._NNP Skiena_NNP ._.
2013_CD ._.
Poly_SYM -_: glot_NN :_: Distributed_VBN word_NN representations_NNS for_IN multilingual_JJ nlp_NN ,_, In_IN Proceedings_NNP of_IN the_DT Seventeenth_NNP Conference_NN on_IN Computational_NNP Natural_NNP Language_NNP Learning_NNP :183_CD --_: 192_CD ._.
P._NNP Annesi_NNP and_CC R._NNP Basili_NNP ._.
2010_CD ._.
Cross-lingual_JJ alignment_NN of_IN FrameNet_NNP annotations_NNS through_IN Hidden_NNP Markov_NNP Models_NNPS ,_, In_IN Proceedings_NNP of_IN CICLing_NNP :12_CD --_: 25_CD ._.
Y._NNP Bengio_NNP ,_, H._NNP Schwenk_NNP ,_, J._NNP Sene_NNP ́cal_JJ ,_, F._NNP Morin_NNP and_CC J._NNP Gau_NNP -_: vain_JJ ._.
2006_CD ._.
Neural_JJ probabilistic_JJ language_NN models_NNS ,_, In_IN Innovations_NNS in_IN Machine_NN Learning_NNP :137_CD --_: 186_CD ._.
L._NNP Bentivogli_NNP ,_, P._NNP Forner_NNP and_CC E._NNP Pianta_NNP ._.
2004_CD ._.
Evaluat_NNP -_: ing_VBG cross-language_JJ annotation_NN transfer_NN in_IN the_DT Multi_NNP -_: SemCor_NNP corpus_VBZ ,_, In_IN Proceedings_NNP of_IN the_DT 20th_JJ interna_NN -_: tional_JJ conference_NN on_IN Computational_NNP Linguistics_NNP :364_CD --_: 370_CD ._.
Association_NNP for_IN Computational_NNP Linguistics_NNP ._.
S._NNP Buchholz_NNP and_CC E._NNP Marsi_NNP ._.
2006_CD ._.
CoNLL-X_JJ shared_VBN task_NN on_IN multilingual_JJ dependency_NN parsing_NN ,_, In_IN Proceedings_NNP of_IN the_DT Tenth_NNP Conference_NN on_IN Computational_NNP Natural_NNP Language_NNP Learning_NNP :149_CD --_: 164_CD ._.
Association_NNP for_IN Com_NNP -_: putational_JJ Linguistics_NNPS ._.
T._NNP Brants_NNPS ._.
2000_CD ._.
TnT_NNP :_: a_DT statistical_JJ part-of-speech_NN tag_NN -_: ger_NN ,_, In_IN Proceedings_NNP of_IN the_DT sixth_JJ conference_NN on_IN Applied_NNP natural_JJ language_NN processing_NN :224_CD --_: 231_CD ._.
R._NNP Collobert_NNP and_CC J._NNP Weston_NNP ._.
2008_CD ._.
A_DT unified_VBN architecture_NN for_IN natural_JJ language_NN processing_NN :_: Deep_JJ neural_JJ networks_NNS with_IN multitask_JJ learning_NN ,_, In_IN Proceedings_NNP of_IN the_DT Interna_NNP -_: tional_JJ Conference_NN on_IN Machine_NN Learning_NNP -LRB-_-LRB- ICML_NNP -RRB-_-RRB- :160_CD --_: 167_CD ._.
R._NNP Collobert_NNP ,_, J._NNP Weston_NNP ,_, L._NNP Bottou_NNP ,_, M._NNP Karlen_NNP ,_, K._NNP Kavukcuoglu_NNP ,_, and_CC P._NNP Kuksa_NNP ._.
2011_CD ._.
Natural_JJ language_NN processing_NN -LRB-_-LRB- almost_RB -RRB-_-RRB- from_IN scratch_NN ,_, In_IN Journal_NNP of_IN Ma_NNP -_: chine_NN Learning_NNP Research_NNP -LRB-_-LRB- JMLR_NNP -RRB-_-RRB- ,_, volume_NN 12:2493_CD --_: 2537_CD ._.
D._NNP Das_NNP and_CC S._NNP Petrov_NNP ._.
2011_CD ._.
Unsupervised_JJ Part-of_NN -_: Speech_NNP Tagging_VBG with_IN Bilingual_NNP Graph-Based_NNP Projec_NNP -_: tions_NNS ,_, In_IN Proceedings_NNP of_IN the_DT 49th_JJ Annual_JJ Meeting_VBG of_IN the_DT Association_NNP for_IN Computational_NNP Linguistics_NNPS :_: Hu_SYM -_: man_NN Language_NNP Technologies_NNP ,_, volume_NN 1:600_CD --_: 609_CD ._.
As_IN -_: sociation_NN for_IN Computational_NNP Linguistics_NNP ._.
L._NNP Duong_NNP ,_, P.Cook_NNP ,_, S._NNP Bird_NNP and_CC P._NNP Pecina_NNP ._.
2013_CD ._.
Simpler_JJR unsupervised_JJ POS_NNP tagging_VBG with_IN bilingual_JJ projections_NNS ,_, In_IN ACL_NNP -LRB-_-LRB- 2_LS -RRB-_-RRB- :634_CD --_: 639_CD ._.
G._NNP Durrett_NNP ,_, A._NN Pauls_NNP and_CC D._NNP Klein_NNP ._.
2012_CD ._.
Syntactic_JJ trans_NNS -_: fer_NN using_VBG a_DT bilingual_JJ lexicon_NN ,_, In_IN Proceedings_NNP of_IN the_DT 2012_CD Joint_NNP Conference_NNP on_IN Empirical_NNP Methods_NNPS in_IN Natu_NNP -_: ral_NN Language_NN Processing_NNP and_CC Computational_NNP Natural_NNP Language_NNP Learning_NNP :1_CD --_: 11_CD ._.
Association_NNP for_IN Computa_NNP -_: tional_JJ Linguistics_NNPS ._.
J.L._NNP Elman_NNP ._.
1990_CD ._.
Finding_VBG structure_NN in_IN time_NN ,_, In_IN Cogni_NNP -_: tive_JJ science_NN :179_CD --_: 211_CD ._.
J._NNP Henderson_NNP ._.
2004_CD ._.
Discriminative_JJ training_NN of_IN a_DT neu_NN -_: ral_NN network_NN statistical_JJ parser_NN ,_, In_IN Proceedings_NNP of_IN the_DT Annual_JJ Meeting_VBG of_IN the_DT Association_NNP for_IN Computational_NNP Linguistics_NNP -LRB-_-LRB- ACL_NNP -RRB-_-RRB- :95_CD --_: 102_CD ._.
S._NNP Federici_NNP and_CC V._NNP Pirrelli_NNP ._.
1993_CD ._.
Analogical_JJ modelling_NN of_IN text_NN tagging_NN ,_, unpublished_JJ report_NN ,_, Istituto_NNP diLin_NNP -_: guistica_NN Computazionale_NNP ,_, Pisa_NNP ,_, Italy_NNP ._.
S._NNP Gouws_NNP and_CC A._NNP Søgaard_NNP 2015_CD ._.
Simple_NN task-specific_JJ bilingual_JJ word_NN embeddings_NNS ,_, In_IN The_DT 2015_CD Conference_NN of_IN the_DT North_JJ American_JJ Chapter_NN of_IN the_DT Association_NNP for_IN Computational_NNP Linguistics_NNPS :_: Human_NNP Language_NNP Tech_NNP -_: nologies_NNS ,_, NAACL_NNP '_POS 15:1386_CD --_: 1390_CD ._.
S._NNP Gouws_NNP ,_, Y._NNP Bengio_NNP and_CC G._NNP Corrado_NNP 2015_CD ._.
Bil_NNP -_: BOWA_NNP :_: Fast_NNP Bilingual_NNP Distributed_VBD Representations_NNS without_IN Word_NN Alignments_NNS ,_, In_IN Proceedings_NNP of_IN the_DT 32nd_CD International_NNP Conference_NNP on_IN Machine_NN Learning_NNP ,_, ICML_NNP 2015:748_CD --_: 756_CD ._.
W._NNP Jiang_NNP ,_, Q._NNP Liu_NNP and_CC Y._NNP Lu_NNP ̈_CD ,_, 2011_CD Relaxed_JJ cross-lingual_JJ projection_NN of_IN constituent_JJ syntax_NN ,_, In_IN Proceedings_NNP of_IN the_DT Conference_NN on_IN Empirical_JJ Methods_NNS in_IN Natural_JJ Lan_SYM -_: guage_NN Processing_NNP :1192_CD --_: 1201_CD ._.
Association_NNP for_IN Com_NNP -_: putational_JJ Linguistics_NNPS ._.
S._NNP Kim_NNP ,_, K._NNP Toutanova_NNP and_CC H._NNP Yu_NNP ,_, 2012_CD Multilin_NNP -_: gual_NN named_VBN entity_NN recognition_NN using_VBG parallel_JJ data_NNS and_CC metadata_NN from_IN wikipedia_NN ,_, In_IN Proceedings_NNP of_IN the_DT 50th_JJ Annual_JJ Meeting_VBG of_IN the_DT Association_NNP for_IN Computational_NNP Linguistics_NNPS :_: Long_NNP Papers-volume_NNP 1:694_CD --_: 702_CD ._.
Associ_SYM -_: ation_NN for_IN Computational_NNP Linguistics_NNP ._.
S._NNP Li_NNP ,_, J.V_NNP Grac_NNP ̧a_NN and_CC B._NNP Taskar_NNP ._.
2012_CD Wiki-ly_JJ super_JJ -_: vised_VBN part-of-speech_NN tagging_NN ,_, In_IN Proceedings_NNP of_IN the_DT 2012_CD Joint_NNP Conference_NNP on_IN Empirical_NNP Methods_NNPS in_IN Nat_NNP -_: ural_JJ Language_NN Processing_NNP and_CC Computational_NNP Natu_NNP -_: ral_NN Language_NN Learning_NNP :1389_CD --_: 1398_CD ._.
Association_NNP for_IN Computational_NNP Linguistics_NNP ._.
T._NNP Mikolov_NNP ,_, M._NNP Karafia_NNP ́t_NN ,_, L._NNP Burget_NNP ,_, J._NNP Cernocky_NNP ́_NN and_CC S._NNP Khudanpur_NNP ._.
2010_CD ._.
Recurrent_JJ neural_JJ network_NN based_VBN language_NN model_NN ,_, In_IN INTERSPEECH_NNP :1045_CD --_: 1048_CD ._.
T._NNP Mikolov_NNP ,_, S._NNP Kombrink_NNP ,_, A._NN Deoras_NNP ,_, L._NNP Burget_NNP and_CC J._NNP Cernocky_NNP ._.
2011_CD ._.
RNNLM-Recurrent_JJ neural_JJ network_NN language_NN modeling_NN toolkit_NN ,_, In_IN Proc_NNP ._.
of_IN the_DT 2011_CD ASRU_NNP Workshop_NNP :196_CD --_: 201_CD ._.
F.Och_NNP and_CC H.Ney_NNP ._.
2000_CD ._.
Improved_VBN Statistical_NNP Alignment_NNP Models_NNPS ,_, In_IN ACL00_CD :440_CD --_: 447_CD ._.
S._NNP Pado_NNP ́_NN ._.
2007_CD ._.
Cross-Lingual_NNP Annotation_NNP Projection_NNP Models_NNPS for_IN Role-Semantic_NNP Information_NNP ,_, In_IN German_NNP Research_NNP Center_NNP for_IN Artificial_NNP Intelligence_NNP and_CC Saar_NNP -_: land_NN University_NNP ,_, volume_NN 21_CD ._.
S._NNP Petrov_NNP ,_, D._NNP Das_NNP and_CC R._NNP McDonald_NNP ._.
2012_CD ._.
A_DT Univer_NNP -_: sal_NN Part-of-Speech_NNP Tagset_NNP ,_, In_IN Proceedings_NNP of_IN the_DT 8th_JJ International_NNP Conference_NNP on_IN Language_NNP Resources_NNPS and_CC Evaluation_NNP -LRB-_-LRB- LREC_NNP '_POS 12_CD -RRB-_-RRB- :2089_CD --_: 2096_CD ._.
P._NNP Koehn_NNP 2005_CD ._.
Europarl_NNP :_: A_DT parallel_JJ corpus_NN for_IN statistical_JJ machine_NN translation_NN ,_, In_IN MT_NNP summit_NN ,_, volume_NN 5_CD :79_CD --_: 86_CD ._.
D._NNP Rumelhart_NNP ,_, E._NNP Hinton_NNP and_CC R.J._NNP Williams_NNP ._.
1985_CD ._.
Learning_NNP internal_JJ representations_NNS by_IN error_NN propagation_NN ,_, In_IN Learning_NNP internal_JJ representations_NNS by_IN error_NN propaga_NN -_: tion_NN ._.
H._NNP Schmid_NNP ._.
1995_CD ._.
TreeTagger_NNP --_: a_DT Language_NN Indepen_NNP -_: dent_NN Part-of-speech_NN Tagger_NNP ,_, In_IN Institut_NNP fu_NNP ̈r_NN Maschinelle_NNP Sprachverarbeitung_NNP ,_, Universita_NNP ̈t_NNP Stuttgart_NNP ,_, volume_NN 43_CD :28_CD ._.
M._NNP Sundermeyer_NNP ,_, I._NNP Oparin_NNP ,_, J._NNP Gauvain_NNP ,_, B._NNP Freiberg_NNP ,_, R._NNP Schluter_NNP and_CC H.Ney_NNP ._.
2013_CD ._.
Comparison_NN of_IN feedfor_NN -_: ward_NN and_CC recurrent_JJ neural_JJ network_NN language_NN models_NNS ,_, In_IN Acoustics_NNP ,_, Speech_NNP and_CC Signal_NNP Processing_NNP -LRB-_-LRB- ICASSP_NNP -RRB-_-RRB- ,_, 2013_CD IEEE_NNP International_NNP Conference_NNP :8430_CD --_: 8434_CD ._.
O._NNP Ta_NNP ̈ckstro_NN ̈m_NN ,_, R._NNP McDonald_NNP and_CC J._NNP Uszkoreit_NNP ._.
2012_CD ._.
Cross-lingual_JJ word_NN clusters_NNS for_IN direct_JJ transfer_NN of_IN lin_NN -_: guistic_JJ structure_NN ,_, In_IN Proceedings_NNP of_IN the_DT 2012_CD Con_NN -_: ference_NN of_IN the_DT North_JJ American_JJ Chapter_NN of_IN the_DT Asso_NNP -_: ciation_NN for_IN Computational_NNP Linguistics_NNPS :_: Human_NNP Lan_NNP -_: guage_NN Technologies_NNPS :477_CD --_: 487_CD ._.
Association_NNP for_IN Com_NNP -_: putational_JJ Linguistics_NNPS ._.
O._NNP Ta_NNP ̈ckstro_NN ̈m_NN ,_, R._NNP McDonald_NNP ,_, J._NNP Nivre_NNP ._.
2013_CD ._.
Target_NN lan_SYM -_: guage_NN adaptation_NN of_IN discriminative_JJ transfer_NN parsers_NNS ,_, In_IN Proceedings_NNP of_IN the_DT Conference_NN of_IN the_DT North_JJ American_JJ Chapter_NN of_IN the_DT Association_NNP for_IN Computational_NNP Linguis_NNP -_: tics_NNS :_: Human_JJ Language_NN Tech-nologies_NNS -LRB-_-LRB- NAACL_NNP -RRB-_-RRB- ._.
O._NNP Ta_NNP ̈ckstro_NN ̈m_NN ,_, D._NNP Das_NNP ,_, S._NNP Petrov_NNP ,_, R._NNP McDonald_NNP and_CC J._NNP Nivre_NNP ._.
2013_CD ._.
Token_JJ and_CC type_NN constraints_NNS for_IN cross_NN -_: lingual_NN part-of-speech_NN tagging_NN ,_, In_IN Transactions_NNS of_IN the_DT Association_NNP for_IN Computational_NNP Linguistics_NNPS :_: volume_NN 1_CD :1_CD --_: 12_CD ._.
Association_NNP for_IN Computational_NNP Linguistics_NNP ._.
I._NN Titov_NNP and_CC A._NNP Klementiev_NNP ._.
2012_CD ._.
Crosslingual_JJ induction_NN of_IN semantic_JJ roles_NNS ,_, In_IN Proceedings_NNP of_IN the_DT 50th_JJ Annual_JJ Meeting_VBG of_IN the_DT Association_NNP for_IN Computational_NNP Linguis_NNP -_: tics_NNS :_: Long_NNP Papers-volume_NNP 1:647_CD --_: 656_CD ._.
Association_NNP for_IN Computational_NNP Linguistics_NNP ._.
L._NNP Van_NNP der_NNP Plas_NNP and_CC M._NNP Apidianaki_NNP ._.
2014_CD ._.
Cross-lingual_JJ Word_NN Sense_NN Disambiguation_NNP for_IN Predicate_NNP Labelling_NNP of_IN French_NNP ,_, In_IN Proceedings_NNP of_IN the_DT 21st_CD TALN_NNP -LRB-_-LRB- Traitement_NNP Automatique_NNP des_NNP Langues_NNP Naturelles_NNP -RRB-_-RRB- conference_NN :46_CD --_: 55_CD ._.
J._NNP Veronis_NNP ,_, O._NNP Hamon_NNP ,_, C._NNP Ayache_NNP ,_, R._NNP Belmouhoub_NNP ,_, O._NNP Kraif_NNP ,_, D._NNP Laurent_NNP ,_, T.M.H._NNP Nguyen_NNP ,_, N._NNP Semmar_NNP ,_, F._NNP Stuck_NNP and_CC Z._NNP Wajdi_NNP ._.
2008_CD ._.
Arcade_NNP II_NNP Action_NNP de_FW recherche_FW concerte_FW sur_FW l’alignement_FW de_FW documents_NNS et_FW son_NN valuation_NN ,_, Chapitre_NNP 2_CD ,_, Editions_NNP Herme_NNP ́s_NNS ._.
L._NNP Van_NNP der_NNP Maaten_NNP and_CC G._NNP Hinton_NNP 2008_CD Visualizing_VBG data_NNS using_VBG t-SNE_JJ ,_, In_IN Journal_NNP of_IN Machine_NNP Learning_NNP Research_NNP -LRB-_-LRB- JMLR_NNP -RRB-_-RRB- ,_, 9:2579_CD --_: 2605_CD ._.
G._NNP Wisniewski_NNP ,_, N._NNP Pe_NNP ́cheux_NNP ,_, S._NNP Gahbiche-Braham_NNP and_CC F._NNP Yvon_NNP ._.
2014_CD ._.
Cross-Lingual_JJ Part-of-Speech_NNP Tagging_NNP through_IN Ambiguous_NNP Learning_NNP ,_, In_IN EMNLP_NNP '_POS 14:1779_CD --_: 1785_CD ._.
M._NNP Xiao_NNP and_CC Y._NNP Guo_NNP ._.
2014_CD ._.
Distributed_VBN Word_NN Represen_NNP -_: tation_NN Learning_NNP for_IN Cross-Lingual_NNP Dependency_NNP Pars_NNP -_: ing_NN ,_, In_IN CoNLL-2014_JJ :119_CD --_: 129_CD ._.
D._NNP Yarowsky_NNP ,_, G._NNP NGAI_NNP and_CC R._NNP Wicentowski_NNP ._.
2001_CD ._.
In_IN -_: ducing_VBG multilingual_JJ text_NN analysis_NN tools_NNS via_IN robust_JJ pro-_JJ jection_NN across_IN aligned_VBN corpora_NN ,_, In_IN Proceedings_NNP of_IN the_DT first_JJ international_JJ conference_NN on_IN Human_JJ language_NN technology_NN research_NN :1_CD --_: 8_CD ._.
Association_NNP for_IN Computa_NNP -_: tional_JJ Linguistics_NNPS ._.
