Surrounding_VBG Word_NN Sense_NN Model_NNP for_IN Japanese_NNP All-words_NNPS Word_NNP Sense_NN Disambiguation_NNP Abstract_NNP This_DT paper_NN proposes_VBZ a_DT surrounding_VBG word_NN sense_NN model_NN -LRB-_-LRB- SWSM_NNP -RRB-_-RRB- that_WDT uses_VBZ the_DT distri_NN -_: bution_NN of_IN word_NN senses_NNS that_WDT appear_VBP near_IN am_VBP -_: biguous_JJ words_NNS for_IN unsupervised_JJ all-words_NNS word_NN sense_NN disambiguation_NN in_IN Japanese_JJ ._.
Although_IN it_PRP was_VBD inspired_VBN by_IN the_DT topic_NN model_NN ,_, ambiguous_JJ Japanese_JJ words_NNS tend_VBP to_TO have_VB similar_JJ topics_NNS since_IN coarse_JJ semantic_JJ polysemy_NN is_VBZ less_RBR likely_JJ to_TO occur_VB than_IN that_DT in_IN Western_JJ languages_NNS as_IN Japanese_JJ uses_NNS Chi_NNP -_: nese_NN characters_NNS ,_, which_WDT are_VBP ideograms_NNS ._.
We_PRP thus_RB propose_VBP a_DT model_NN that_WDT uses_VBZ the_DT dis_SYM -_: tribution_NN of_IN word_NN senses_NNS that_WDT appear_VBP near_IN ambiguous_JJ words_NNS :_: SWSM_NNP ._.
We_PRP embed_VBD -_: ded_VBD the_DT concept_NN dictionary_NN of_IN an_DT Elec_NNP -_: tronic_JJ Dictionary_NNP Research_NNP -LRB-_-LRB- EDR_NNP -RRB-_-RRB- elec_SYM -_: tronic_JJ dictionary_NN in_IN the_DT system_NN and_CC used_VBD the_DT Japanese_JJ Corpus_NNP of_IN EDR_NNP for_IN the_DT exper_NN -_: iments_NNS ,_, which_WDT demonstrated_VBD that_IN SWSM_NNP outperformed_VBD a_DT system_NN with_IN a_DT random_JJ baseline_NN and_CC a_DT system_NN that_WDT used_VBD a_DT topic_NN model_NN called_VBN Dirichlet_NNP Allocation_NNP with_IN WORDNET_NNP -LRB-_-LRB- LDAWN_NNP -RRB-_-RRB- ,_, especially_RB when_WRB there_EX were_VBD high_JJ levels_NNS of_IN entropy_NN for_IN the_DT word_NN sense_NN distribution_NN of_IN ambiguous_JJ words_NNS ._.
1_CD Introduction_NNP This_DT paper_NN proposes_VBZ a_DT surrounding_VBG word_NN sense_NN model_NN -LRB-_-LRB- SWSM_NNP -RRB-_-RRB- for_IN unsupervised_JJ Japanese_JJ all_DT -_: words_NNS Word_NN Sense_NN Disambiguation_NNP -LRB-_-LRB- WSD_NNP -RRB-_-RRB- ._.
SWSM_NNP assumes_VBZ that_IN the_DT sense_NN distribution_NN of_IN sur_NN -_: rounding_VBG words_NNS varies_VBZ according_VBG to_TO the_DT sense_NN of_IN a_DT polysemous_JJ word_NN ._.
For_IN instance_NN ,_, a_DT word_NN ``_`` 可能性_FW ''_'' -LRB-_-LRB- possibility_NN -RRB-_-RRB- has_VBZ three_CD senses_NNS according_VBG to_TO the_DT Electronic_JJ Dictionary_NNP Research_NNP -LRB-_-LRB- EDR_NNP -RRB-_-RRB- electronic_JJ dictionary_NN -LRB-_-LRB- Miyoshi_NNP et_FW al._FW ,_, 1996_CD -RRB-_-RRB- :_: -LRB-_-LRB- 1_LS -RRB-_-RRB- The_DT ability_NN to_TO do_VB something_NN well_RB -LRB-_-LRB- 2_LS -RRB-_-RRB- Its_PRP$ feasibility_NN -LRB-_-LRB- 3_LS -RRB-_-RRB- The_DT certainty_NN of_IN something_NN happenings_NNS Although_IN sense_NN -LRB-_-LRB- 3_LS -RRB-_-RRB- is_VBZ the_DT most_RBS frequent_JJ in_IN the_DT prior_JJ distributions_NNS ,_, sense_NN -LRB-_-LRB- 1_LS -RRB-_-RRB- will_MD be_VB more_RBR likely_JJ when_WRB the_DT local_JJ context_NN includes_VBZ some_DT concepts_NNS like_IN ``_`` 人間_FW ''_'' -LRB-_-LRB- man_NN -RRB-_-RRB- or_CC ``_`` 誰々の_FW ''_'' -LRB-_-LRB- someone_NN 's_POS -RRB-_-RRB- ._.
It_PRP is_VBZ challenging_VBG in_IN practice_NN to_TO accurately_RB learn_VB the_DT difference_NN in_IN the_DT senses_NNS of_IN surrounding_VBG words_NNS in_IN an_DT unsupervised_JJ manner_NN ,_, but_CC we_PRP developed_VBD an_DT ap_NN -_: proximate_JJ model_NN that_WDT took_VBD conditions_NNS into_IN consid_NN -_: eration_NN ._.
SWSM_NNP is_VBZ a_DT method_NN for_IN all-words_NNS WSD_NNP in_IN -_: spired_JJ by_IN the_DT topic_NN model_NN -LRB-_-LRB- Section_NN 2_CD -RRB-_-RRB- ._.
It_PRP treats_VBZ the_DT similarities_NNS of_IN word_NN senses_NNS using_VBG WORDNET_NNP -_: WALK_NNP and_CC it_PRP generates_VBZ word_NN senses_NNS of_IN ambigu_NN -_: ous_JJ words_NNS and_CC their_PRP$ surrounding_VBG words_NNS -LRB-_-LRB- Section_NN 3_CD -RRB-_-RRB- ._.
First_RB ,_, SWSM_NNP abstracted_VBD the_DT concepts_NNS of_IN the_DT concept_NN dictionary_NN -LRB-_-LRB- Section_NN 4_CD -RRB-_-RRB- and_CC calculated_VBN the_DT transition_NN probabilities_NNS for_IN priors_NNS -LRB-_-LRB- Section_NN 5_CD -RRB-_-RRB- ._.
Then_RB it_PRP estimated_VBD the_DT word_NN senses_NNS using_VBG Gibbs_NNP Sam_NNP -_: pling_NN -LRB-_-LRB- Section_NN 6_CD -RRB-_-RRB- ._.
Our_PRP$ experiments_NNS with_IN an_DT EDR_NNP Japanese_NNP corpus_NN and_CC a_DT Concept_NNP Dictionary_NNP -LRB-_-LRB- Sec_NNP -_: tion_NN 7_CD -RRB-_-RRB- indicated_VBD that_IN SWSM_NNP was_VBD effective_JJ for_IN Japanese_JJ all-words_NNS WSD_NNP -LRB-_-LRB- Section_NNP 8_CD -RRB-_-RRB- ._.
We_PRP dis_SYM -_: cuss_VB the_DT results_NNS -LRB-_-LRB- Section_NN 9_CD -RRB-_-RRB- and_CC concludes_VBZ this_DT pa_NN -_: per_IN -LRB-_-LRB- Section_NN 10_CD -RRB-_-RRB- ._.
2_CD Related_JJ Work_NN There_EX are_VBP many_JJ methods_NNS of_IN all-words_JJ WSD_NNP ._.
Ped_SYM -_: ersen_FW et_FW al._FW -LRB-_-LRB- 2005_CD -RRB-_-RRB- proposed_VBN calculation_NN of_IN the_DT se_FW -_: mantic_JJ relatedness_NN of_IN the_DT word_NN senses_NNS of_IN ambigu_NN -_: ous_JJ words_NNS and_CC their_PRP$ surrounding_VBG words_NNS ._.
Some_DT papers_NNS have_VBP reported_VBN that_IN methods_NNS using_VBG topic_NN models_NNS -LRB-_-LRB- Blei_NNP et_FW al._FW ,_, 2003_CD -RRB-_-RRB- are_VBP most_RBS effective_JJ ._.
Boyd-Graber_NNP et_FW al._FW -LRB-_-LRB- 2007_CD -RRB-_-RRB- proposed_VBD a_DT model_NN ,_, called_VBN Latent_NNP Dirichlet_NNP Allocation_NNP with_IN WORD_NNP -_: NET_NNP -LRB-_-LRB- LDAWN_NNP -RRB-_-RRB- ,_, which_WDT was_VBD a_DT model_NN where_WRB the_DT probability_NN distributions_NNS of_IN words_NNS that_IN the_DT topics_NNS had_VBD were_VBD replaced_VBN with_IN a_DT word_NN generation_NN pro-_JJ cess_NN on_IN WordNet_NNP :_: WORDNET-WALK_NNP ._.
They_PRP ap_SYM -_: plied_VBD the_DT topic_NN model_NN to_TO unsupervised_JJ English_NNP all_DT -_: words_NNS WSD_NNP ._.
Although_IN Guo_NNP and_CC Diab_NNP -LRB-_-LRB- 2011_CD -RRB-_-RRB- also_RB used_VBD the_DT topic_NN model_NN and_CC WordNet_NNP ,_, they_PRP also_RB used_VBD WordNet_NNP as_IN a_DT lexical_JJ resource_NN for_IN sense_NN definitions_NNS and_CC they_PRP did_VBD not_RB use_VB its_PRP$ conceptual_JJ structure_NN ._.
They_PRP reported_VBD that_IN the_DT performance_NN of_IN their_PRP$ system_NN was_VBD comparable_JJ with_IN that_DT reported_VBN by_IN Boyd-Graber_NNP et_FW al._FW ._.
There_EX has_VBZ been_VBN little_JJ work_NN ,_, on_IN the_DT other_JJ hand_NN ,_, on_IN unsupervised_JJ Japanese_JJ all-words_NNS WSD_NNP ._.
As_IN far_RB as_IN we_PRP know_VBP ,_, there_EX has_VBZ only_RB been_VBN one_CD paper_NN -LRB-_-LRB- Bald_SYM -_: win_VB et_FW al._FW ,_, 2008_CD -RRB-_-RRB- and_CC there_EX have_VBP been_VBN no_DT reported_VBN methods_NNS that_WDT have_VBP used_VBN the_DT topic_NN model_NN ._.
We_PRP think_VBP this_DT is_VBZ because_IN ambiguous_JJ words_NNS in_IN Japanese_JJ tend_VBP to_TO have_VB similar_JJ topics_NNS since_IN coarse_JJ semantic_JJ poly_NN -_: semy_NN is_VBZ less_RBR likely_JJ to_TO occur_VB compared_VBN to_TO that_DT with_IN Western_JJ languages_NNS as_IN Japanese_JJ uses_NNS Chinese_JJ char_NN -_: acters_NNS ,_, which_WDT are_VBP ideograms_NNS ._.
In_IN addition_NN ,_, Guo_NNP and_CC Diab_NNP -LRB-_-LRB- 2011_CD -RRB-_-RRB- reported_VBD that_IN in_IN word_NN sense_NN disam_NN -_: biguation_NN -LRB-_-LRB- WSD_NNP -RRB-_-RRB- ,_, an_DT even_RB narrower_JJR context_NN was_VBD taken_VBN into_IN consideration_NN ,_, as_IN Mihalcea_NNP -LRB-_-LRB- 2005_CD -RRB-_-RRB- had_VBD reported_VBN ._.
Therefore_RB ,_, we_PRP assumed_VBD that_IN the_DT word_NN senses_NNS of_IN the_DT local_JJ context_NN are_VBP differentiated_VBN de_IN -_: pending_VBG on_IN the_DT word_NN sense_NN of_IN the_DT target_NN word_NN like_IN that_DT in_IN supervised_JJ WSD_NNP ._.
SWSM_NNP was_VBD inspired_VBN by_IN LDAWN_NNP ,_, it_PRP thus_RB uses_VBZ WORDNET-WALK_NNP and_CC Gibbs_NNP sampling_VBG but_CC it_PRP does_VBZ not_RB use_VB the_DT topics_NNS but_CC the_DT word_NN senses_NNS of_IN the_DT surrounding_VBG words_NNS ._.
We_PRP pro-_JJ pose_VBP SWSM_NNP as_IN an_DT approach_NN to_TO unsupervised_JJ WSD_NNP and_CC carried_VBD out_IN Japanese_JJ all-words_NNS WSD_NNP ._.
3_CD Surrounding_VBG Word_NN Sense_NN Model_NNP SWSM_NNP uses_VBZ the_DT distribution_NN of_IN word_NN senses_NNS that_WDT appear_VBP near_IN the_DT target_NN word_NN in_IN WSD_NNP to_TO estimate_VB the_DT word_NN senses_NNS assuming_VBG that_IN the_DT word_NN senses_NNS of_IN the_DT local_JJ context_NN are_VBP differentiated_VBN depending_VBG on_IN the_DT word_NN sense_NN of_IN the_DT target_NN word_NN ._.
In_IN other_JJ words_NNS ,_, SWSM_NNP estimates_VBZ the_DT word_NN sense_NN accord_NN -_: ing_NN to_TO p_VB -LRB-_-LRB- s_PRP |_VBP w_VBN -RRB-_-RRB- ,_, which_WDT is_VBZ a_DT conditional_JJ probability_NN of_IN a_DT string_NN of_IN senses_NNS ,_, s_VBZ ,_, given_VBN a_DT string_NN of_IN words_NNS w._VBP SWSM_NNP involves_VBZ three_CD assumptions_NNS ._.
First_RB ,_, each_DT word_NN sense_NN has_VBZ a_DT probability_NN distribution_NN of_IN the_DT senses_NNS of_IN the_DT surrounding_VBG words_NNS ._.
Second_NNP ,_, when_WRB ci_FW denotes_VBZ the_DT sense_NN string_NN of_IN the_DT surrounding_VBG words_NNS of_IN the_DT target_NN word_NN wi_NNS ,_, the_DT conditional_JJ probability_NN of_IN ci_FW given_VBN wi_NN is_VBZ the_DT product_NN of_IN the_DT those_DT of_IN the_DT senses_NNS in_IN ci_JJ given_VBN wi_NN ._.
For_IN example_NN ,_, when_WRB wi_NN is_VBZ ``_`` 可能性_FW ''_'' -LRB-_-LRB- possibility_NN -RRB-_-RRB- and_CC its_PRP$ surrounding_VBG words_NNS are_VBP ``_`` 両者_FW ''_'' -LRB-_-LRB- both_DT sides_NNS -RRB-_-RRB- and_CC ``_`` 人間_FW ''_'' -LRB-_-LRB- human_JJ -RRB-_-RRB- ,_, ci_FW =_SYM -LRB-_-LRB- sboth_NN ,_, shuman_NN -RRB-_-RRB- and_CC P_NNP -LRB-_-LRB- ci_FW |_FW spossibility_FW -RRB-_-RRB- =_SYM P_NNP -LRB-_-LRB- sboth_JJ |_NN spossibility_NN -RRB-_-RRB- P_NN -LRB-_-LRB- shuman_NN |_NN spossibility_NN -RRB-_-RRB- are_VBP de_IN -_: fined_VBN where_WRB spossibility_NN ,_, sboth_NN ,_, and_CC shuman_NN denote_VBP word_NN senses_NNS of_IN ``_`` 可能性_FW ''_'' -LRB-_-LRB- possibility_NN -RRB-_-RRB- ,_, ``_`` 両者_FW ''_'' -LRB-_-LRB- both_DT sides_NNS -RRB-_-RRB- ,_, and_CC ``_`` 人間_FW ''_'' -LRB-_-LRB- human_JJ -RRB-_-RRB- ._.
Finally_RB ,_, each_DT polyseme_NN has_VBZ a_DT prior_JJ distribution_NN of_IN the_DT senses_NNS ._.
Given_VBN these_DT assumptions_NNS ,_, SWSM_NNP calculates_VBZ the_DT conditional_JJ probability_NN of_IN s_PRP that_DT corresponds_VBZ to_TO w_VB ,_, under_IN the_DT condition_NN where_WRB w_NN is_VBZ observed_VBN as_IN :_: P_NN -LRB-_-LRB- s_PRP ,_, c_NN |_CD w_NN -RRB-_-RRB- =_SYM ∏_CD N_NNP i_FW =_SYM 1_CD P_NNP -LRB-_-LRB- si_FW |_FW wi_FW -RRB-_-RRB- P_NNP -LRB-_-LRB- ci_FW |_FW si_FW ,_, w_NN -RRB-_-RRB- ,_, -LRB-_-LRB- 1_LS -RRB-_-RRB- where_WRB c_NN denotes_VBZ the_DT string_NN of_IN ci_NNS and_CC N_NNP denotes_VBZ the_DT number_NN of_IN all_PDT the_DT words_NNS in_IN the_DT text_NN ._.
The_DT initial_JJ part_NN on_IN the_DT right_NN is_VBZ the_DT probability_NN distribution_NN of_IN the_DT word_NN sense_NN of_IN each_DT word_NN and_CC the_DT last_JJ part_NN is_VBZ that_IN of_IN the_DT senses_NNS of_IN the_DT surrounding_VBG words_NNS for_IN each_DT word_NN sense_NN ._.
We_PRP set_VBP the_DT Dirichlet_NNP distribution_NN as_IN their_PRP$ prior_JJ ._.
The_DT final_JJ equation_NN considering_VBG prior_RB is_VBZ de_IN -_: scribed_VBN using_VBG the_DT following_VBG parameters_NNS :_: P_NN -LRB-_-LRB- s_PRP ,_, c_NN ,_, θ_NN ,_, φ_FW |_FW w_FW ,_, γk_NN ,_, τj_NN -RRB-_-RRB- =_SYM ∏_CD W_NNP ∏_CD S_NNP ∏_CD N_NNP P_NNP -LRB-_-LRB- θk_FW |_FW γk_FW -RRB-_-RRB- k_NN =_SYM 1_CD j_NN =_SYM 1_CD P_NN -LRB-_-LRB- φj_FW |_FW τj_FW -RRB-_-RRB- P_NNP -LRB-_-LRB- si_FW |_FW θwi_FW -RRB-_-RRB- P_NNP -LRB-_-LRB- ci_FW |_FW φsj_FW ,_, w_NN -RRB-_-RRB- ,_, -LRB-_-LRB- 2_LS -RRB-_-RRB- where_WRB W_NNP denotes_VBZ the_DT number_NN of_IN words_NNS ,_, S_NNP de_NNP -_: notes_VBZ the_DT number_NN of_IN senses_NNS ,_, θk_JJ denotes_NNS the_DT prob_NN -_: ability_NN distribution_NN of_IN the_DT senses_NNS of_IN word_NN k_NN ,_, and_CC φj_NN denotes_NNS the_DT probability_NN distribution_NN of_IN the_DT word_NN senses_NNS surrounding_VBG word_NN sense_NN j._NN θk_NN and_CC φj_NN are_VBP the_DT parameters_NNS of_IN the_DT multinomial_JJ distribution_NN ._.
γ_NN and_CC τ_NN are_VBP the_DT parameters_NNS of_IN the_DT Dirichlet_NNP distribution_NN ._.
Eq_NN ._.
-LRB-_-LRB- 2_LS -RRB-_-RRB- is_VBZ the_DT basic_JJ form_NN ._.
We_PRP re_SYM -_: place_NN φ_NN ,_, the_DT probability_NN distribution_NN of_IN each_DT sense_NN ,_, with_IN the_DT generation_NN process_NN by_IN using_VBG the_DT WORDNET-WALK_NNP of_IN the_DT concept_NN dictionary_NN ._.
The_DT WORDNET-WALK_NNP in_IN this_DT work_NN does_VBZ not_RB generate_VB words_NNS but_CC word_NN senses_NNS using_VBG a_DT hyper_JJ -_: transition_NN probability_NN parameter_NN ,_, Sα_NNP ._.
We_PRP set_VBP α_JJ according_VBG to_TO the_DT senses_NNS to_TO differentiate_VB the_DT sense_NN distribution_NN of_IN the_DT surrounding_VBG words_NNS before_IN train_NN -_: ing_NN ._.
By_IN doing_VBG this_DT ,_, we_PRP can_MD determine_VB which_WDT sense_NN in_IN the_DT model_NN corresponds_VBZ to_TO the_DT senses_NNS in_IN the_DT dic_NN -_: tionary_JJ ._.
SWSM_NNP estimates_VBZ the_DT word_NN senses_NNS using_VBG Gibbs_NNP sampling_NN as_IN :_: -LRB-_-LRB- 1_CD -RRB-_-RRB- Pre-processing_NN 1_CD Abstracttheconceptsintheconceptdictio_NNP -_: nary_PDT i_FW =_SYM 1_CD Figure_NN 1_CD :_: Example_NNP of_IN WORNET-WALK_NNP 2_CD Calculate_VB the_DT transition_NN parameters_NNS using_VBG the_DT sense_NN frequencies_NNS -LRB-_-LRB- 2_CD -RRB-_-RRB- Training_NN :_: Gibbs_NNS sampling_VBG to_TO estimate_VB the_DT word_NN senses_NNS 4_CD Concept_NNP Abstraction_NNP SWSM_NNP obtains_VBZ the_DT sense_NN probability_NN of_IN the_DT surrounding_VBG words_NNS using_VBG WORDNET-WALK_NNP ._.
WORDNET-WALK_NN involves_VBZ the_DT generation_NN pro-_JJ cess_NN ,_, which_WDT represents_VBZ the_DT probabilistic_JJ walks_VBZ over_IN the_DT hierarchy_NN of_IN conceptual_JJ structures_NNS like_IN Word_NN -_: Net_NN ._.
Figure_NN 1_CD shows_VBZ the_DT easy_JJ example_NN of_IN the_DT generation_NN probabilities_NNS of_IN words_NNS by_IN WORDNET_NNP -_: WALK_NNP ._.
When_WRB circle_NN nodes_VBZ represent_VB concepts_NNS and_CC triangle_NN nodes_NNS represent_VBP words_NNS of_IN leaf_NN concepts_NNS ,_, i.e._FW ,_, X_NNP and_CC Y_NNP ,_, and_CC numbers_NNS represent_VBP the_DT transi_NN -_: tion_NN probabilities_NNS ,_, the_DT generation_NN probabilities_NNS of_IN words_NNS A_DT ,_, B_NNP ,_, C_NNP ,_, and_CC D_NNP are_VBP 0.03,0.27,0.28_CD ,_, and_CC 0.42_CD ._.
LDAWN_NNP calculated_VBD the_DT probabilities_NNS of_IN word_NN senses_NNS using_VBG the_DT transition_NN probability_NN from_IN the_DT root_NN node_NN in_IN a_DT concept_NN dictionary_NN ._.
WORDNET_SYM -_: WALK_VB generated_VBN words_NNS in_IN -LRB-_-LRB- Boyd-Graber_NNP et_FW al._FW ,_, 2007_CD -RRB-_-RRB- but_CC our_PRP$ WORDNET-WALK_NNP generates_VBZ word_NN senses_NNS ._.
However_RB ,_, the_DT word_NN senses_NNS sometimes_RB do_VBP not_RB correspond_VB to_TO leaf_VB nodes_NNS but_CC to_TO internal_JJ nodes_NNS in_IN our_PRP$ model_NN and_CC that_IN causes_VBZ a_DT problem_NN :_: the_DT sum_NN of_IN the_DT probabilities_NNS is_VBZ not_RB one_CD ._.
Thus_RB ,_, we_PRP added_VBD leaf_NN nodes_NNS of_IN the_DT word_NN senses_NNS directly_RB below_IN the_DT inter_NN -_: nal_NN nodes_NNS of_IN the_DT concept_NN dictionary_NN -LRB-_-LRB- c.f._FW Figure_NN 2_CD -RRB-_-RRB- ._.
Concept_NN abstraction_NN involves_VBZ the_DT process_NN by_IN which_WDT hyponym_NN concepts_NNS map_VBP onto_IN hypernym_NN concepts_NNS ._.
Most_JJS concepts_NNS in_IN a_DT very_RB deep_JJ hierar_NN -_: chy_NN are_VBP fine_JJ grained_VBN like_IN the_DT ``_`` Tokyo_NNP University_NNP of_IN Agriculture_NNP and_CC Technology_NNP ''_'' and_CC ``_`` Ibaraki_NNP Univer_NNP -_: sity_NN ''_'' and_CC they_PRP should_MD be_VB combined_VBN together_RB like_IN ``_`` university_NN ''_'' to_TO avoid_VB the_DT zero_CD frequency_NN problem_NN ._.
Figure_NN 2_CD :_: Addition_NNP of_IN Word_NNP Sense_NN Nodes_VBZ Thus_RB ,_, SWSM_NNP combines_VBZ semantically_RB similar_JJ con_NN -_: cepts_NNS in_IN the_DT concept_NN dictionary_NN ._.
Hirakawa_NNP and_CC Kimura_NNP -LRB-_-LRB- 2003_CD -RRB-_-RRB- reported_VBD that_IN they_PRP compared_VBD three_CD methods_NNS for_IN concept_NN abstraction_NN ,_, i.e_FW ,_, flat_JJ depth_NN ,_, flat_JJ size_NN ,_, and_CC flat_JJ probability_NN meth_NN -_: ods_NNS ,_, by_IN using_VBG the_DT EDR_NNP concept_NN dictionary_NN ,_, and_CC the_DT flat_JJ probability_NN method_NN was_VBD the_DT best_JJS ._.
Therefore_RB ,_, we_PRP used_VBD the_DT flat_JJ probability_NN method_NN for_IN concept_NN ab_SYM -_: straction_NN ._.
The_DT flat_JJ probability_NN method_NN consists_VBZ of_IN two_CD steps_NNS ._.
First_RB ,_, there_EX is_VBZ a_DT search_NN for_IN nodes_NNS from_IN the_DT root_NN node_NN in_IN depth_NN first_JJ order_NN ._.
Second_NNP ,_, if_IN the_DT con_NN -_: cept_JJ probability_NN calculated_VBN based_VBN on_IN the_DT corpus_NN is_VBZ less_JJR than_IN a_DT threshold_NN value_NN ,_, the_DT concept_NN and_CC its_PRP$ hyponym_NN concepts_NNS are_VBP mapped_VBN onto_IN its_PRP$ hypernym_NN concept_NN ._.
We_PRP employed_VBD the_DT methods_NNS of_IN -LRB-_-LRB- Ribas_NNP ,_, 1995_CD -RRB-_-RRB- and_CC -LRB-_-LRB- McCarthy_NNP ,_, 1997_CD -RRB-_-RRB- to_TO calculate_VB the_DT concept_NN prob_SYM -_: ability_NN ._.
Ribas_NNP -LRB-_-LRB- 1995_CD -RRB-_-RRB- calculated_VBN the_DT frequency_NN of_IN sense_NN s_PRP as_IN :_: freq_NN -LRB-_-LRB- s_PRP -RRB-_-RRB- =_SYM ∑_FW |_FW senses_NNS -LRB-_-LRB- w_NN -RRB-_-RRB- ∈_CD U_NNP -LRB-_-LRB- s_PRP -RRB-_-RRB- |_JJ count_NN -LRB-_-LRB- w_FW -RRB-_-RRB- ,_, w_FW |_FW senses_NNS -LRB-_-LRB- w_NN -RRB-_-RRB- |_NN -LRB-_-LRB- 3_LS -RRB-_-RRB- where_WRB senses_NNS -LRB-_-LRB- w_NN -RRB-_-RRB- denotes_VBZ the_DT possible_JJ senses_NNS of_IN a_DT word_NN w_NN ,_, U_NNP -LRB-_-LRB- s_PRP -RRB-_-RRB- denotes_NNS concept_NN s_VBZ and_CC its_PRP$ hyponym_NN concepts_NNS ,_, and_CC count_NN -LRB-_-LRB- w_FW -RRB-_-RRB- denotes_VBZ the_DT frequency_NN of_IN word_NN w_NN ._.
This_DT equation_NN weights_NNS count_VBP -LRB-_-LRB- w_NN -RRB-_-RRB- by_IN the_DT ratio_NN of_IN concept_NN s_VBZ and_CC its_PRP$ hyponym_NN concepts_NNS in_IN all_PDT the_DT word_NN senses_NNS of_IN w._NN probability_NN P_NNP -LRB-_-LRB- si_FW -RRB-_-RRB- was_VBD cal_JJ -_: culated_VBN as_IN :_: freq_NN -LRB-_-LRB- si_FW -RRB-_-RRB- P_NNP -LRB-_-LRB- si_FW -RRB-_-RRB- =_SYM N_NNP ,_, -LRB-_-LRB- 4_LS -RRB-_-RRB- where_WRB N_NNP denotes_VBZ the_DT number_NN of_IN word_NN tokens_NNS ._.
Figure_NN 3_CD demonstrates_VBZ the_DT example_NN of_IN the_DT con_NN -_: ceptual_JJ structure1_NN ._.
The_DT nodes_NNS A_DT ∼_CD F_NN represent_VBP the_DT 1The_JJ leaf_NN concepts_NNS below_IN C_NNP ,_, D_NNP ,_, E_NNP ,_, and_CC F_NN are_VBP omitted_VBN ._.
Figure_NN 3_CD :_: Example_NN of_IN Concept_NNP Structure_NNP concepts_NNS and_CC -LRB-_-LRB- a_DT -RRB-_-RRB- ∼_CD -LRB-_-LRB- c_NN -RRB-_-RRB- represent_VBP the_DT words_NNS ,_, which_WDT indicates_VBZ that_IN word_NN -LRB-_-LRB- a_DT -RRB-_-RRB- is_VBZ a_DT polyseme_NN that_WDT have_VBP two_CD word_NN senses_NNS ,_, i.e._FW ,_, C_NNP and_CC D_NNP ._.
When_WRB word_NN -LRB-_-LRB- a_DT -RRB-_-RRB- appeared_VBD twice_RB and_CC word_NN -LRB-_-LRB- b_NN -RRB-_-RRB- appeared_VBD once_RB ,_, the_DT probabilities_NNS are_VBP as_IN illustrated_VBN in_IN Figure_NN 3_CD ._.
Note_VB that_DT C_NNP and_CC D_NNP share_VBP the_DT frequencies_NNS of_IN word_NN -LRB-_-LRB- a_DT -RRB-_-RRB- ._.
A_DT Turing_JJ estimator_NN -LRB-_-LRB- Gale_NNP and_CC Sampson_NNP ,_, 1995_CD -RRB-_-RRB- was_VBD used_VBN for_IN smoothing_VBG with_IN rounding_VBG of_IN the_DT weighted_JJ frequencies_NNS ._.
Concept_NN abstraction_NN sometimes_RB causes_VBZ a_DT prob_NN -_: lem_NN where_WRB some_DT word_NN senses_NNS of_IN a_DT polyseme_NN are_VBP mapped_VBN onto_IN the_DT same_JJ concept_NN ._.
The_DT most_RBS frequent_JJ sense_NN in_IN the_DT corpus_NN has_VBZ been_VBN chosen_VBN for_IN the_DT answer_NN in_IN these_DT cases_NNS ._.
5_CD Transition_NN Probability_NNP SWSM_NNP differentiates_VBZ the_DT sense_NN distribution_NN of_IN the_DT surrounding_VBG words_NNS of_IN each_DT target_NN word_NN before_IN training_NN using_VBG α_NN :_: the_DT transition_NN probability_NN param_NN -_: eter_NN ._.
As_IN our_PRP$ method_NN is_VBZ an_DT unsupervised_JJ approach_NN ,_, we_PRP can_MD not_RB know_VB the_DT word_NN senses_NNS in_IN the_DT corpus_NN ._.
Therefore_RB ,_, SWSM_NNP counts_VBZ the_DT frequencies_NNS of_IN all_PDT the_DT possible_JJ word_NN senses_NNS of_IN the_DT surrounding_VBG words_NNS in_IN the_DT corpus_NN ._.
That_DT is_VBZ ,_, if_IN there_EX are_VBP polysemes_NNS A_DT and_CC B_NNP in_IN the_DT corpus_NN and_CC B_NN is_VBZ a_DT surrounding_VBG word_NN of_IN A_DT ,_, SWSM_NNP counts_VBZ the_DT frequencies_NNS of_IN the_DT senses_NNS by_IN considering_VBG that_IN all_PDT the_DT senses_NNS of_IN B_NNP appeared_VBD near_IN all_DT the_DT senses_NNS of_IN A_DT ._.
That_DT makes_VBZ no_DT difference_NN in_IN the_DT sense_NN distributions_NNS of_IN A_DT ;_: however_RB ,_, if_IN there_EX is_VBZ another_DT polyseme_NN or_CC a_DT monosemic_JJ word_NN ,_, C_NNP ,_, and_CC a_DT sense_NN of_IN C_NNP is_VBZ identical_JJ with_IN a_DT sense_NN of_IN A_DT ,_, the_DT sense_NN distributions_NNS of_IN A_DT will_MD be_VB differentiated_VBN by_IN count_NN -_: ing_VBG the_DT frequencies_NNS of_IN the_DT senses_NNS of_IN C_NNP ._.
As_IN this_DT ex_FW -_: ample_JJ indicates_VBZ ,_, SWSM_NNP expects_VBZ that_IN words_NNS that_WDT have_VBP an_DT identical_JJ sense_NN ,_, like_IN A_DT and_CC C_NNP ,_, have_VBP similar_JJ local_JJ contexts_NNS ._.
SWSM_NNP uses_VBZ these_DT counted_VBN frequencies_NNS to_TO cal_SYM -_: culate_VB the_DT transition_NN parameter_NN α_NN so_IN that_IN the_DT transi_NN -_: tion_NN probabilities_NNS to_TO each_DT concept_NN are_VBP proportional_JJ to_TO the_DT word_NN sense_NN frequencies_NNS of_IN the_DT surround_VBP -_: ing_VBG words_NNS ._.
We_PRP calculate_VBP αsi_FW ,_, sj_NN ,_, i.e._FW ,_, the_DT transition_NN probability_NN from_IN hypernym_NN si_FW to_TO hyponym_VB sj_NN ,_, like_IN that_DT in_IN -LRB-_-LRB- Jiang_NNP and_CC Conrath_NNP ,_, 1997_CD -RRB-_-RRB- as_IN :_: αsi_FW ,_, sj_VBP =_SYM P_NNP -LRB-_-LRB- sj_FW |_FW si_FW -RRB-_-RRB- =_SYM P_NNP -LRB-_-LRB- si_FW ,_, sj_NN -RRB-_-RRB- =_SYM P_NNP -LRB-_-LRB- sj_VBN -RRB-_-RRB- ._.
-LRB-_-LRB- 5_CD -RRB-_-RRB- P_NN -LRB-_-LRB- si_FW -RRB-_-RRB- P_NNP -LRB-_-LRB- si_FW -RRB-_-RRB- In_IN addition_NN ,_, probability_NN P_NNP -LRB-_-LRB- si_FW -RRB-_-RRB- is_VBZ calculated_VBN as_IN :_: P_NN -LRB-_-LRB- si_FW -RRB-_-RRB- =_SYM freq_NN -LRB-_-LRB- si_FW -RRB-_-RRB- ,_, -LRB-_-LRB- 6_CD -RRB-_-RRB- N_NNP where_WRB freq_NN -LRB-_-LRB- si_FW -RRB-_-RRB- denotes_VBZ the_DT frequency_NN of_IN sense_NN si_FW ._.
Moreover_RB ,_, freq_NN -LRB-_-LRB- si_FW -RRB-_-RRB- is_VBZ calculated_VBN like_IN that_DT in_IN -LRB-_-LRB- Resnik_NNP ,_, 1995_CD -RRB-_-RRB- :_: freq_NN -LRB-_-LRB- si_FW -RRB-_-RRB- =_SYM ∑_FW w_FW ∈_FW words_NNS -LRB-_-LRB- si_FW -RRB-_-RRB- count_NN -LRB-_-LRB- w_FW -RRB-_-RRB- ._.
-LRB-_-LRB- 7_CD -RRB-_-RRB- Here_RB ,_, words_NNS -LRB-_-LRB- si_FW -RRB-_-RRB- denotes_VBZ a_DT concept_NN set_VBD that_IN in_IN -_: cludes_NNS si_FW and_CC its_PRP$ hyponyms_NNS ,_, and_CC N_NNP denotes_VBZ the_DT number_NN of_IN the_DT word_NN tokens_NNS in_IN the_DT corpus_NN ._.
How_WRB -_: ever_RB ,_, the_DT probability_NN that_WDT Eq_NNP ._.
-LRB-_-LRB- 7_CD -RRB-_-RRB- will_MD have_VB a_DT prob_NN -_: lem_NN ,_, i.e._FW ,_, the_DT sum_NN of_IN the_DT transition_NN probabilities_NNS from_IN a_DT concept_NN to_TO its_PRP$ hyponyms_NNS is_VBZ not_RB one_CD ._.
Thus_RB ,_, we_PRP calculate_VBP the_DT probability_NN by_IN considering_VBG that_IN the_DT same_JJ concept_NN that_WDT follow_VBP a_DT different_JJ path_NN is_VBZ dif_SYM -_: ferent_NN :_: freq_NN -LRB-_-LRB- si_FW -RRB-_-RRB- =_SYM ∑_SYM ∑_FW path_NN -LRB-_-LRB- si_FW ,_, sj_NN -RRB-_-RRB- count_NN -LRB-_-LRB- w_FW -RRB-_-RRB- ,_, sj_FW ∈_FW L_NNP -LRB-_-LRB- si_FW -RRB-_-RRB- w_SYM ∈_FW words_NNS -LRB-_-LRB- si_FW -RRB-_-RRB- -LRB-_-LRB- 8_CD -RRB-_-RRB- where_WRB path_NN -LRB-_-LRB- si_FW ,_, sj_NN -RRB-_-RRB- denotes_VBZ the_DT number_NN of_IN the_DT paths_NNS from_IN concept_NN si_FW to_TO its_PRP$ hyponym_NN sj_NN and_CC L_NNP -LRB-_-LRB- si_FW -RRB-_-RRB- denotes_VBZ the_DT leaf_NN concepts_NNS below_IN si_NNS ._.
Consequently_RB ,_, the_DT transition_NN probability_NN can_MD be_VB calculated_VBN by_IN di_FW -_: viding_VBG the_DT frequencies_NNS of_IN the_DT hyponym_NN by_IN that_DT of_IN its_PRP$ hypernym_NN ._.
When_WRB word_NN -LRB-_-LRB- a_DT -RRB-_-RRB- appeared_VBD twice_RB and_CC word_NN -LRB-_-LRB- b_NN -RRB-_-RRB- ap_SYM -_: peared_VBN once_RB ,_, the_DT transition_NN probability_NN from_IN A_DT to_TO B_NNP ,_, i.e._FW ,_, αA_NNP ,_, B_NNP is_VBZ 1/2_CD because_IN the_DT frequencies_NNS of_IN A_DT and_CC B_NNP are_VBP six_CD 2_CD and_CC three_CD in_IN Figure_NN 3_CD ._.
Here_RB ,_, p_NN -LRB-_-LRB- pathsl_NN -RRB-_-RRB- ,_, i.e._FW ,_, a_DT transition_NN probability_NN of_IN an_DT arbitrary_JJ path_NN from_IN the_DT root_NN node_NN to_TO a_DT leaf_NN con_NN -_: cept_NN ,_, pathsl_NN ,_, is_VBZ :_: p_NN -LRB-_-LRB- pathsl_NN -RRB-_-RRB- 2It_JJ is_VBZ sum_NN of_IN twice_RB from_IN path_NN ABD_NNP -LRB-_-LRB- a_DT -RRB-_-RRB- ,_, twice_RB from_IN path_NN AC_NNP -LRB-_-LRB- a_DT -RRB-_-RRB- ,_, once_RB from_IN path_NN ABE_NNP -LRB-_-LRB- b_NN -RRB-_-RRB- ,_, and_CC once_RB from_IN path_NN ACE_NNP -LRB-_-LRB- b_NN -RRB-_-RRB- ._.
=_SYM freq_NN -LRB-_-LRB- c1_CD -RRB-_-RRB- freq_NN -LRB-_-LRB- c2_FW -RRB-_-RRB- ..._: freq_NN -LRB-_-LRB- cn_VBN -RRB-_-RRB- freq_NN -LRB-_-LRB- sl_NN -RRB-_-RRB- freq_NN -LRB-_-LRB- sroot_NN -RRB-_-RRB- freq_NN -LRB-_-LRB- c1_CD -RRB-_-RRB- freq_NN -LRB-_-LRB- cn_SYM −_SYM 1_LS -RRB-_-RRB- freq_NN -LRB-_-LRB- cn_VBN -RRB-_-RRB- freq_NN -LRB-_-LRB- sl_NN -RRB-_-RRB- =_SYM freq_NN -LRB-_-LRB- sroot_NN -RRB-_-RRB- ,_, -LRB-_-LRB- 9_CD -RRB-_-RRB- where_WRB c1c2_NN ..._: cn_NN denote_VBP the_DT concepts_NNS in_IN pathsl_NN ._.
Therefore_RB ,_, when_WRB we_PRP set_VBP the_DT frequency_NN of_IN the_DT word_NN sense_NN frequencies_NNS of_IN sl_NN ,_, the_DT surrounding_VBG words_NNS ,_, as_IN freq_NN -LRB-_-LRB- s_PRP -RRB-_-RRB- ,_, p_NN -LRB-_-LRB- path_NN -RRB-_-RRB- are_VBP proportional_JJ to_TO the_DT fre_NN -_: l_NN sl_NN quency_NN ._.
We_PRP eventually_RB used_VBD the_DT following_VBG transition_NN probability_NN parameter_NN to_TO avoid_VB the_DT zero_CD frequency_NN problem_NN :_: s_PRP Saαa_NNP +_IN Sbαb_NNP ,_, -LRB-_-LRB- 10_CD -RRB-_-RRB- where_WRB αa_NN denotes_VBZ a_DT transition_NN probability_NN parame_NN -_: ter_NN where_WRB all_PDT the_DT leaf_NN nodes_NNS have_VBP the_DT same_JJ amount_NN probability_NN and_CC αbs_NNS denotes_VBZ the_DT transition_NN probabil_NN -_: ity_NN parameter_NN that_WDT is_VBZ pre-trained_JJ using_VBG the_DT above_JJ equations_NNS ._.
Sa_NNP and_CC Sb_NNP are_VBP constant_JJ numbers_NNS to_TO con_VB -_: trol_IN the_DT effect_NN of_IN pre-processing_NN ._.
The_DT transition_NN probability_NN parameter_NN where_WRB all_PDT the_DT leaf_NN nodes_NNS have_VBP the_DT same_JJ amount_NN probability_NN ,_, αa_NN ,_, is_VBZ calculated_VBN by_IN assuming_VBG that_IN the_DT frequencies_NNS of_IN all_PDT the_DT leaf_NN nodes_NNS are_VBP as_IN follows_VBZ ._.
3_CD SWSM_NNP estimates_VBZ the_DT word_NN sense_NN ,_, s_VBZ ,_, using_VBG Gibbs_NNP sampling_NN -LRB-_-LRB- Liu_NNP ,_, 1994_CD -RRB-_-RRB- ._.
As_IN described_VBN in_IN Section_NN 3_CD ,_, the_DT conditional_JJ probability_NN of_IN the_DT model_NN is_VBZ in_IN Eq_NNP ._.
-LRB-_-LRB- 12_CD -RRB-_-RRB- ._.
P_NN -LRB-_-LRB- s_PRP ,_, c_NN ,_, θ_NN ,_, φ_FW |_FW w_FW -RRB-_-RRB- =_SYM ∏_CD W_NNP ∏_CD S_NNP ∏_CD N_NNP ∑_CD −_CD i_FW x_LS freq_NN -LRB-_-LRB- sl_NN -RRB-_-RRB- =_SYM 6_CD Sense_NN Estimation_NN using_VBG Gibbs_NNP 1_CD -LRB-_-LRB- 11_CD -RRB-_-RRB- path_NN -LRB-_-LRB- sroot_NN ,_, sl_NN -RRB-_-RRB- the_DT ith_NN variate_NN ._.
my_PRP$ -LRB-_-LRB- j_NN ,_, yj_NN -RRB-_-RRB- is_VBZ the_DT frequency_NN where_WRB word_NN sense_NN yj_NN appear_VBP before_IN the_DT jth_NN surrounding_VBG word_NN sense_NN in_IN y_NN and_CC it_PRP can_MD be_VB ignored_VBN if_IN yj_NN ap_SYM -_: peared_VBN once_RB in_IN y_NN ._.
We_PRP approximately_RB and_CC determi_SYM -_: nately_RB assign_VB the_DT sequence_NN of_IN the_DT word_NN senses_NNS to_TO y_VB ,_, calculate_VB each_DT probability_NN of_IN si_FW ,_, and_CC determine_VB si_FW ,_, i.e._FW ,_, the_DT word_NN sense_NN that_WDT corresponds_VBZ to_TO word_NN wi_NNS ._.
If_IN the_DT probability_NN distributions_NNS of_IN word_NN senses_NNS are_VBP replaced_VBN with_IN WORDNET-WALK_NNP ,_, the_DT last_JJ part_NN of_IN the_DT right_JJ side_NN of_IN Eq_NNP ._.
-LRB-_-LRB- 13_CD -RRB-_-RRB- will_MD also_RB be_VB re_SYM -_: placed_VBN ._.
When_WRB rj_SYM ,0_CD ,_, rj_SYM ,1_CD ,_, ..._: ,_, rj_NN ,_, l_NN denotes_VBZ the_DT path_NN from_IN the_DT root_NN concept_NN of_IN word_NN sense_NN yj_NN in_IN y_NN ,_, we_PRP obtain_VBP Eq_NNP ._.
-LRB-_-LRB- 14_CD -RRB-_-RRB- by_IN calculating_VBG the_DT following_VBG val_NN -_: ues_NNS of_IN all_DT combinations_NNS from_IN the_DT root_NN concept_NN for_IN all_DT word_NN senses_NNS ,_, and_CC summing_VBG them_PRP ._.
P_NN -LRB-_-LRB- θk_FW |_FW γk_FW -RRB-_-RRB- P_NN -LRB-_-LRB- φj_FW |_FW τj_FW -RRB-_-RRB- P_NNP -LRB-_-LRB- si_FW |_FW θwi_FW -RRB-_-RRB- P_NNP -LRB-_-LRB- ci_FW |_FW φsj_FW ,_, w_NN -RRB-_-RRB- k_NN =_SYM 1_CD j_NN =_SYM 1_CD i_FW =_SYM 1_CD -LRB-_-LRB- 12_CD -RRB-_-RRB- We_PRP calculate_VBP the_DT conditional_JJ distribution_NN that_WDT is_VBZ necessary_JJ for_IN sampling_NN ._.
We_PRP regard_VBP variants_NNS except_IN those_DT for_IN word_NN wi_NNS as_IN constant_JJ numbers_NNS ._.
The_DT prob_NN -_: ability_NN distribution_NN ,_, φ_NN ,_, of_IN the_DT word_NN sense_NN is_VBZ actu_SYM -_: ally_NN replaced_VBN by_IN WORDNET-WALK_NNP in_IN the_DT word_NN sense_NN generation_NN process_NN and_CC it_PRP will_MD have_VB plural_JJ 3The_JJ reason_NN we_PRP did_VBD not_RB set_VBN the_DT frequencies_NNS of_IN all_PDT the_DT leaf_NN nodes_VBZ to_TO one_CD -LRB-_-LRB- freq_NN -LRB-_-LRB- sl_NN -RRB-_-RRB- =_SYM 1_LS -RRB-_-RRB- is_VBZ as_IN follows_VBZ ._.
If_IN so_RB ,_, all_PDT the_DT probabilities_NNS of_IN all_PDT the_DT paths_NNS from_IN the_DT root_NN node_NN to_TO each_DT leaf_NN node_NN would_MD have_VB been_VBN the_DT same_JJ ._.
However_RB ,_, the_DT more_JJR paths_NNS from_IN the_DT root_NN node_VBD a_DT leaf_NN node_NN has_VBZ ,_, the_DT higher_JJR the_DT probability_NN the_DT leaf_NN node_NN will_MD have_VB ._.
We_PRP used_VBD Eq_NNP ._.
-LRB-_-LRB- 11_CD -RRB-_-RRB- so_RB that_IN all_PDT the_DT leaf_NN nodes_NNS would_MD have_VB the_DT same_JJ probability_NN ._.
T_NNP −_CD i_FW x_LS ,_, rj_NN ,_, p_NN ,_, rj_NN ,_, p_NN +1_CD multinomial_JJ distributions_NNS of_IN the_DT transitions_NNS to_TO the_DT hyponym_NN concepts_NNS ._.
We_PRP calculated_VBD the_DT conditional_JJ distribution_NN P_NNP -LRB-_-LRB- si_FW ,_, ci_FW |_FW s_PRP −_FW i_FW ,_, c_NN −_CD i_FW ,_, w_NN -RRB-_-RRB- as_IN :_: P_NN -LRB-_-LRB- si_FW =_SYM x_LS ,_, ci_FW =_SYM y_FW |_FW s_PRP −_FW i_FW ,_, c_NN −_CD i_FW ,_, w_NN -RRB-_-RRB- |_FW y_FW |_FW −_FW i_FW ∝_FW -LRB-_-LRB- n_SYM −_FW i_FW +_FW γ_FW -RRB-_-RRB- ·_SYM ∏_SYM ∑_CD -LRB-_-LRB- nx_NN ,_, yj_FW +_FW my_PRP$ -LRB-_-LRB- j_NN ,_, yi_FW -RRB-_-RRB- +_SYM τx_FW ,_, yj_NN -RRB-_-RRB- ,_, wi_FW ,_, x_LS -LRB-_-LRB- n_SYM −_FW i_FW +_FW τ_FW j_FW =_SYM 1_CD sen_NN x_LS ,_, sen_NN x_LS ,_, sen_NN -RRB-_-RRB- +_NN -LRB-_-LRB- j_SYM −_SYM 1_LS -RRB-_-RRB- -LRB-_-LRB- 13_CD -RRB-_-RRB- where_WRB x_LS and_CC y_JJ correspond_VBP to_TO the_DT real_JJ values_NNS of_IN word_NN sense_NN si_NNS and_CC the_DT vector_NN of_IN the_DT word_NN senses_NNS of_IN the_DT surrounding_VBG words_NNS ,_, c_NN ._.
n_SYM −_FW i_FW denotes_VBZ the_DT num_NN -_: i_FW wi_FW ,_, x_SYM ber_FW of_IN x_LS ,_, i.e._FW ,_, the_DT word_NN senses_NNS that_WDT are_VBP assigned_VBN to_TO word_NN wi_NNS except_IN for_IN the_DT ith_NN variate_NN ,_, which_WDT is_VBZ the_DT sampling_NN target_NN now_RB ._.
n_SYM −_FW i_FW denotes_VBZ the_DT frequency_NN x_LS ,_, yi_FW where_WRB yj_NN appears_VBZ around_IN word_NN sense_NN x_LS except_IN for_IN Sampling_VBG |_FW y_FW |_FW l_NN −_NN 1_CD ∏_CD ∏_CD -LCB-_-LRB- T_NN −_CD i_FW /_FW -LCB-_-LRB- +_NN m_NN -LRB-_-LRB- j_FW ,_, r_NN ,_, r_NN -RRB-_-RRB- y_NN j_NN ,_, p_NN j_NN ,_, p_NN +1_CD x_LS ,_, rj_NN ,_, p_NN ,_, rj_NN ,_, p_NN +1_CD +_NN S_NNP α_NNP +_NNP S_NNP αx_FW j_FW =_SYM 1_CD p_NN =_SYM 1_CD -RCB-_-RRB- -LRB-_-LRB- Tx_NNP ,_, rj_NNP ,_, p_NN ,_, r_NN +_NN my_PRP$ -LRB-_-LRB- j_NN ,_, rj_NN ,_, p_NN ,_, r_NN -RRB-_-RRB- +_SYM Sbαb_NNP ,_, rj_NNP ,_, p_NN ,_, r_NN -RRB-_-RRB- +_FW Sa_FW -RCB-_-RRB- ,_, -LRB-_-LRB- 14_CD -RRB-_-RRB- a_DT a_DT ,_, rj_NN ,_, p_NN ,_, rj_NN ,_, p_NN +1_CD b_NN r_NN b_NN ,_, rj_NN ,_, p_NN ,_, rj_NN ,_, p_NN +1_CD where_WRB the_DT word_NN sense_NN of_IN the_DT surrounding_VBG words_NNS of_IN word_NN sense_NN x_LS pass_VB the_DT link_NN from_IN concept_NN rj_NN ,_, p_NN to_TO concept_NN rj_NN ,_, p_NN +1_CD except_IN for_IN the_DT ith_NN variate_NN ._.
my_PRP$ -LRB-_-LRB- j_NN ,_, rj_NN ,_, p_NN ,_, rj_NN ,_, p_NN +1_CD -RRB-_-RRB- denotes_VBZ the_DT frequency_NN where_WRB the_DT link_NN from_IN con_NN -_: cept_JJ rj_NN ,_, p_NN to_TO concept_NN rj_NN ,_, p_SYM +1_CD is_VBZ passed_VBN before_IN the_DT jth_NN path_NN ._.
The_DT value_NN of_IN Tsi_NNP should_MD be_VB updated_VBN after_IN word_NN sense_NN si_FW is_VBZ assigned_VBN ._.
Thus_RB ,_, the_DT paths_NNS of_IN the_DT word_NN senses_NNS of_IN the_DT surrounding_VBG words_NNS are_VBP neces_NNS -_: sary_JJ ._.
This_DT time_NN ,_, we_PRP assign_VBP values_NNS proportional_JJ to_TO each_DT probability_NN to_TO each_DT path_NN ._.
When_WRB path1_NN ,_, path2_NN ,_, denotes_VBZ the_DT frequency_NN where_WRB ..._: ,_, pathn_NN denote_VBP the_DT paths_NNS from_IN the_DT root_NN concept_NN to_TO word_NN sense_NN ci_NNS ,_, j_NN ,_, i.e._FW ,_, a_DT word_NN sense_NN of_IN surround_VBP -_: ing_VBG words_NNS ci_NNS of_IN word_NN sense_NN si_FW ,_, we_PRP added_VBD following_VBG value_NN to_TO Tsi_NNP ,_, pathk_NN ,_, which_WDT is_VBZ the_DT frequency_NN where_WRB a_DT link_NN in_IN pathk_NN is_VBZ passed_VBN ,_, for_IN each_DT word_NN sense_NN ci_NNS ,_, j_NN ._.
∑_CD P_NN -LRB-_-LRB- pathk_FW |_FW si_FW -RRB-_-RRB- -LRB-_-LRB- 15_CD -RRB-_-RRB- nl_NN =_SYM 1_CD P_NN -LRB-_-LRB- pathl_FW |_FW si_FW -RRB-_-RRB- The_DT probability_NN p_NN -LRB-_-LRB- pathk_FW |_FW si_FW -RRB-_-RRB- is_VBZ as_IN follows_VBZ ,_, when_WRB r1_CD ,_, r2_CD ,_, ·_CD ·_CD ·_NN ,_, rl_NN denote_VBP the_DT concepts_NNS that_WDT pathk_VBP fol_SYM -_: lows_NNS ._.
P_NN -LRB-_-LRB- pathk_FW |_FW si_FW -RRB-_-RRB- l_NN −_NN 1_CD −_CD i_FW si_FW second_JJ version_NN of_IN the_DT EDR_NNP electronic_JJ dictionary_NN ._.
All_PDT the_DT nouns_NNS and_CC verbs_NNS that_WDT could_MD be_VB followed_VBN from_IN the_DT root_NN node_NN in_IN the_DT concept_NN dictionary_NN were_VBD used_VBN for_IN the_DT experiments_NNS ._.
In_IN addition_NN ,_, we_PRP added_VBD some_DT nouns_NNS by_IN deleting_VBG ``_`` する_FW -LRB-_-LRB- suru_NN ,_, the_DT suffix_NN that_WDT means_VBZ do_VB -RRB-_-RRB- ''_'' from_IN nominal_JJ verbs_NNS ,_, to_TO the_DT con_NN -_: cept_JJ dictionary_NNS ._.
Consequently_RB ,_, the_DT concept_NN dic_SYM -_: tionary_JJ included_VBD 263,757_CD words_NNS and_CC 406,710_CD leaf_NN concepts_NNS ,_, and_CC 199,430_CD leaf_NN concepts_NNS in_IN them_PRP were_VBD used_VBN for_IN the_DT experiments_NNS ._.
The_DT internal_JJ nodes_NNS that_WDT were_VBD used_VBN for_IN the_DT experiments_NNS were_VBD 203,565_CD con_NN -_: cepts_NNS ._.
Most_JJS of_IN the_DT concepts_NNS that_WDT were_VBD not_RB used_VBN were_VBD those_DT that_WDT had_VBD no_DT links_NNS to_TO Japanese_JJ words_NNS ._.
In_IN addition_NN ,_, the_DT concept_NN dictionary_NN included_VBD 13,846_CD concepts_NNS and_CC 6,905_CD leaf_NN concepts_NNS after_IN concept_NN abstraction_NN ._.
The_DT threshold_NN value_NN we_PRP used_VBD was_VBD 5.0_CD ×_CD 10_CD −_NN 5_CD ._.
The_DT Japanese_JJ corpus_NN consisted_VBD of_IN seven_CD sub_SYM -_: corpora_NN :_: the_DT Nikkei_NNP ,_, the_DT Asahi_NNP Shimbun_NNP ,_, AERA_NNP ,_, Heibonsha_NNP World_NNP Encyclopedia_NNP ,_, Encyclopedic_NNP Dictionary_NNP of_IN Computer_NNP Science_NNP ,_, Magazines_NNS ,_, and_CC Collections_NNS ._.
They_PRP were_VBD annotated_VBN with_IN word_NN sense_NN tags_NNS that_WDT were_VBD the_DT concepts_NNS in_IN the_DT concept_NN dictio_NN -_: nary_PDT ._.
Table_NNP 1_CD summarizes_VBZ the_DT numbers_NNS of_IN docu_NN -_: ments_NNS and_CC word_NN tokens_NNS according_VBG to_TO the_DT type_NN of_IN text_NN ._.
The_DT documents_NNS in_IN this_DT corpus_NN only_RB consisted_VBD of_IN one_CD sentence_NN ._.
∑_CD Tsi_NNP ,_, rp_NN ,_, rp_NN +1_CD +_SYM Saαa_NNP ,_, rp_NN ,_, rp_NN +1_CD +_SYM Sbαb_NNP ,_, rp_NN ,_, rp_NN +1_CD =_SYM ∑_FW -LRB-_-LRB- T_NNP −_CD i_FW +_FW S_NNP αsi_FW p_FW =_SYM 1_CD r_NN si_FW ,_, rp_NN ,_, r_NN b_NN b_NN ,_, rp_NN ,_, r_NN -RRB-_-RRB- +_NN S_NNP a_DT Concepts_NNS that_WDT have_VBP many_JJ paths_NNS from_IN the_DT root_NN con_NN -_: cept_NN are_VBP concepts_NNS that_WDT have_VBP many_JJ properties_NNS ._.
Thus_RB ,_, we_PRP can_MD view_VB these_DT cases_NNS as_IN that_DT of_IN an_DT appearance_NN of_IN word_NN sense_NN ci_NNS ,_, j_NN that_WDT was_VBD assigned_VBN to_TO multiple_JJ properties_NNS ._.
Algorithm_NNP 1_CD demonstrates_VBZ the_DT algorithm_NN of_IN one_CD iteration_NN in_IN Gibbs_NNP Sampling_NNP of_IN SWSM_NNP ._.
Note_VB that_DT x_LS and_CC y_NN are_VBP sampled_VBN according_VBG to_TO Eq_NNP ._.
-LRB-_-LRB- 13_CD -RRB-_-RRB- where_WRB the_DT last_JJ part_NN on_IN the_DT right_JJ side_NN is_VBZ replaced_VBN with_IN Eq_NNP ._.
-LRB-_-LRB- 14_CD -RRB-_-RRB- and_CC each_DT Tsi_NNP ,_, pathk_NN is_VBZ updated_VBN with_IN Eq_NNP ._.
-LRB-_-LRB- 15_CD -RRB-_-RRB- ._.
Algorithm_NNP 1_CD Processes_NNS of_IN One_CD Iteration_NN in_IN Gibbs_NNP Sampling_NNP of_IN SWSM_NNP Require_NNP :_: Disambiguate_NNP the_DT word_NN sense_NN si_FW in_IN text_NN for_IN each_DT word_NN wi_NN in_IN text_NN do_VBP nwi_NNS ,_, si_FW ⇐_FW nwi_FW ,_, si_FW −_FW 1_CD for_IN each_DT word_NN sense_NN ci_NNS ,_, j_NN in_IN ci_NNS do_VBP for_IN each_DT path_NN pathk_NN for_IN ci_NNS ,_, j_NN do_VBP -LRB-_-LRB- 16_CD -RRB-_-RRB- Type_NN of_IN Text_NNP Docs_NNP Word_NNP tokens_NNS The_DT Nikkei_NNP 5,018_CD 121,301_CD The_DT Asahi_NNP Shimbun_NNP 91,400_CD 2,272,555_CD AERA_NNP 49,589_CD 1,183,897_CD Heibonsha_NNP World_NNP Encyclopedia_NNP 10,072_CD 284,059_CD Encyclopedic_NNP Dictionary_NNP of_IN Computer_NNP Science_NNP 13,578_CD 357,607_CD Magazines_NNS 21,199_CD 528,452_CD Collections_NNS 16,946_CD 368,285_CD Tsi_NNP ,_, pathk_NN end_NN for_IN end_NN for_IN ci_FW ⇐_FW y_FW si_FW ⇐_FW x_LS P_NNP -LRB-_-LRB- pathk_FW |_FW si_FW -RRB-_-RRB- ⇐_CD Tsi_NNP ,_, pathk_NN −_CD ∑_CD nl_NN =_SYM 1_CD P_NN -LRB-_-LRB- pathl_FW |_FW si_FW -RRB-_-RRB- nwi_NNS ,_, si_FW ⇐_FW nwi_FW ,_, si_FW +1_FW for_IN each_DT word_NN sense_NN ci_NNS ,_, j_NN in_IN ci_NNS do_VBP for_IN each_DT path_NN pathk_NN for_IN ci_NNS ,_, j_NN do_VBP Table_NNP 1_CD :_: Summary_NNP of_IN Sub-corpora_NNP ._.
We_PRP used_VBD the_DT Nikkei_NNP for_IN evaluation_NN ._.
The_DT other_JJ six_CD sub-corpora_NN were_VBD used_VBN for_IN pre-processing_NN in_IN an_DT unsupervised_JJ manner_NN ._.
The_DT EDR_NNP Japanese_NNP corpus_NN did_VBD not_RB include_VB the_DT basic_JJ forms_NNS of_IN words_NNS ._.
Thus_RB we_PRP used_VBD a_DT morphological_JJ analyzer_NN ,_, Mecab4_NNP ,_, to_TO iden_VB -_: tify_VB the_DT basic_JJ forms_NNS of_IN words_NNS in_IN the_DT corpus_NN ._.
Shirai_NNP -LRB-_-LRB- 2002_CD -RRB-_-RRB- set_VBN up_RP the_DT three_CD difficulty_NN classes_NNS listed_VBN in_IN Table_NNP 2_CD ._.
Tables_NNS 7_CD and_CC 3_CD indicate_VBP the_DT num_NN -_: ber_NN of_IN word_NN types_NNS ,_, noun_NN tokens_NNS ,_, and_CC verb_NN tokens_NNS according_VBG to_TO difficulty_NN and_CC the_DT average_JJ polysemy_NN 4_CD https://github.com/jordwest/mecab-docs-en_JJ Tsi_NNP ,_, pathk_NN end_NN for_IN end_NN for_IN end_NN for_IN 7_CD Data_NNP P_NNP -LRB-_-LRB- pathk_FW |_FW si_FW -RRB-_-RRB- ⇐_CD Tsi_NNP ,_, pathk_NN +_NN ∑_CD nl_NN =_SYM 1_CD P_NN -LRB-_-LRB- pathl_FW |_FW si_FW -RRB-_-RRB- We_PRP used_VBD the_DT Japanese_JJ word_NN dictionary_NN ,_, the_DT con_NN -_: cept_JJ dictionary_NNS ,_, and_CC the_DT Japanese_JJ corpus_NN of_IN the_DT of_IN target_NN words_NNS according_VBG to_TO difficulty_NN ._.
Only_RB words_NNS that_IN appeared_VBD more_JJR than_IN four_CD times_NNS in_IN the_DT corpus_NN were_VBD classified_VBN based_VBN on_IN difficulty_NN ._.
Table_NNP 2_CD :_: Difficulty_NN of_IN disambiguation_NN Table_NNP 3_CD :_: Types_NNS and_CC tokens_NNS of_IN words_NNS according_VBG to_TO difficulty_NN Table_NNP 4_CD :_: Average_JJ polysemy_NN of_IN target_NN words_NNS ac_SYM -_: cording_NN to_TO difficulty_NN 8_CD Result_NN We_PRP used_VBD nouns_NNS and_CC independent_JJ verbs_NNS in_IN a_DT local_JJ window_NN whose_WP$ size_NN was_VBD 2N_CD except_IN for_IN marks_NNS ,_, as_IN the_DT surrounding_VBG words_NNS ._.
We_PRP set_VBP N_NNP =_SYM 10_CD in_IN this_DT research_NN ._.
In_IN addition_NN ,_, we_PRP deleted_VBD word_NN senses_NNS that_WDT appeared_VBD only_RB once_RB through_IN pre-processing_NN ._.
We_PRP performed_VBD experiments_NNS using_VBG the_DT nine_CD set_NN -_: tings_NNS of_IN the_DT transition_NN probability_NN parameters_NNS :_: Sa_NNP =_SYM -LCB-_-LRB- 1.0_CD ,_, 5.0_CD ,_, 10.0_CD -RCB-_-RRB- and_CC Sb_NNP =_SYM -LCB-_-LRB- 10.0_CD ,_, 15.0_CD ,_, 20.0_CD -RCB-_-RRB- in_IN Eq_NNP ._.
-LRB-_-LRB- 10_CD -RRB-_-RRB- ._.
We_PRP set_VBP the_DT hyper-parameter_NN γ_NN =_SYM 0.1_CD in_IN Eq_NNP ._.
-LRB-_-LRB- 2_LS -RRB-_-RRB- for_IN all_DT experiments_NNS ._.
Gibbs_NNS sampling_VBG was_VBD it_PRP -_: erated_VBD 2,000_CD times_NNS and_CC the_DT most_RBS frequent_JJ senses_NNS of_IN 100_CD samples_NNS in_IN the_DT latter_JJ 1,800_CD times_NNS were_VBD chosen_VBN for_IN the_DT answers_NNS ._.
We_PRP performed_VBD experiments_NNS three_CD times_NNS per_IN setting_VBG for_IN the_DT transition_NN probability_NN pa_NN -_: rameters_NNS and_CC calculated_VBN the_DT average_JJ accuracies_NNS ._.
Table_NNP 4_CD summaries_NNS the_DT results_NNS ._.
It_PRP includes_VBZ the_DT micro_NN -_: and_CC macro-averaged_JJ accuracies_NNS of_IN SWSM_NNP for_IN the_DT nine_CD settings_NNS of_IN the_DT parameters_NNS ,_, those_DT of_IN the_DT random_JJ baseline_NN ,_, and_CC those_DT of_IN LDAWN_NNP 5_CD ._.
The_DT ex_FW -_: periments_NNS for_IN the_DT random_JJ baseline_NN were_VBD performed_VBN 1,000_CD times_NNS ._.
The_DT best_JJS results_NNS are_VBP indicated_VBN in_IN bold_JJ -_: face_NN ._.
Difficulty_NN Entoropy_NNP Easy_NNP E_NNP -LRB-_-LRB- w_VBN -RRB-_-RRB- <_SYM 0.5_CD Normal_JJ 0.5_CD ≤_CD E_NNP -LRB-_-LRB- w_VBN -RRB-_-RRB- <_SYM 1_CD Hard_JJ 1_CD ≤_CD E_NNP -LRB-_-LRB- w_NN -RRB-_-RRB- Sa_NN Sb_JJ micro_NN macro_NN 1_CD 5_CD 10_CD 10_CD 10_CD 10_CD 38.91_CD %_NN 38.67_CD %_NN 37.62_CD %_NN 42.58_CD %_NN 42.42_CD %_NN 42.37_CD %_NN 1_CD 5_CD 10_CD 15_CD 15_CD 15_CD 39.20_CD %_NN 38.23_CD %_NN 38.41_CD %_NN 42.43_CD %_NN 42.29_CD %_NN 42.17_CD %_NN 1_CD 5_CD 10_CD 20_CD 20_CD 20_CD 37.78_CD %_NN 39.60_CD %_NN 36.67_CD %_NN 42.26_CD %_NN 42.09_CD %_NN 42.04_CD %_NN Random_NNP baseline_NN 30.97_CD %_NN 36.63_CD %_NN LDAWN_NNP 36.12_CD %_NN 42.51_CD %_NN Difficulty_NN Word_NN types_NNS Tokens_NNP -LRB-_-LRB- N_NNP -RRB-_-RRB- Tokens_NNP -LRB-_-LRB- V_NNP -RRB-_-RRB- All_DT 4,822_CD 12,149_CD 6,199_CD Easy_NNP 399_CD 3,630_CD 1,723_CD Normal_JJ 337_CD 2,929_CD 1,541_CD Hard_JJ 105_CD 1,028_CD 1,196_CD Table_NNP 5_CD :_: Summary_NNP of_IN result_NN The_DT table_NN indicates_VBZ that_IN our_PRP$ model_NN ,_, SWSM_NNP ,_, was_VBD better_JJR than_IN both_DT the_DT random_JJ baseline_NN and_CC LDAWN_NNP ._.
Although_IN the_DT macro-averaged_JJ accura_NN -_: cies_NNS of_IN LDAWN_NNP were_VBD better_JJR than_IN those_DT of_IN SWSM_NNP except_IN when_WRB Sa_NNP =_SYM 1_CD and_CC Sb_NNP =_SYM 10_CD ,_, both_CC the_DT micro_NN -_: and_CC macro-averaged_JJ accuracies_NNS of_IN SWSM_NNP outperformed_VBD those_DT of_IN LDAWN_NNP when_WRB Sa_NNP =_SYM 1_CD and_CC Sb_NNP =_SYM 10_CD ._.
Tables_NNS 5_CD and_CC 6_CD summarize_VBP the_DT micro-averaged_JJ accuracies_NNS of_IN all_DT words_NNS and_CC the_DT macro-averaged_JJ accuracies_NNS of_IN all_DT words_NNS ._.
SWSM1_CD and_CC SWSM2_CD in_IN these_DT tables_NNS denote_VBP the_DT SWSMs_NNS with_IN the_DT set_NN -_: ting_VBG when_WRB the_DT best_JJS macro-averaged_JJ accuracy_NN for_IN all_DT words_NNS was_VBD obtained_VBN -LRB-_-LRB- Sa_NNP =_SYM 1_CD and_CC Sb_NNP =_SYM 10_CD -RRB-_-RRB- and_CC with_IN the_DT setting_NN when_WRB the_DT best_JJS micro-averaged_JJ ac_SYM -_: curacy_NN for_IN all_DT words_NNS was_VBD obtained_VBN -LRB-_-LRB- Sa_NNP =_SYM 5_CD and_CC Sb_NNP =_SYM 20_CD -RRB-_-RRB- ._.
The_DT best_JJS results_NNS in_IN each_DT table_NN are_VBP indicated_VBN in_IN boldface_NN ._.
These_DT tables_NNS indicate_VBP that_IN SWSM1_CD or_CC SWSM2_CD was_VBD always_RB better_JJR than_IN both_DT 5The_JJ best_JJS results_NNS for_IN the_DT 13_CD settings_NNS ._.
We_PRP changed_VBD the_DT num_NN -_: ber_NN of_IN topics_NNS and_CC the_DT scale_NN parameters_NNS according_VBG to_TO -LRB-_-LRB- Boyd_NNP -_: Graber_NNP et_FW al._FW ,_, 2007_CD -RRB-_-RRB- ._.
In_IN addition_NN ,_, we_PRP tested_VBD that_IN the_DT effect_NN of_IN the_DT size_NN of_IN a_DT text_NN ,_, a_DT sentence_NN ,_, or_CC a_DT whole_JJ daily_JJ publica_NN -_: tion_NN because_IN a_DT document_NN only_RB consisted_VBD of_IN a_DT sentence_NN in_IN our_PRP$ Japanese_JJ corpus_NN and_CC there_EX was_VBD no_DT clues_NNS that_WDT indicated_VBD to_TO what_WP article_NN the_DT sentence_NN belonged_VBD ._.
Furthermore_RB ,_, we_PRP tested_VBD two_CD kinds_NNS of_IN transition_NN probabilities_NNS ,_, those_DT that_WDT used_VBD priors_NNS and_CC those_DT where_WRB all_PDT the_DT leaf_NN nodes_NNS had_VBD the_DT same_JJ amount_NN proba_NN -_: bility_NN ._.
The_DT best_JJS was_VBD the_DT setting_NN where_WRB there_EX were_VBD 32_CD topics_NNS ,_, scale_NN parameter_NN S_NNP was_VBD 10_CD ,_, the_DT text_NN size_NN was_VBD a_DT sentence_NN ,_, and_CC the_DT transition_NN probabilities_NNS were_VBD those_DT where_WRB all_PDT the_DT leaf_NN nodes_NNS had_VBD the_DT same_JJ amount_NN probability_NN ._.
The_DT details_NNS are_VBP similar_JJ to_TO those_DT in_IN -LRB-_-LRB- Sasaki_NNP et_FW al._FW ,_, 2014_CD -RRB-_-RRB- ._.
However_RB ,_, we_PRP performed_VBD the_DT ex_FW -_: periments_NNS three_CD times_NNS and_CC calculated_VBN the_DT accuracies_NNS but_CC they_PRP only_RB performed_VBD the_DT experiments_NNS twice_RB ._.
Difficulty_NN Noun_NN polysemy_JJ Verb_NN polysemy_NN All_DT 4.2_CD 5.5_CD Easy_NNP 3.9_CD 4.0_CD Normal_JJ 4.4_CD 5.3_CD Hard_JJ 8.6_CD 10.3_CD the_DT random_JJ baseline_NN and_CC LDAWN_NNP ._.
Table_NNP 6_CD :_: Micro-averaged_JJ accuracies_NNS for_IN all_DT words_NNS -LRB-_-LRB- %_NN -RRB-_-RRB- Table_NNP 7_CD :_: Macro-averaged_JJ accuracies_NNS for_IN all_DT words_NNS -LRB-_-LRB- %_NN -RRB-_-RRB- Table_NNP 6_CD indicates_VBZ that_IN the_DT macro_NN averaged_VBD accu_SYM -_: racies_NNS of_IN LDAWN_NNP -LRB-_-LRB- 42.51_CD %_NN -RRB-_-RRB- outperformed_VBD those_DT of_IN SWSM2_NNP -LRB-_-LRB- 42.09_CD %_NN -RRB-_-RRB- when_WRB all_PDT the_DT words_NNS were_VBD evaluated_VBN ._.
However_RB ,_, the_DT same_JJ table_NN reveals_VBZ that_IN the_DT reason_NN is_VBZ due_JJ to_TO the_DT results_NNS for_IN the_DT easy_JJ class_NN words_NNS ,_, i.e._FW ,_, the_DT words_NNS that_WDT almost_RB always_RB had_VBD the_DT same_JJ sense_NN ._.
In_IN addition_NN ,_, Tables_NNP 5_CD and_CC 6_CD indicate_VBP that_IN SWSM_NNP clearly_RB outperformed_VBD the_DT other_JJ sys_SYM -_: tems_NNS for_IN words_NNS in_IN the_DT normal_JJ and_CC hard_JJ classes_NNS ._.
9_CD Discussion_NNP The_NNP examples_NNS ``_`` 可能性_FW -LRB-_-LRB- possibility_NN -RRB-_-RRB- ''_'' and_CC ``_`` 洗う_FW -LRB-_-LRB- wash_NN -RRB-_-RRB- ''_'' were_VBD cases_NNS where_WRB most_RBS senses_NNS were_VBD cor_SYM -_: rectly_NN predicted_VBD ._.
``_`` 可能性_FW -LRB-_-LRB- possibility_NN -RRB-_-RRB- ''_'' is_VBZ a_DT hard_JJ -_: class_NN word_NN and_CC it_PRP appeared_VBD 18_CD times_NNS in_IN the_DT corpus_NN ._.
SWSM_NNP correctly_RB predicted_VBD the_DT senses_NNS of_IN ∼_CD 70_CD %_NN of_IN them_PRP ._.
It_PRP had_VBD three_CD senses_NNS as_IN described_VBN in_IN Section_NN 1_CD :_: -LRB-_-LRB- 1_LS -RRB-_-RRB- the_DT ability_NN to_TO do_VB something_NN well_RB ,_, -LRB-_-LRB- 2_LS -RRB-_-RRB- its_PRP$ fea_NN -_: sibility_NN ,_, and_CC -LRB-_-LRB- 3_LS -RRB-_-RRB- the_DT certainty_NN of_IN something_NN hap_NN -_: penings_NNS ._.
First_RB ,_, SWSM_NNP could_MD correctly_RB predict_VB the_DT first_JJ sense_NN ._.
The_DT words_NNS that_WDT surrounded_VBD them_PRP were_VBD ,_, for_IN instance_NN ,_, ``_`` 両者_FW -LRB-_-LRB- both_DT sides_NNS -RRB-_-RRB- ''_'' and_CC ``_`` 人間_FW -LRB-_-LRB- hu_SYM -_: man_NN -RRB-_-RRB- ''_'' ,_, and_CC ``_`` 研究_FW -LRB-_-LRB- research_NN -RRB-_-RRB- ''_'' ,_, ``_`` コンヒ_FW ナート_FW -LRB-_-LRB- in_IN -_: dustrial_JJ complex_JJ -RRB-_-RRB- ''_'' ,_, and_CC ``_`` 今後_FW -LRB-_-LRB- hereafter_NN -RRB-_-RRB- ''_'' ._.
Sec_SYM -_: ond_NN ,_, SWSM_NNP could_MD correctly_RB predict_VB almost_RB none_NN of_IN the_DT words_NNS that_WDT had_VBD the_DT second_JJ sense_NN ._.
The_DT words_NNS surrounding_VBG an_DT example_NN were_VBD ``_`` 毎日_FW -LRB-_-LRB- every_DT day_NN -RRB-_-RRB- ''_'' ,_, ``_`` 違う_FW -LRB-_-LRB- various_JJ -RRB-_-RRB- ''_'' ,_, ``_`` 直面する_FW -LRB-_-LRB- to_TO face_VB -RRB-_-RRB- ''_'' ,_, and_CC ``_`` 人々_FW -LRB-_-LRB- people_NNS -RRB-_-RRB- ''_'' ,_, and_CC SWSM_NNP predicted_VBD the_DT sense_NN as_IN sense_NN -LRB-_-LRB- 1_LS -RRB-_-RRB- ._.
We_PRP think_VBP that_IN ``_`` 人々_FW -LRB-_-LRB- people_NNS -RRB-_-RRB- ''_'' misled_VBD the_DT an_DT -_: swer_NN ._.
The_DT words_NNS surrounding_VBG another_DT example_NN were_VBD ``_`` 破る_FW -LRB-_-LRB- break_NN through_IN -RRB-_-RRB- ''_'' ,_, ``_`` 音楽_FW -LRB-_-LRB- music_NN -RRB-_-RRB- ''_'' ,_, and_CC ``_`` 広け_FW る_FW -LRB-_-LRB- spread_NN -RRB-_-RRB- ''_'' ,_, and_CC SWSM_NNP predict_VBP the_DT sense_NN as_IN sense_NN -LRB-_-LRB- 1_LS -RRB-_-RRB- ._.
We_PRP think_VBP that_IN ``_`` 広け_FW る_FW -LRB-_-LRB- spread_NN -RRB-_-RRB- ''_'' could_MD be_VB a_DT clue_NN to_TO predict_VB the_DT sense_NN ,_, but_CC ``_`` 音楽_FW -LRB-_-LRB- music_NN -RRB-_-RRB- ''_'' misled_VBD the_DT answer_NN because_IN it_PRP appeared_VBD many_JJ times_NNS in_IN the_DT corpus_NN ._.
Finally_RB ,_, SWSM_NNP could_MD correctly_RB pre_VB -_: dicted_VBD the_DT last_JJ sense_NN ._.
The_DT words_NNS surrounded_VBD them_PRP were_VBD ,_, for_IN instance_NN ,_, -LRB-_-LRB- 1_LS -RRB-_-RRB- ``_`` 事態_FW -LRB-_-LRB- situation_NN -RRB-_-RRB- ''_'' ,_, ``_`` 生す_FW る_FW -LRB-_-LRB- arise_VB -RRB-_-RRB- ''_'' ,_, and_CC ``_`` 出る_FW -LRB-_-LRB- appear_VB -RRB-_-RRB- ''_'' ,_, -LRB-_-LRB- 2_LS -RRB-_-RRB- ``_`` 円高_FW -LRB-_-LRB- apprecia_NN -_: tion_NN -RRB-_-RRB- ''_'' ,_, ``_`` 進む_FW -LRB-_-LRB- escalate_VB -RRB-_-RRB- ''_'' ,_, and_CC ``_`` 出る_FW -LRB-_-LRB- appear_VB -RRB-_-RRB- ''_'' ,_, and_CC -LRB-_-LRB- 3_LS -RRB-_-RRB- ``_`` 読む_FW -LRB-_-LRB- read_VB -RRB-_-RRB- ''_'' and_CC ``_`` 否定する_FW -LRB-_-LRB- deny_VB -RRB-_-RRB- ''_'' ._.
``_`` 洗う_FW -LRB-_-LRB- wash_NN -RRB-_-RRB- ''_'' is_VBZ a_DT normal-class_JJ word_NN and_CC it_PRP ap_SYM -_: peared_VBN five_CD times_NNS in_IN the_DT corpus_NN ._.
SWSM_NNP correctly_RB predicted_VBD the_DT senses_NNS of_IN ∼_CD 80_CD %_NN ,_, viz._FW ,_, four_CD of_IN them_PRP ._.
It_PRP has_VBZ two_CD senses_NNS in_IN the_DT corpus_NN :_: -LRB-_-LRB- 1_LS -RRB-_-RRB- sanctify_VBP -LRB-_-LRB- some_DT -_: one_PRP 's_POS heart_NN -RRB-_-RRB- and_CC -LRB-_-LRB- 2_LS -RRB-_-RRB- wash_VBP out_RP a_DT stain_VB with_IN wa_SYM -_: ter_NN ._.
The_DT words_NNS surrounding_VBG the_DT example_NN that_WDT were_VBD incorrectly_RB predicted_VBN were_VBD ``_`` 今夜_FW -LRB-_-LRB- tonight_NN -RRB-_-RRB- ''_'' ,_, ``_`` 体_FW -LRB-_-LRB- body_NN -RRB-_-RRB- ''_'' ,_, and_CC ``_`` 否_FW -LRB-_-LRB- not_RB -RRB-_-RRB- ''_'' ,_, and_CC SWSM_NNP answered_VBD the_DT sense_NN as_IN -LRB-_-LRB- 1_LS -RRB-_-RRB- even_RB though_IN it_PRP was_VBD -LRB-_-LRB- 2_LS -RRB-_-RRB- ._.
The_DT words_NNS surrounding_VBG the_DT examples_NNS that_WDT were_VBD correctly_RB pre_SYM -_: dicted_VBN were_VBD -LRB-_-LRB- 1_LS -RRB-_-RRB- ``_`` 島民_FW -LRB-_-LRB- islander_NN -RRB-_-RRB- ''_'' ,_, ``_`` 涙_FW -LRB-_-LRB- tear_VB -RRB-_-RRB- ''_'' ,_, and_CC ``_`` 石_FW -LRB-_-LRB- stone_NN -RRB-_-RRB- ''_'' ,_, -LRB-_-LRB- 2_LS -RRB-_-RRB- ``_`` 見る_FW -LRB-_-LRB- look_NN at_IN -RRB-_-RRB- ''_'' and_CC ``_`` 心_FW -LRB-_-LRB- heart_NN -RRB-_-RRB- ''_'' ,_, -LRB-_-LRB- 3_LS -RRB-_-RRB- ``_`` 手足_FW -LRB-_-LRB- limb_NN -RRB-_-RRB- ''_'' ,_, ``_`` 顔_FW -LRB-_-LRB- face_NN -RRB-_-RRB- ''_'' ,_, ``_`` 私_FW -LRB-_-LRB- I_PRP -RRB-_-RRB- ''_'' ,_, and_CC ``_`` 風呂_FW -LRB-_-LRB- bath_NN -RRB-_-RRB- ''_'' ,_, -LRB-_-LRB- 4_LS -RRB-_-RRB- ``_`` 体_FW -LRB-_-LRB- body_NN -RRB-_-RRB- ''_'' ,_, ``_`` 水_FW -LRB-_-LRB- water_NN -RRB-_-RRB- ''_'' ,_, and_CC ``_`` 抜く_FW -LRB-_-LRB- drain_NN -RRB-_-RRB- ''_'' ._.
These_DT examples_NNS demonstrate_VBP that_IN the_DT surround_VBP -_: ing_VBG words_NNS were_VBD good_JJ clues_NNS to_TO disambiguate_VB the_DT word_NN senses_NNS ._.
10_CD Conclusion_NN We_PRP proposed_VBD the_DT surrounding_VBG word_NN sense_NN model_NN -LRB-_-LRB- SWSM_NNP -RRB-_-RRB- ,_, which_WDT used_VBD the_DT word_NN sense_NN distribution_NN around_IN ambiguous_JJ words_NNS ,_, and_CC performed_VBD unsuper_JJ -_: vised_VBD all-words_JJ word_NN sense_NN disambiguation_NN in_IN the_DT Japanese_JJ language_NN ._.
The_DT system_NN incorporated_VBD the_DT EDR_NNP concept_NN dictionary_NN and_CC we_PRP performed_VBD exper_SYM -_: iments_NNS using_VBG the_DT EDR_NNP Japanese_NNP corpus_NN ._.
We_PRP evalu_SYM -_: ated_VBD the_DT performance_NN of_IN the_DT model_NN using_VBG difficulty_NN classes_NNS based_VBN on_IN the_DT entropy_NN of_IN senses_NNS in_IN the_DT cor_NN -_: pus_NN :_: easy_JJ ,_, normal_JJ ,_, and_CC hard_RB ._.
We_PRP performed_VBD exper_SYM -_: iments_NNS with_IN SWSM_NNP in_IN nine_CD settings_NNS for_IN the_DT tran_NN -_: sition_NN probability_NN parameters_NNS ._.
The_DT experiments_NNS revealed_VBD that_IN SWSM_NNP outperformed_VBD the_DT random_JJ baseline_NN and_CC LDAWN_NNP ,_, which_WDT is_VBZ a_DT system_NN that_WDT uses_VBZ the_DT topic_NN model_NN ._.
The_DT SWSM_NNP model_NN clearly_RB out_IN -_: performed_VBD the_DT other_JJ systems_NNS for_IN senses_NNS in_IN the_DT nor_CC -_: mal_NN and_CC hard_JJ classes_NNS ._.
Some_DT examples_NNS that_WDT cor_SYM -_: rectly_NN predicted_VBD senses_NNS indicated_VBD that_IN the_DT surround_VBP -_: ing_VBG words_NNS were_VBD good_JJ clues_NNS to_TO disambiguate_VB word_NN senses_NNS even_RB if_IN we_PRP used_VBD unsupervised_JJ WSD_NNP ._.
Method_NN All_DT Easy_NNP Normal_NNP Hard_NNP Random_NNP 30.97_CD 33.01_CD 29.35_CD 13.47_CD LDAWN_NNP 36.12_CD 42.06_CD 30.66_CD 13.52_CD SWSM1_NNP 38.91_CD 46.87_CD 33.44_CD 19.92_CD SWSM2_NNP 39.60_CD 48.90_CD 32.85_CD 23.95_CD Method_NNP All_NNP Easy_NNP Normal_NNP Hard_NNP Random_NNP 36.63_CD 36.91_CD 32.09_CD 16.03_CD LDAWN_NNP 42.51_CD 44.65_CD 34.83_CD 17.80_CD SWSM1_NNP 42.58_CD 44.78_CD 36.38_CD 21.06_CD SWSM2_NNP 42.09_CD 43.68_CD 36.01_CD 20.44_CD References_NNP Timothy_NNP Baldwin_NNP ,_, Su_NNP Nam_NNP Kim_NNP ,_, Francis_NNP Bond_NNP ,_, Sanae_NNP Fujita_NNP ,_, David_NNP Martinez_NNP ,_, and_CC Takaaki_NNP Tanaka_NNP ._.
2008_CD ._.
Mrd-based_JJ word_NN sense_NN disambiguation_NN :_: Further_JJ ex_FW -_: tending_VBG lesk_NN ._.
In_IN Proceedings_NNP of_IN the_DT 2008_CD Interna_NNP -_: tional_JJ Joint_NNP Conference_NN on_IN Natural_JJ Language_NN Pro-_JJ cessing_NN ,_, pages_NNS 775_CD --_: 780_CD ._.
David_NNP Blei_NNP ,_, Andrew_NNP Ng_NNP ,_, and_CC Michael_NNP Jordan_NNP ._.
2003_CD ._.
Latent_NN dirichlet_NN allocation_NN ._.
Journal_NNP of_IN Machine_NNP Learning_NNP Research_NNP ,_, 1_CD -LRB-_-LRB- 3_LS -RRB-_-RRB- :993_CD --_: 1022_CD ._.
Jordan_NNP Boyd-Graber_NNP ,_, David_NNP M._NNP Blei_NNP ,_, and_CC Xiaojin_NNP Zhu_NNP ._.
2007_CD ._.
A_DT topic_NN model_NN for_IN word_NN sense_NN disambiguation_NN ._.
In_IN Proceedings_NNP of_IN the_DT 2007_CD Joint_NNP Conference_NNP on_IN Empirical_NNP Methods_NNPS in_IN Natural_NNP Language_NNP Process_NNP -_: ing_NN and_CC Computational_NNP Natural_NNP Language_NNP Learning_NNP ,_, pages_NNS 1024_CD --_: 1033_CD ._.
W._NNP Gale_NNP and_CC G._NNP Sampson_NNP ._.
1995_CD ._.
Good-turing_JJ smooth_JJ -_: ing_NN without_IN tears_NNS ._.
Journal_NNP of_IN Quantitative_NNP Linguis_NNP -_: tics_NNS ,_, 2_CD -LRB-_-LRB- 3_LS -RRB-_-RRB- :217_CD --_: 237_CD ._.
Weiwei_NNP Guo_NNP and_CC Mona_NNP Diab_NNP ._.
2011_CD ._.
Semantic_JJ topic_NN models_NNS :_: Combining_VBG word_NN distributional_JJ statistics_NNS and_CC dictionary_JJ definitions_NNS ._.
In_IN Proceedings_NNP of_IN the_DT 2011_CD Conference_NN on_IN Empirical_JJ Methods_NNS in_IN Natural_JJ Language_NN Processing_NNP ,_, pages_NNS 552_CD --_: 561_CD ._.
Hideki_NNP Hirakawa_NNP and_CC Kazuhiro_NNP Kimura_NNP ._.
2003_CD ._.
Con_NN -_: cept_JJ abstraction_NN methods_NNS using_VBG concept_NN classifica_NN -_: tion_NN and_CC their_PRP$ evaluation_NN on_IN word_NN sense_NN disam_NN -_: biguation_NN task_NN ._.
IPSJ_NNP Journal_NNP ,_, 2_CD -LRB-_-LRB- 44_CD -RRB-_-RRB- :421_CD --_: 432_CD ,_, -LRB-_-LRB- In_IN Japanese_JJ -RRB-_-RRB- ._.
Jay_NNP J._NNP Jiang_NNP and_CC David_NNP W._NNP Conrath_NNP ._.
1997_CD ._.
Semantic_JJ similarity_NN based_VBN on_IN corpus_NN statistics_NNS and_CC lexical_JJ tax_NN -_: onomy_NN ._.
In_IN Proceedings_NNP of_IN International_NNP Conference_NNP Research_NNP on_IN Computational_NNP Linguistics_NNP ,_, pages_NNS 19_CD --_: 33_CD ._.
Jun_NNP S_NNP Liu_NNP ._.
1994_CD ._.
The_DT collapsed_JJ gibbs_NNS sampler_NN in_IN bayesian_JJ computations_NNS with_IN applications_NNS to_TO a_DT gene_NN regulation_NN problem_NN ._.
Journal_NNP of_IN the_DT American_JJ Statis_NNP -_: tical_JJ Association_NNP ,_, 427_CD -LRB-_-LRB- 40_CD -RRB-_-RRB- :958_CD --_: 966_CD ._.
Diana_NNP McCarthy_NNP ._.
1997_CD ._.
Estimation_NN of_IN a_DT probability_NN distribution_NN over_IN a_DT hierarchical_JJ classification_NN ._.
In_IN The_DT Tenth_NNP White_NNP House_NNP Papers_NNP COGS_NNP -_: CSRP_NNP ,_, pages_NNS 1_CD --_: 9_CD ._.
Rada_NNP Mihalcea_NNP ._.
2005_CD ._.
Unsupervised_JJ large-vocabulary_JJ word_NN sense_NN disambiguation_NN with_IN graph-based_JJ algo_NN -_: rithms_NNS for_IN sequence_NN data_NNS labeling_VBG ._.
In_IN Proceedings_NNP of_IN the_DT 2005_CD Conference_NN on_IN Empirical_JJ Methods_NNS in_IN Nat_NNP -_: ural_JJ Language_NN Processing_NNP ,_, pages_NNS 411_CD --_: 418_CD ._.
Hideo_NNP Miyoshi_NNP ,_, Kenji_NNP Sugiyama_NNP ,_, Masahiro_NNP Kobayashi_NNP ,_, and_CC Takano_NNP Ogino_NNP ._.
1996_CD ._.
An_DT overview_NN of_IN the_DT edr_NN electronic_JJ dictionary_NN and_CC the_DT current_JJ status_NN of_IN its_PRP$ uti_NN -_: lization_NN ._.
In_IN Proceedings_NNP of_IN the_DT COLING_NNP 1996_CD Vol_NNP -_: ume_NN 2_CD :_: The_DT 16th_JJ International_NNP Conference_NNP on_IN Com_NNP -_: putational_JJ Linguistics_NNP ,_, pages_NNS 1090_CD --_: 1093_CD ._.
Ted_NNP Pedersen_NNP ,_, Satanjeev_NNP Banerjee_NNP ,_, and_CC Siddharth_NNP Pat_NNP -_: wardhan_NN ._.
2005_CD ._.
Maximizing_VBG semantic_JJ relatedness_NN to_TO perform_VB word_NN sense_NN disambiguation_NN ._.
In_IN Research_NNP Report_NNP UMSI_NNP ._.
Philip_NNP Resnik_NNP ._.
1995_CD ._.
Using_VBG information_NN content_NN to_TO evaluate_VB semantic_JJ similarity_NN in_IN a_DT taxonomy_NN ._.
In_IN Inter_NNP -_: national_JJ Joint_NNP Conferences_NNPS on_IN Artificial_NNP Intelligence_NNP ,_, pages_NNS 448_CD --_: 453_CD ._.
Francesc_NNP Ribas_NNP ._.
1995_CD ._.
On_IN learning_VBG more_RBR appropriate_JJ selectional_JJ restrictions_NNS ._.
In_IN Proceedings_NNP of_IN the_DT Sev_NNP -_: enth_NN Conference_NN of_IN the_DT European_JJ Chapter_NN of_IN the_DT As_IN -_: sociation_NN for_IN Computational_NNP Linguistics_NNP ,_, pages_NNS 112_CD --_: 118_CD ._.
Yuto_NNP Sasaki_NNP ,_, Kanako_NNP Komiya_NNP ,_, and_CC Yoshiyuki_NNP Kotani_NNP ._.
2014_CD ._.
Word_NN sense_NN disambiguation_NN using_VBG topic_NN model_NN and_CC thesaurus_NN ._.
In_IN Proceedings_NNP of_IN the_DT fifth_JJ corpus_NN Japanese_JJ workshop_NN ,_, pages_NNS 71_CD --_: 80_CD -LRB-_-LRB- In_IN Japanese_JJ -RRB-_-RRB- ._.
Kiyoaki_NNP Shirai_NNP ._.
2002_CD ._.
Construction_NN of_IN a_DT word_NN sense_NN tagged_VBD corpus_NN for_IN senseval-2_JJ japanese_JJ dictionary_NN task_NN ._.
In_IN Proceedings_NNP of_IN the_DT third_JJ International_NNP Conference_NNP on_IN Language_NNP Resources_NNPS and_CC Evaluation_NNP ,_, pages_NNS 605_CD --_: 608_CD ._.
