Computing_NNP Semantic_NNP Text_NNP Similarity_NNP Using_VBG Rich_NNP Features_NNP Abstract_NNP Semantic_NNP text_NN similarity_NN -LRB-_-LRB- STS_NNP -RRB-_-RRB- is_VBZ an_DT essential_JJ problem_NN in_IN many_JJ Natural_JJ Language_NN Pro-_JJ cessing_NN tasks_NNS ,_, which_WDT has_VBZ drawn_VBN a_DT considerable_JJ amount_NN of_IN attention_NN by_IN research_NN community_NN in_IN recent_JJ years_NNS ._.
In_IN this_DT paper_NN ,_, our_PRP$ work_NN focused_VBD on_IN computing_NN semantic_JJ similarity_NN between_IN texts_NNS of_IN sentence_NN length_NN ._.
We_PRP employed_VBD a_DT Sup_NNP -_: port_NN Vector_NNP Regression_NNP model_NN with_IN rich_JJ effec_NN -_: tive_JJ features_NNS to_TO predict_VB the_DT similarity_NN scores_NNS between_IN short_JJ English_JJ sentence_NN pairs_NNS ._.
Our_PRP$ model_NN used_VBD WordNet-Based_JJ features_NNS ,_, Corpus_NNP -_: Based_VBD features_NNS ,_, Word2Vec-based_JJ features_NNS ,_, Alignment-Based_JJ feature_NN and_CC Literal-Based_JJ features_NNS to_TO cover_VB various_JJ aspects_NNS of_IN sentences_NNS ._.
And_CC the_DT experiment_NN conducted_VBN on_IN SemEval_NNP 2015_CD task_NN 2a_NN shows_VBZ that_IN our_PRP$ method_NN achieved_VBD a_DT Pearson_NNP correlation_NN :_: 80.486_CD %_NN which_WDT outper_SYM -_: formed_VBD the_DT wining_NN system_NN -LRB-_-LRB- 80.15_CD %_NN -RRB-_-RRB- by_IN a_DT small_JJ margin_NN ,_, the_DT results_NNS indicated_VBD a_DT high_JJ cor_NN -_: relation_NN with_IN human_JJ judgments_NNS ._.
Specially_RB ,_, among_IN the_DT five_CD test_NN sets_NNS which_WDT come_VBP from_IN dif_SYM -_: ferent_JJ domains_NNS used_VBN in_IN the_DT estimation_NN ,_, our_PRP$ method_NN got_VBD better_JJR results_NNS than_IN the_DT top_JJ team_NN on_IN two_CD of_IN them_PRP whose_WP$ domain-related_JJ data_NNS is_VBZ available_JJ for_IN training_NN ,_, while_IN comparable_JJ re_SYM -_: sults_NNS were_VBD achieved_VBN on_IN the_DT rest_NN three_CD unseen_JJ test_NN sets_NNS ._.
The_DT experiments_NNS results_NNS indicated_VBD that_IN our_PRP$ solution_NN is_VBZ more_RBR competitive_JJ when_WRB the_DT domain-specific_JJ training_NN data_NNS is_VBZ available_JJ and_CC our_PRP$ method_NN still_RB keeps_VBZ good_JJ generalization_NN ability_NN on_IN novel_NN data_NNS ._.
1_CD Introduction_NNP Semantic_NNP text_NN similarity_NN is_VBZ a_DT fundamental_JJ challenge_NN in_IN many_JJ Natural_JJ Language_NN Processing_NNP tasks_NNS ,_, such_JJ as_IN Machine_NN Reading_VBG ,_, Deep_NNP Question_NNP Answering_NNP -LRB-_-LRB- Narayanan_NNP &_CC Harabagiu_NNP ,_, 2004_CD -RRB-_-RRB- ,_, Automatic_NNP Ma_NNP -_: chine_NN Translation_NN Evaluation_NN -LRB-_-LRB- Papineni_NNP ,_, Roukos_NNP et_FW al._FW ,_, 2002_CD -RRB-_-RRB- ,_, Automatic_NNP Text_NNP Summarization_NNP -LRB-_-LRB- Fattah_NNP &_CC Ren_NNP ,_, 2008_CD -RRB-_-RRB- and_CC Query_NNP Reformulation_NNP -LRB-_-LRB- Metzler_NNP ,_, Dumais_NNP et_FW al._FW ,_, 2007_CD -RRB-_-RRB- ,_, etc._FW ._.
Previous_JJ researches_VBZ on_IN semantic_JJ text_NN similarity_NN have_VBP been_VBN focused_VBN on_IN doc_NN -_: uments_NNS and_CC paragraphs_NNS ,_, while_IN comparison_NN objects_NNS in_IN many_JJ NLP_NNP tasks_NNS are_VBP texts_NNS of_IN sentence_NN length_NN ,_, such_JJ as_IN Video_NNP descriptions_NNS ,_, News_NNP headlines_NNS and_CC beliefs_NNS ,_, etc._FW ._.
In_IN this_DT paper_NN ,_, we_PRP study_VBP semantic_JJ simi_NNS -_: larity_NN between_IN sentences_NNS ._.
Given_VBN two_CD input_NN text_NN segments_NNS ,_, we_PRP need_VBP to_TO automatically_RB determine_VB a_DT score_NN that_WDT indicates_VBZ their_PRP$ semantic_JJ similarity_NN ._.
The_DT difficulties_NNS of_IN this_DT task_NN lie_NN on_IN several_JJ aspects_NNS ._.
First_RB ,_, there_EX were_VBD no_DT existing_VBG effective_JJ measures_NNS to_TO repre_VB -_: sent_VBN sentences_NNS which_WDT could_MD be_VB understood_VBN by_IN com_NN -_: puters_NNS without_IN losing_VBG any_DT information_NN ._.
Second_NNP ,_, even_RB with_IN good_JJ representations_NNS ,_, it_PRP 's_VBZ very_RB hard_JJ to_TO find_VB a_DT metric_JJ which_WDT can_MD fully_RB compare_VB the_DT equiva_NN -_: lence_NN between_IN two_CD sentence_NN representations_NNS ._.
Third_NNP ,_, similarity_NN itself_PRP is_VBZ a_DT very_RB complex_JJ concept_NN ,_, and_CC se_FW -_: mantic_JJ space_NN is_VBZ also_RB hard_JJ to_TO define_VB and_CC quantize_VB ._.
Given_VBN the_DT same_JJ pair_NN of_IN sentences_NNS ,_, different_JJ people_NNS may_MD mark_VB different_JJ similarity_NN scores_NNS ;_: this_DT incon_NN -_: sistency_NN is_VBZ derived_VBN from_IN people_NNS 's_POS judgments_NNS of_IN dif_NN -_: ference_NN ._.
Although_IN with_IN these_DT difficulties_NNS ahead_RB ,_, a_DT lot_NN of_IN methods_NNS have_VBP been_VBN proposed_VBN to_TO handle_VB this_DT problem_NN in_IN recent_JJ years_NNS ._.
And_CC our_PRP$ efforts_NNS mainly_RB focused_VBD on_IN trying_VBG to_TO combine_VB different_JJ existing_VBG approaches_NNS to_TO represent_VB a_DT sentence_NN ,_, and_CC hope_VBP to_TO cover_VB as_RB many_JJ aspects_NNS of_IN sentence_NN as_IN possible_JJ on_IN semantic_JJ level_NN ._.
In_IN this_DT paper_NN ,_, we_PRP exploited_VBD WordNet-Based_NNP ,_, Corpus-Based_NNP ,_, Word2Vec-based_JJ ,_, Alignment_NNP -_: Based_VBN and_CC Literal-Based_JJ features_NNS to_TO measure_VB se_FW -_: mantic_JJ equivalence_NN between_IN short_JJ English_JJ sentenc_NN -_: es_NNS ._.
We_PRP used_VBD a_DT SVR_NNP model_NN to_TO combine_VB all_DT of_IN these_DT similarities_NNS and_CC predict_VB a_DT final_JJ score_NN between_IN 0_CD ~_NN 5_CD to_TO denote_VB the_DT magnitude_NN of_IN semantic_JJ similarity_NN ._.
And_CC the_DT experiment_NN conducted_VBN on_IN SemEval_NNP 2015_CD task_NN 2a_NN shows_VBZ that_IN our_PRP$ method_NN achieved_VBD a_DT Pearson_NNP correlation_NN :_: 80.486_CD %_NN which_WDT outperformed_VBD the_DT win_VB -_: ing_NN system_NN -LRB-_-LRB- 80.15_CD %_NN -RRB-_-RRB- by_IN a_DT small_JJ margin_NN ._.
Experi_SYM -_: mental_JJ results_NNS demonstrate_VBP the_DT effectiveness_NN of_IN our_PRP$ approach_NN ._.
Feature_NNP Category_NNP Feature_NNP Name_NN WordNet-Based_JJ Path_similarity_NN ,_, Res_similarity_NNP ,_, Lin_similarity_NNP ,_, Wup_similarity_NNP Corpus-Based_NNP LSA_similarity_NNP ,_, IDF_LSA_similarity_NNP ,_, Freq_LSA_similarity_NNP ,_, Text_LSA_similarity_NNP ,_, LDA_similarity_NNP ,_, RIC_Difference_NNP Word2Vec-Based_VBD W2V_similarity_NNP ,_, IDF_W2V_similarity_NNP ,_, S2V_similarity_NNP ,_, Text_W2V_similarity_NNP Alignment-Based_NNP Alignment_similarity_NNP Literal-Based_NNP EditDistance_similarity_NNP ,_, ShallowSyntatic_similarity_NNP ,_, DifferLen_Rate_NNP ,_, Dig_NNP -_: it_similarity_NN ,_, Digit_in_Fea_NNP ,_, No_overlap_Fea_NNP ,_, Neg_Sentiment_Fea_NNP 2_CD Related_NNP Work_NNP Previous_JJ efforts_NNS have_VBP focused_VBN on_IN computing_NN se_FW -_: mantic_JJ similarity_NN between_IN documents_NNS ,_, concepts_NNS or_CC phrases_NNS ._.
Recent_JJ natural_JJ language_NN processing_NN appli_NNS -_: cations_NNS show_VBP a_DT stronger_JJR demand_NN of_IN finding_VBG effec_SYM -_: tive_JJ methods_NNS to_TO measure_VB semantic_JJ similarity_NN between_IN texts_NNS of_IN variable_JJ length_NN ,_, and_CC extensive_JJ method_NN have_VBP been_VBN proposed_VBN in_IN these_DT years_NNS ._.
Related_VBN work_NN could_MD roughly_RB be_VB divided_VBN into_IN five_CD major_JJ cat_NN -_: egories_NNS :_: Word_NN co-occurrence_NN methods_NNS ,_, Corpus_NNP -_: based_VBN and_CC Knowledge-based_JJ methods_NNS ,_, String_NNP simi_SYM -_: larity_NN methods_NNS ,_, Descriptive_JJ feature-based_JJ methods_NNS and_CC Alignment-based_JJ methods_NNS ._.
Word_NN co-occurrence_NN methods_NNS are_VBP usually_RB used_VBN in_IN Information_NNP Retrieval_NNP -LRB-_-LRB- IR_NNP -RRB-_-RRB- systems_NNS -LRB-_-LRB- Manning_NNP ,_, Raghavan_NNP et_FW al._FW ,_, 2008_CD -RRB-_-RRB- ._.
This_DT method_NN is_VBZ based_VBN on_IN the_DT hypothesis_NN that_WDT more_JJR similar_JJ documents_NNS would_MD have_VB more_RBR words_NNS in_IN common_JJ ._.
This_DT method_NN has_VBZ some_DT drawbacks_NNS when_WRB used_VBN in_IN sentence_NN ._.
As_IN sen_NN -_: tences_NNS are_VBP relatively_RB short_JJ compared_VBN to_TO documents_NNS ,_, they_PRP would_MD share_VB fewer_JJR words_NNS in_IN common_JJ ;_: moreo_JJR -_: ver_NN ,_, IR_NNP systems_NNS often_RB exclude_VBP function_NN words_NNS in_IN their_PRP$ method_NN while_IN these_DT words_NNS carry_VBP structural_JJ information_NN in_IN sentences_NNS -LRB-_-LRB- Li_NNP ,_, McLean_NNP et_FW al._FW ,_, 2006_CD -RRB-_-RRB- ,_, which_WDT eventually_RB may_MD lead_VB to_TO unsatisfactory_JJ results_NNS ._.
Many_JJ methods_NNS combined_VBN both_DT corpus-based_JJ and_CC knowledge-based_JJ measures_NNS to_TO reach_VB a_DT better_JJR result_NN ._.
Two_CD well-known_JJ corpus-based_JJ methods_NNS are_VBP Latent_NNP Semantic_NNP Analysis_NNP -LRB-_-LRB- LSA_NNP -RRB-_-RRB- -LRB-_-LRB- Dumais_NNP ,_, 2004_CD -RRB-_-RRB- and_CC Hyperspace_NNP analogues_NNS to_TO Language_NN -LRB-_-LRB- HAL_NNP -RRB-_-RRB- -LRB-_-LRB- Burgess_NNP ,_, Livesay_NNP et_FW al._FW ,_, 1998_CD -RRB-_-RRB- ._.
Another_DT effective_JJ corpus-based_JJ measure_NN is_VBZ Explicit_NNP Semantic_NNP Analy_NNP -_: sis_NN -LRB-_-LRB- ESA_NNP -RRB-_-RRB- -LRB-_-LRB- Gabrilovich_NNP &_CC Markovitch_NNP ,_, 2007_CD -RRB-_-RRB- ._.
ESA_NNP is_VBZ a_DT method_NN that_WDT represents_VBZ the_DT meaning_NN of_IN texts_NNS in_IN a_DT high-dimension_JJ space_NN of_IN concepts_NNS derived_VBN from_IN Wikipedia_NNP ._.
As_IN this_DT methodology_NN explicitly_RB uses_VBZ the_DT knowledge_NN collected_VBN and_CC organized_VBN by_IN humans_NNS ,_, common-sense_JJ and_CC domain-specific_JJ world_NN knowledge_NN are_VBP considered_VBN in_IN it_PRP which_WDT leads_VBZ to_TO sub_SYM -_: stantial_JJ improvements_NNS in_IN measure_NN semantic_JJ simi_NNS -_: larity_NN between_IN sentences_NNS ,_, and_CC it_PRP is_VBZ also_RB easy_JJ to_TO interpret_VB by_IN human_JJ ._.
Knowledge-based_JJ methods_NNS are_VBP often_RB based_VBN on_IN semantic_JJ networks_NNS such_JJ as_IN Word_NN -_: Net_NN ._.
Some_DT well-known_JJ knowledge-based_JJ measures_NNS include_VBP :_: S&P_NNP 's_POS Measure_NN ,_, Wu_NNP &_CC Palmer_NNP Measure_NN ,_, Leakcock_NNP &_CC Chodorow_NNP 's_POS Measure_NN ,_, Renik_NNP 's_POS Measure_NN ,_, Lin_NNP 's_POS Measure_NN and_CC Jiang_NNP 's_POS Measure_NN ._.
As_IN to_TO String-based_JJ similarity_NN ,_, Islam_NNP et_FW al._FW pro-_JJ posed_VBD a_DT normalized_JJ and_CC modified_VBD version_NN of_IN the_DT Longest_NNP Common_NNP Subsequence_NNP -LRB-_-LRB- LCS_NNP -RRB-_-RRB- string_NN matching_VBG algorithm_NN to_TO measure_VB text_NN similarity_NN -LRB-_-LRB- Islam_NNP &_CC Inkpen_NNP ,_, 2008_CD -RRB-_-RRB- ._.
Combined_VBN with_IN a_DT corpus_NN -_: based_VBN measure_NN ,_, their_PRP$ methods_NNS achieved_VBD a_DT very_RB competing_VBG result_NN ._.
Descriptive_JJ feature-based_JJ methods_NNS uses_VBZ prede_SYM -_: fined_VBN features_NNS to_TO capture_VB information_NN contained_VBN in_IN the_DT sentence_NN ._.
Then_RB feed_VB these_DT features_NNS into_IN the_DT classifier_NN ,_, this_DT supervised_JJ method_NN achieved_VBD best_JJS in_IN SemEval_NNP 2012_CD -LRB-_-LRB- Šarić_NNP ,_, Glavaš_NNP et_FW al._FW ,_, 2012_CD -RRB-_-RRB- ._.
For_IN alignment-based_JJ methods_NNS ,_, Sultan_NNP et_FW al._FW -LRB-_-LRB- Sultan_NNP ,_, Bethard_NNP et_FW al._FW ,_, 2014a_JJ -RRB-_-RRB- proposed_VBD an_DT effec_NN -_: tive_JJ solution_NN to_TO align_VB words_NNS in_IN monolingual_JJ sen_NN -_: tences_NNS which_WDT achieved_VBD state-of-the-art_JJ performance_NN while_IN relying_VBG on_IN almost_RB no_DT supervision_NN and_CC a_DT very_RB small_JJ number_NN of_IN external_JJ ._.
Based_VBN on_IN the_DT output_NN of_IN word_NN aligner_NN ,_, they_PRP taking_VBG the_DT proportion_NN of_IN their_PRP$ aligned_VBN content_JJ words_NNS as_IN the_DT semantic_JJ degree_NN of_IN the_DT two_CD sentences_NNS ._.
This_DT simple_JJ unsupervised_JJ meth_NN -_: od_NN leads_VBZ to_TO state-of-art_JJ results_NNS for_IN sentence_NN level_NN semantic_JJ similarity_NN in_IN SemEval_NNP 2014_CD STS_NNP task_NN ._.
Specially_RB ,_, SemEval_NNP has_VBZ hold_VB STS_NNP for_IN four_CD years_NNS in_IN a_DT row_NN ,_, and_CC many_JJ wining_NN methods_NNS have_VBP been_VBN published_VBN -LRB-_-LRB- Bär_NNP ,_, Biemann_NNP et_FW al._FW ,_, 2012_CD ;_: Han_NNP ,_, Kashyap_NNP et_FW al._FW ,_, 2013_CD ;_: Sultan_NNP ,_, Bethard_NNP et_FW al._FW ,_, 2014b_JJ -RRB-_-RRB- ._.
Table_NNP 1_CD Feature_NNP sets_NNS of_IN our_PRP$ system_NN configuration_NN 3_CD Feature_NNP Generation_NNP The_NNP core_NN idea_NN of_IN our_PRP$ method_NN is_VBZ to_TO use_VB the_DT combina_NN -_: tion_NN of_IN word_NN similarities_NNS to_TO estimate_VB sentence_NN simi_SYM -_: larity_NN ,_, as_IN lots_NNS of_IN effective_JJ methods_NNS have_VBP been_VBN proposed_VBN to_TO measure_VB word-to-word_JJ similarity_NN in_IN recent_JJ years_NNS ._.
Our_PRP$ features_NNS could_MD roughly_RB be_VB divided_VBN into_IN five_CD categories_NNS :_: WordNet-Based_JJ features_NNS ,_, Cor_NNP -_: pus-Based_JJ features_NNS ,_, Word2Vec-based_JJ features_NNS ,_, Alignment-Based_JJ feature_NN and_CC Literal-Based_JJ fea_NN -_: tures_NNS ._.
Generally_RB ,_, Word2Vec-Based_JJ methods_NNS also_RB can_MD be_VB regarded_VBN as_IN Corpus-Based_JJ methods_NNS ,_, to_TO ex_FW -_: plore_VB the_DT effectiveness_NN of_IN deep_JJ learning_NN based_VBN methods_NNS ,_, in_IN our_PRP$ paper_NN ,_, we_PRP separately_RB classified_VBD Word2Vec-Based_JJ features_NNS into_IN a_DT category_NN ._.
Fea_SYM -_: tures_NNS used_VBN in_IN our_PRP$ model_NN are_VBP shown_VBN in_IN Table_NNP 1_CD ._.
After_IN combination_NN of_IN these_DT features_NNS ,_, we_PRP got_VBD a_DT very_RB competitive_JJ result_NN ,_, which_WDT indicated_VBD that_IN dif_SYM -_: ferent_NN features_NNS capture_VBP different_JJ aspects_NNS of_IN seman_NN -_: tics_NNS in_IN sentences_NNS ._.
We_PRP will_MD look_VB into_IN these_DT features_NNS in_IN detail_NN in_IN the_DT following_JJ sections_NNS ._.
3.1_CD WordNet-Based_JJ Features_NNS WordNet_NNP -LRB-_-LRB- Miller_NNP ,_, 1995_CD -RRB-_-RRB- is_VBZ a_DT widely_RB used_VBN semantic_JJ net_NN of_IN English_NNP ,_, and_CC it_PRP is_VBZ an_DT effective_JJ tool_NN to_TO find_VB synonyms_NNS of_IN nouns_NNS ,_, verbs_NNS ,_, adjectives_NNS and_CC adverbs_NNS ._.
WordNet_NNP is_VBZ particularly_RB well_RB suited_VBN for_IN similarity_NN and_CC relatedness_NN measures_NNS ,_, since_IN it_PRP organizes_VBZ nouns_NNS and_CC verbs_NNS into_IN hierarchies_NNS of_IN is-a_JJ relations_NNS -LRB-_-LRB- Pedersen_NNP ,_, Patwardhan_NNP et_FW al._FW ,_, 2004_CD -RRB-_-RRB- ._.
In_IN this_DT paper_NN ,_, these_DT similarity_NN measures_NNS were_VBD tried_VBN in_IN our_PRP$ experi_NNS -_: ments_NNS ._.
After_IN selection_NN ,_, four_CD of_IN them_PRP were_VBD kept_VBN in_IN our_PRP$ final_JJ model_NN :_: Path_similarity_NNP ,_, Res_similarity_NNP ,_, Lin_similarity_NNP ,_, and_CC Wup_similarity_NNP ._.
We_PRP provided_VBD below_IN a_DT short_JJ description_NN for_IN each_DT of_IN these_DT metrics_NNS first_RB ,_, and_CC then_RB explain_VB how_WRB these_DT measures_NNS were_VBD used_VBN in_IN our_PRP$ evaluation_NN of_IN sentence_NN semantic_JJ simi_NNS -_: larity_NN ._.
The_DT main_JJ idea_NN of_IN the_DT Path_similarity_NNP measure_NN -LRB-_-LRB- The_DT Shortest_JJS Path_NN based_VBN Measure_NN -RRB-_-RRB- is_VBZ that_IN the_DT simi_NN -_: larity_NN between_IN two_CD concepts_NNS can_MD be_VB derived_VBN from_IN the_DT length_NN of_IN the_DT path_NN linking_VBG the_DT concepts_NNS and_CC the_DT shortest_JJS path_NN from_IN synset_NN 1_CD to_TO synset_VB 2_CD in_IN Word_NN -_: Net_NN ._.
-LRB-_-LRB- Meng_NNP ,_, Huang_NNP et_FW al._FW ,_, 2013_CD -RRB-_-RRB- ._.
Formally_RB ,_, the_DT the_DT taxonomy_NN and_CC -LRB-_-LRB- ,_, -RRB-_-RRB- is_VBZ the_DT length_NN of_IN the_DT 12_CD where_WRB the_DT deep_max_NN is_VBZ the_DT maximum_NN depth_NN of_IN 12_CD Res_similarity_NNP -LRB-_-LRB- Resnik_NNP 's_POS Measure_NN -RRB-_-RRB- is_VBZ a_DT similari_NN -_: ty_NN measure_NN based_VBN on_IN information_NN content_NN ._.
It_PRP as_IN -_: sumes_NNS that_WDT similarity_NN is_VBZ dependent_JJ on_IN the_DT corpus_NN that_WDT generates_VBZ the_DT information_NN content_NN ._.
-LRB-_-LRB- 1_CD ,_, 2_CD -RRB-_-RRB- =_SYM −_FW -LRB-_-LRB- 1_CD ,_, 2_CD -RRB-_-RRB- =_SYM -LRB-_-LRB- -LRB-_-LRB- 1_CD ,_, 2_CD -RRB-_-RRB- -RRB-_-RRB- -LRB-_-LRB- 2_LS -RRB-_-RRB- where_WRB -LRB-_-LRB- ,_, -RRB-_-RRB- is_VBZ the_DT lowest_JJS common_JJ subsume_NN 12_CD of_IN 1_CD and_CC 2_CD ._.
Lin_similarity_NNP -LRB-_-LRB- Lin_NNP 's_POS Measure_NN -RRB-_-RRB- -LRB-_-LRB- Lin_NNP ,_, 1998_CD -RRB-_-RRB- is_VBZ a_DT similarity_NN measure_NN based_VBN on_IN the_DT Resnik_NNP measure_NN ,_, which_WDT adds_VBZ a_DT normalization_NN factor_NN consisting_VBG of_IN the_DT information_NN content_NN of_IN the_DT two_CD input_NN concepts_NNS :_: -LRB-_-LRB- ,_, -RRB-_-RRB- =_SYM 2_CD ∗_CD -LRB-_-LRB- -RRB-_-RRB- -LRB-_-LRB- 3_LS -RRB-_-RRB- 1_CD 2_CD -LRB-_-LRB- 1_CD -RRB-_-RRB- +_NN -LRB-_-LRB- 2_LS -RRB-_-RRB- Wup_similarity_NNP -LRB-_-LRB- Wu_NNP &_CC Palmer_NNP 's_POS Measure_NN -RRB-_-RRB- -LRB-_-LRB- Wu_NNP &_CC Palmer_NNP ,_, 1994_CD -RRB-_-RRB- measure_NN is_VBZ based_VBN on_IN the_DT depth_NN of_IN two_CD given_VBN concepts_NNS in_IN the_DT WordNet_NNP taxonomy_NN and_CC that_IN of_IN their_PRP$ Least_NNP Common_NNP Subsumer_NNP -LRB-_-LRB- LCS_NNP -RRB-_-RRB- ,_, the_DT similarity_NN score_NN of_IN two_CD concepts_NNS is_VBZ defined_VBN as_IN fol_NN -_: lowing_VBG formula_NN -LRB-_-LRB- Resnik_NNP ,_, 1999_CD -RRB-_-RRB- :_: -LRB-_-LRB- ,_, -RRB-_-RRB- =_SYM 2_CD ∗_CD h_NN -LRB-_-LRB- -RRB-_-RRB- -LRB-_-LRB- 4_LS -RRB-_-RRB- 1_CD 2_CD h_NN -LRB-_-LRB- 1_CD -RRB-_-RRB- +_NN h_NN -LRB-_-LRB- 2_LS -RRB-_-RRB- position_NN of_IN the_DT concepts_NNS in_IN the_DT WordNet_NNP taxonomy_NN In_IN our_PRP$ experiment_NN ,_, we_PRP used_VBD the_DT NLTK1_NNP toolkit_NN -LRB-_-LRB- Bird_NNP ,_, 2006_CD -RRB-_-RRB- WordNet_NNP APIs_NNS to_TO calculate_VB WordNet_NNP -_: based_VBN similarities_NNS ._.
Based_VBN on_IN WordNet_NNP and_CC Brown_NNP corpus_VBZ -LRB-_-LRB- to_TO obtain_VB IC_NNP through_IN statistical_JJ analysis_NN of_IN Brown_NNP corpus_NN -RRB-_-RRB- ,_, we_PRP generated_VBD the_DT four_CD WordNet_NNP -_: based_VBN features_NNS following_VBG the_DT same_JJ steps_NNS proposed_VBN in_IN -LRB-_-LRB- Liu_NNP ,_, Sun_NNP et_FW al._FW -RRB-_-RRB- ._.
Issues_NNS that_WDT required_VBD attention_NN is_VBZ that_IN the_DT results_NNS of_IN Res_similarity_NNP measure_NN needs_VBZ to_TO process_VB nor_CC -_: malization_NN to_TO make_VB sure_JJ the_DT value_NN lies_VBZ in_IN the_DT inter_NN -_: val_NN -LSB-_NNP 0.0_CD ,_, 1.0_CD -RSB-_NN ._.
Figure_NN 1_CD A_DT simple_JJ example_NN of_IN word_NN alignment_NN using_VBG knowledge-based_JJ similarity_NN measures_NNS 1_CD Path_similarity_NN between_IN concepts_NNS and_CC is_VBZ de_IN -_: fined_VBN as_IN following_VBG formula_NN :_: h_NN -LRB-_-LRB- 1_CD ,_, 2_CD -RRB-_-RRB- =_SYM 2_CD ∗_CD __NN max_NN −_NN -LRB-_-LRB- 1_CD ,_, 2_CD -RRB-_-RRB- -LRB-_-LRB- 1_CD -RRB-_-RRB- http://www.nltk.org/_NN Parameters_NNS num_topics_NNS passes_VBZ update_every_NNP alpha_NN eval_every_NN Values_NNS 400_CD 10_CD 1_CD `_`` auto_NN '_'' 10_CD Figure_NN 1_CD is_VBZ an_DT example_NN of_IN how_WRB we_PRP find_VBP the_DT most_RBS probable_JJ sense_NN in_IN second_JJ sentence_NN which_WDT has_VBZ the_DT maximum_NN WordNet_NNP similarity_NN with_IN word_NN in_IN first_JJ sentence_NN ._.
3.2_CD Corpus-Based_NNP Features_NNP Text_LSA_similarity_NNP measures_VBZ similarity_NN be_VB -_: tween_VB two_CD sentences_NNS and_CC using_VBG the_DT following_VBG Table_NNP 2_CD Parameter_NNP setting_NN of_IN LDA_NNP model_NN Sim_NNP -LRB-_-LRB- ,_, -RRB-_-RRB- 12_CD 12_CD scoring_NN function_NN -LRB-_-LRB- Mihalcea_NNP ,_, Corley_NNP et_FW al._FW ,_, 2006_CD -RRB-_-RRB- :_: =_SYM 1_CD -LRB-_-LRB- ∑_NN 2_CD 1_CD ument_NN frequency_NN of_IN w._NN -LRB-_-LRB- ,_, -RRB-_-RRB- ∗_CD -LRB-_-LRB- -RRB-_-RRB- ∈_CD -LCB-_-LRB- -RCB-_-RRB- 2_CD ∑_CD ∈_NN -LCB-_-LRB- -RCB-_-RRB- -LRB-_-LRB- -RRB-_-RRB- ∑_CD -LRB-_-LRB- ,_, -RRB-_-RRB- ∗_CD -LRB-_-LRB- -RRB-_-RRB- 1_CD +1_CD -RRB-_-RRB- -LRB-_-LRB- ,_, -RRB-_-RRB- =_SYM MAX_NNP -LCB-_-LRB- __NN -LRB-_-LRB- -LRB-_-LRB- -RRB-_-RRB- ,_, -LRB-_-LRB- __NN -RRB-_-RRB- -RRB-_-RRB- -RCB-_-RRB- ,_, __SYM ∈_FW -LRB-_-LRB- 8_CD -RRB-_-RRB- Latent_NN semantic_JJ analysis_NN -LRB-_-LRB- LSA_NNP -RRB-_-RRB- is_VBZ a_DT technique_NN for_IN comparing_VBG texts_NNS using_VBG a_DT vector-based_JJ representa_NN -_: tion_NN learned_VBD from_IN a_DT corpus_NN ._.
A_DT term-document_NN ma_FW trix_FW describes_VBZ the_DT occurrences_NNS of_IN terms_NNS in_IN docu_NN -_: ment_NN ._.
The_DT matrix_NN is_VBZ decomposed_VBN by_IN singular_JJ value_NN decomposition_NN -LRB-_-LRB- SVD_NNP -RRB-_-RRB- ._.
SVD_NNP is_VBZ a_DT factorization_NN of_IN a_DT SVD_NNP decompose_VB the_DT term-by-document_NN matrix_NN into_IN three_CD smaller_JJR matrixes_NNS like_IN follows_VBZ :_: ∈_CD -LCB-_-LRB- -RCB-_-RRB- 2_CD ∑_CD ∈_NN -LCB-_-LRB- 2_CD -RCB-_-RRB- -LRB-_-LRB- -RRB-_-RRB- =_SYM -LRB-_-LRB- 5_CD -RRB-_-RRB- This_DT similarity_NN score_NN has_VBZ a_DT value_NN between_IN 0_CD and_CC 1_CD ,_, with_IN a_DT score_NN 1_CD indicating_VBG identical_JJ text_NN segments_NNS ,_, and_CC a_DT score_NN 0_CD indicating_VBG no_DT semantic_JJ overlap_VBP be_VB -_: tween_VB two_CD texts_NNS ._.
real_JJ or_CC complex_JJ matrix_NN in_IN linear_JJ algebra_NN ._.
In_IN LSA_NNP ,_, where_WRB U_NNP and_CC V_NNP are_VBP column-orthogonal_JJ matrixes_NNS and_CC Σ_NNP is_VBZ a_DT diagonal_JJ matrix_NN containing_VBG singular_JJ val_NN -_: ues_NNS ._.
Now_RB ,_, columns_NNS in_IN U_NNP could_MD be_VB preserved_VBN as_IN the_DT semantic_JJ representations_NNS of_IN words_NNS ._.
Similarity_NN is_VBZ then_RB measured_VBN by_IN the_DT cosine_NN distance_NN between_IN their_PRP$ corresponding_JJ row_NN vectors_NNS ._.
To_TO make_VB full_JJ use_NN of_IN the_DT semantic_JJ information_NN in_IN LSA_NNP model_NN ,_, we_PRP proposed_VBD several_JJ methods_NNS to_TO compute_VB the_DT sentence_NN similarity_NN based_VBN on_IN LSA_NNP ._.
These_DT features_NNS incude_VBP :_: LSA_similarity_NNP ,_, Text_LSA_similarity_NNP ,_, IDF_LSA_similarity_NNP and_CC Freq_LSA_similarity_NNP ._.
In_IN our_PRP$ experiment_NN ,_, we_PRP directly_RB use_VBP the_DT LSA_NNP model_NN provided_VBN by_IN SEMILAR_NNP 2_CD -LRB-_-LRB- Ștefănescu_NNP ,_, Banjade_NNP et_FW al._FW ,_, 2014_CD -RRB-_-RRB- ._.
The_DT model_NN was_VBD decomposed_VBN from_IN the_DT whole_JJ 2014_CD Wikipedia_NNP articles_NNS ._.
One_CD word_NN is_VBZ represented_VBN as_IN a_DT 200-dimension_JJ real_JJ value_NN vector_NN ._.
We_PRP call_VBP it_PRP ``_`` LSA_NNP vector_NN ''_'' in_IN the_DT rest_NN of_IN the_DT paper_NN ._.
LSA_similarity_NNP represent_VBP a_DT sentence_NN by_IN summing_VBG We_PRP also_RB generated_VBD two_CD weighted_JJ features_NNS :_: IDF_LSA_similarity_NNP and_CC Freq_LSA_similarity_NNP ._.
-LRB-_-LRB- -RRB-_-RRB- -LRB-_-LRB- -RRB-_-RRB- =_SYM -LRB-_-LRB- -RRB-_-RRB- ∗_CD -LRB-_-LRB- -LRB-_-LRB- -RRB-_-RRB- -RRB-_-RRB- ∈_NN &_CC ∈_NN ̸_NN -LRB-_-LRB- 7_CD -RRB-_-RRB- -LRB-_-LRB- 9_CD -RRB-_-RRB- -LRB-_-LRB- -RRB-_-RRB- -LRB-_-LRB- -LRB-_-LRB- -RRB-_-RRB- -RRB-_-RRB- where_WRB -LRB-_-LRB- -RRB-_-RRB- is_VBZ the_DT word_NN frequency_NN of_IN w_NN ._.
In_IN our_PRP$ experiment_NN ,_, the_DT inverse_JJ document_NN frequency_NN and_CC word_NN frequency_NN of_IN each_DT word_NN is_VBZ computed_VBN on_IN Wik_NNP -_: ipedia_NN corpus_NN dumped_VBD in_IN December_NNP of_IN 2013_CD ._.
After_IN got_VBD the_DT vector_NN representations_NNS of_IN sentenc_NN -_: es_NNS ,_, the_DT cosine_NN distance_NN between_IN two_CD vectors_NNS is_VBZ the_DT value_NN of_IN two_CD features_NNS ._.
where_WRB StW_NNP is_VBZ the_DT predefined_VBN stop_NN words_NNS list_NN ,_, LSA_NNP -LRB-_-LRB- w_VBN -RRB-_-RRB- is_VBZ LSA_NNP vector_NN of_IN w_NN and_CC IDF_NNP -LRB-_-LRB- w_VBN -RRB-_-RRB- is_VBZ the_DT inverse_JJ doc_NN -_: -LRB-_-LRB- -RRB-_-RRB- =_SYM -LRB-_-LRB- -RRB-_-RRB- ∗_SYM ∈_NN &_CC ∈_NN ̸_NN -LRB-_-LRB- 10_CD -RRB-_-RRB- all_DT LSA_NNP vectors_NNS of_IN words_NNS appeared_VBD in_IN the_DT sentence_NN Thus_RB we_PRP can_MD get_VB vector_NN representations_NNS and_CC 12_CD and_CC then_RB averaged_VBD it_PRP with_IN the_DT length_NN of_IN the_DT sentence_NN ._.
of_IN the_DT two_CD sentences_NNS ._.
The_DT LSA_similarity_NNP could_MD be_VB measured_VBN with_IN cosine_NN similarity_NN between_IN the_DT two_CD vectors_NNS ._.
Latent_NN Dirichlet_NNP Allocation_NNP -LRB-_-LRB- LDA_NNP -RRB-_-RRB- -LRB-_-LRB- Blei_NNP ,_, Ng_NNP et_FW al._FW ,_, 2003_CD -RRB-_-RRB- is_VBZ a_DT widely_RB used_VBN topic_NN model_NN ,_, typically_RB used_VBN to_TO find_VB topics_NNS distribution_NN in_IN documents_NNS ;_: we_PRP tried_VBD this_DT technology_NN in_IN our_PRP$ model_NN ._.
The_DT LDA_NNP mod_NN -_: el_FW is_VBZ trained_VBN on_IN the_DT training_NN set_NN of_IN SemEval_NNP 2015_CD ._.
__NN -LRB-_-LRB- ,_, -RRB-_-RRB- =_SYM 12_CD The_DT Cosine_NNP similarity_NN is_VBZ defined_VBN as_IN follows_VBZ :_: 1_CD ∙_CD 2_CD ‖_NN 1_CD ‖_CD ‖_NN 2_CD ‖_CD -LRB-_-LRB- 6_CD -RRB-_-RRB- 2_CD http://www.semanticsimilarity.org/_NN In_IN our_PRP$ experiments_NNS ,_, we_PRP use_VBP the_DT gensim3_CD toolkit_NN -LRB-_-LRB- Řehůřek_NNP &_CC Sojka_NNP ,_, 2010_CD -RRB-_-RRB- to_TO find_VB the_DT topic_NN distribu_NN -_: tion_NN of_IN each_DT sentence_NN ,_, and_CC the_DT cosine_NN distance_NN of_IN the_DT vectors_NNS could_MD be_VB regarded_VBN as_IN the_DT topic_NN similari_SYM -_: ty_NN of_IN the_DT sentence_NN pair_NN ._.
The_DT parameter_NN setting_VBG in_IN the_DT experiment_NN is_VBZ shown_VBN in_IN Table_NNP 2_CD ._.
Furthermore_RB ,_, to_TO improve_VB our_PRP$ performance_NN ,_, we_PRP also_RB used_VBD the_DT recently_RB proposed-Sent2V_NN ec_FW -LRB-_-LRB- also_RB known_VBN as_IN paragraph_NN vector_NN -RRB-_-RRB- -LRB-_-LRB- Le_NNP &_CC Mikolov_NNP ,_, 2014_CD -RRB-_-RRB- to_TO represent_VB a_DT sentence_NN ._.
Paragraph_NNP Vector_NNP is_VBZ an_DT un_NN -_: supervised_JJ learning_NN algorithm_NN that_WDT learns_VBZ vector_NN representations_NNS for_IN variable_JJ length_NN pieces_NNS of_IN texts_NNS such_JJ as_IN sentences_NNS and_CC documents_NNS ._.
In_IN our_PRP$ experi_NNS -_: ment_NN ,_, we_PRP use_VBP the_DT open_JJ source_NN code_NN Sentence2vec4_NNP to_TO train_VB paragraph_NN vectors_NNS on_IN Wikipedia_NNP ._.
And_CC the_DT cosine_NN distance_NN between_IN two_CD paragraph_NN vectors_NNS denote_VBP the_DT sentence_NN semantic_JJ similarity_NN ._.
This_DT fea_NN -_: ture_NN is_VBZ called_VBN S2V_similarity_NNP ._.
In_IN our_PRP$ development_NN stage_NN ,_, we_PRP observed_VBD that_IN if_IN more_JJR corpora_NN were_VBD given_VBN to_TO train_VB Sent2Vec_NNP ,_, this_DT feature_NN could_MD be_VB more_RBR effec_SYM -_: tive_JJ ._.
3.4_CD Alignment-Based_NNP Features_VBZ RIC_Difference_NN measures_NNS difference_NN of_IN infor_NN -_: mation_NN content_NN the_DT sentences_NNS bearing_NN ._.
In_IN infor_NN -_: mation_NN theory_NN ,_, the_DT information_NN content_NN of_IN a_DT concept_NN can_MD be_VB quantified_VBN as_IN negative_JJ the_DT log_VB likelihood_NN -_: logp_NN -LRB-_-LRB- c_NN -RRB-_-RRB- ._.
In_IN our_PRP$ work_NN ,_, the_DT information_NN content_NN of_IN a_DT word_NN w_NN is_VBZ defined_VBN as_IN :_: fied_VBN as_IN :_: ∑_SYM ∈_SYM ∈_FW -LRB-_-LRB- ,_, -RRB-_-RRB- =_SYM -LRB-_-LRB- -RRB-_-RRB- =_SYM ln_SYM ∑_SYM ′_SYM ∈_CD -LRB-_-LRB- ′_NN -RRB-_-RRB- -LRB-_-LRB- 11_CD -RRB-_-RRB- -LRB-_-LRB- -RRB-_-RRB- where_WRB is_VBZ the_DT set_NN of_IN words_NNS in_IN the_DT corpus_NN and_CC -LRB-_-LRB- -RRB-_-RRB- is_VBZ the_DT frequency_NN of_IN the_DT word_NN in_IN the_DT corpus_NN ._.
We_PRP use_VBP the_DT Wikipedia_NNP to_TO obtain_VB word_NN fre_SYM -_: between_IN two_CD sentences_NNS and_CC could_MD be_VB quanti_NNS -_: Alignment_similarity_NNP is_VBZ a_DT similarity_NN measure_NN based_VBN on_IN monolingual_JJ alignment_NN ._.
We_PRP first_RB align_VBP related_JJ words_NNS across_IN the_DT two_CD input_NN sentences_NNS ._.
And_CC the_DT proportion_NN of_IN aligned_VBN content_NN words_NNS is_VBZ regarded_VBN as_IN their_PRP$ semantic_JJ similarity_NN ._.
In_IN our_PRP$ model_NN ,_, we_PRP di_FW -_: rectly_NN used_VBD the_DT monolingual_JJ word_NN aligner_NN provided_VBN by_IN -LRB-_-LRB- Sultan_NNP et_FW al._FW ,_, 2014_CD -RRB-_-RRB- ._.
The_DT aligner_NN is_VBZ based_VBN on_IN the_DT hypothesis_NN that_WDT words_NNS with_IN similar_JJ meanings_NNS repre_VBP -_: sent_VBN potential_JJ pairs_NNS for_IN alignment_NN if_IN located_VBN in_IN simi_NN -_: lar_NN contexts_NNS ._.
More_JJR details_NNS about_IN the_DT aligner_NN may_MD refer_VB to_TO the_DT paper_NN ,_, we_PRP did_VBD n't_RB discuss_VB here_RB ._.
Based_VBN on_IN the_DT alignment_NN results_NNS ,_, we_PRP can_MD compute_VB the_DT similari_NN -_: ty_NN using_VBG the_DT following_VBG formula_NN :_: quency_NN ._.
And_CC the_DT Information_NN Content_NN difference_NN 12_CD -LRB-_-LRB- -RRB-_-RRB- −_SYM ∑_FW -LRB-_-LRB- -RRB-_-RRB- 1_CD 2_CD -LRB-_-LRB- ∑_NN 1_CD -LRB-_-LRB- -RRB-_-RRB- ,_, ∑_FW 2_CD -LRB-_-LRB- -RRB-_-RRB- -RRB-_-RRB- ∈_SYM 1_CD ∈_CD 2_CD -LRB-_-LRB- 12_CD -RRB-_-RRB- 3.3_CD Word2Vec-Based_JJ Features_NNS Word2Vec_JJ -LRB-_-LRB- Mikolov_NNP ,_, Chen_NNP et_FW al._FW ,_, 2013_CD -RRB-_-RRB- is_VBZ a_DT lan_NN -_: guage_NN modeling_NN technique_NN that_WDT maps_NNS words_NNS from_IN vocabulary_NN to_TO continuous_JJ vectors_NNS -LRB-_-LRB- usually_RB 200_CD to_TO 500_CD dimensions_NNS -RRB-_-RRB- ._.
Recently_RB ,_, word_NN embedding_NN has_VBZ shown_VBN its_PRP$ ability_NN to_TO boost_VB the_DT performance_NN in_IN NLP_NNP tasks_NNS such_JJ as_IN syntactic_NN parsing_NN and_CC sentiment_NN anal_JJ -_: ysis_NN ._.
In_IN our_PRP$ work_NN ,_, we_PRP employ_VBP this_DT technology_NN to_TO represent_VB a_DT word_NN and_CC use_VB several_JJ different_JJ methods_NNS to_TO combine_VB these_DT word_NN vectors_NNS to_TO represent_VB a_DT sen_NN -_: tence_NN ._.
These_DT generated_VBD features_NNS include_VBP :_: W2V_similarity_NNP ,_, IDF_W2V_similarity_NNP ,_, Text_W2V_similarity_NNP and_CC S2V_similarity_NNP ._.
Similar_JJ to_TO generation_NN of_IN LSA-based_JJ features_NNS ,_, we_PRP generate_VBP W2V_similarity_NNP Text_W2V_similarity_NNP is_VBZ similar_JJ to_TO Text_LSA_similarity_NNP ,_, computed_VBD using_VBG the_DT same_JJ formula_NN ._.
Only_RB replace_VB the_DT maxSim_NN with_IN the_DT follow_VBP -_: ing_VBG formula_NN :_: -LRB-_-LRB- 1_CD ,_, 2_CD -RRB-_-RRB- =_SYM -LRB-_-LRB- 1_CD -RRB-_-RRB- +_NN -LRB-_-LRB- 2_LS -RRB-_-RRB- -LRB-_-LRB- 14_CD -RRB-_-RRB- -LRB-_-LRB- 1_CD -RRB-_-RRB- +_NN -LRB-_-LRB- 2_LS -RRB-_-RRB- w_RB here_RB -LRB-_-LRB- i_FW -RRB-_-RRB- and_CC -LRB-_-LRB- i_FW -RRB-_-RRB- are_VBP the_DT number_NN of_IN content_JJ words_NNS and_CC the_DT number_NN of_IN aligned_VBN content_NN words_NNS in_IN ._.
We_PRP did_VBD n't_RB achieve_VB as_RB good_JJ results_NNS as_IN in_IN the_DT pa_NN -_: per_IN ,_, the_DT reason_NN may_MD because_RB that_IN we_PRP did_VBD n't_RB consid_VB -_: er_VB some_DT stopwords_NNS in_IN that_DT filed_VBN ._.
-LRB-_-LRB- ,_, -RRB-_-RRB- =_SYM MAX_NNP -LCB-_-LRB- __NN -LRB-_-LRB- 2_CD -LRB-_-LRB- -RRB-_-RRB- ,_, 2_CD -LRB-_-LRB- -RRB-_-RRB- -RRB-_-RRB- ,_, ∈_FW -RCB-_-RRB- In_IN our_PRP$ experiments_NNS ,_, we_PRP also_RB used_VBD plenty_NN of_IN style-related_JJ features_NNS ,_, we_PRP call_VBP it_PRP ``_`` literal-based_JJ ''_'' fea_SYM -_: tures_NNS ._.
Here_RB ,_, we_PRP give_VBP a_DT short_JJ description_NN to_TO each_DT of_IN them_PRP ._.
-LRB-_-LRB- 13_CD -RRB-_-RRB- 3.5_CD Literal-Based_NNP Features_VBZ EditDistance_similarity_NNP is_VBZ based_VBN on_IN the_DT hypothesis_NN :_: two_CD sentences_NNS that_WDT look_VBP more_JJR similar_JJ are_VBP closer_RBR in_IN semantics_NNS ._.
So_IN we_PRP use_VBP the_DT Levenshtein_NNP Distance_NNP 4_CD https://github.com/klb3713/sentence2vec_JJ 3_CD http://radimrehurek.com/gensim/_NN over_IN characters_NNS to_TO measure_VB the_DT similarity_NN between_IN two_CD sentences_NNS ._.
DifferLen_Rate_NNP measures_VBZ the_DT difference_NN of_IN length_NN of_IN two_CD sentences_NNS which_WDT can_MD be_VB regarded_VBN as_IN evidence_NN of_IN comparing_VBG the_DT similarity_NN between_IN sen_NN -_: tences_NNS ._.
Shallow_NNP Syntactic_NNP Similarity_NNP considers_VBZ the_DT simi_NN -_: larity_NN in_IN terms_NNS of_IN English_NNP voices_NNS ._.
After_IN Part-Of_NNP -_: Speech_NN tagging_VBG to_TO each_DT sentence_NN ,_, we_PRP use_VBP the_DT Jac_NNP -_: card_NN Distance_NNP to_TO compute_VB the_DT syntactic_NN constituent_NN overlap_VBP ._.
Neg_Sentiment_Fea_NN is_VBZ feature_NN measures_NNS shal_SYM -_: low_JJ sentiment_NN of_IN sentences_NNS ,_, we_PRP manually_RB chose_VBD a_DT list_NN NEG_SENTIMENT_NNP =_SYM -LCB-_-LRB- `_`` no_RB '_'' ,_, `_`` not_RB '_'' ,_, ’n_JJ ev_SYM -_: er_NN '_'' ,_, '_'' little_JJ '_'' ,_, `_`` few_JJ '_'' ,_, `_`` nobody_NN '_'' ,_, `_`` neither_DT '_'' ,_, `_`` seldom_RB '_'' `_`` hardly_RB '_'' ,_, `_`` rarely_RB '_'' ,_, `_`` scarcely_RB '_'' -RCB-_-RRB- to_TO judge_VB the_DT senti_NNS -_: ment_NN ,_, the_DT appearance_NN of_IN word_NN in_IN this_DT list_NN indicating_VBG an_DT opposed_JJ meaning_NN ,_, if_IN only_RB one_CD word_NN in_IN the_DT list_NN appeared_VBD only_RB once_RB in_IN this_DT pair_NN of_IN sentences_NNS ,_, we_PRP think_VBP that_IN this_DT pair_NN of_IN sentences_NNS expressing_VBG oppo_SYM -_: site_NN meaning_NN ._.
Digit_in_Fea_NNP is_VBZ a_DT binary_JJ feature_NN which_WDT cares_VBZ about_IN whether_IN there_EX is_VBZ digit_NN numbers_NNS appeared_VBN in_IN only_JJ sentence_NN in_IN the_DT pair_NN ._.
To_TO our_PRP$ intuitive_JJ ,_, if_IN only_RB one_CD sentence_NN obtain_VB numbers_NNS in_IN it_PRP and_CC another_DT con_NN -_: tains_NNS only_RB text_NN ,_, then_RB human_JJ annotators_NNS tend_VBP to_TO give_VB a_DT lower_JJR score_NN to_TO this_DT pair_NN ._.
So_RB ,_, if_IN Digit_in_Fea_NNP of_IN a_DT pair_NN of_IN sentences_NNS was_VBD set_VBN to_TO `_`` 1_CD '_'' ,_, this_DT can_MD be_VB inter_NN -_: preted_VBN to_TO give_VB classifier_NN a_DT signal_NN to_TO give_VB a_DT lower_JJR similarity_NN score_NN ._.
Digit_similarity_NN could_MD be_VB regarded_VBN as_IN comple_NN -_: ment_NN to_TO feature_VB Digit_in_Fea_NNP ._.
We_PRP implemented_VBD a_DT simple_JJ algorithm_NN to_TO extract_VB numbers_NNS from_IN text.and_JJ then_RB compares_VBZ the_DT difference_NN of_IN numbers_NNS appeared_VBN in_IN two_CD sentences_NNS ._.
No_overlap_Fea_NN measures_NNS whether_IN two_CD sen_SYM -_: tences_NNS are_VBP totally_RB different_JJ in_IN terms_NNS of_IN words_NNS ap_SYM -_: peared_VBN in_IN the_DT sentences_NNS ._.
Although_IN this_DT hypothesis_NN is_VBZ not_RB always_RB true_JJ ,_, but_CC we_PRP observed_VBD that_IN this_DT as_IN -_: sumption_NN is_VBZ correct_JJ under_IN most_JJS cases_NNS and_CC this_DT fea_NN -_: ture_NN still_RB contributes_VBZ to_TO our_PRP$ overall_JJ performance_NN ._.
4_CD Experiments_NNS 4.1_CD Datasets_NNPS In_IN SemEval_NNP 2015_CD 2a_CD ,_, the_DT trial_NN dataset_NN comprises_VBZ the_DT 2012_CD ,_, 2013_CD and_CC 2014_CD datasets_NNS ,_, which_WDT can_MD be_VB used_VBN to_TO develop_VB and_CC train_VB models_NNS ._.
The_DT details_NNS of_IN the_DT da_NN -_: taset_NN refer_VBP to_TO -LRB-_-LRB- Agirre_NNP &_CC Banea_NNP ,_, 2015_CD -RRB-_-RRB- ._.
4.2_CD Evaluation_NN Metrics_NNS The_DT official_JJ estimation_NN is_VBZ based_VBN on_IN the_DT average_NN of_IN Pearson_NNP correlation_NN ._.
This_DT metric_JJ is_VBZ determined_VBN as_IN :_: =_SYM ∑_SYM −_SYM ∑_SYM ∑_FW -LRB-_-LRB- 15_CD -RRB-_-RRB- ,_, 2_CD -LRB-_-LRB- ∑_NN -RRB-_-RRB- 2_CD -LRB-_-LRB- ∑_NN -RRB-_-RRB- ∑_SYM −_SYM 2_CD ∑_CD −_NN 2_CD Classifier_NNP kernel_NN Gamma_NNP C_NNP epsilon_NNP Clf-1_NNP `_`` rbf_NN '_'' 0.1_CD 1.8_CD 0.1_CD Clf-2_NN `_`` rbf_NN '_'' 0.16_CD 100_CD 0.1_CD Clf-3_NN `_`` rbf_NN '_'' 0.16_CD 100_CD 0.1_CD We_PRP conduct_VB our_PRP$ experiments_NNS on_IN the_DT SemEval_NNP 2015_CD where_WRB X_NNP is_VBZ the_DT golden-standard_JJ scores_NNS vector_NN ,_, and_CC Y_NNP is_VBZ the_DT output_NN of_IN SVRs_NNS ._.
4.3_CD Results_NNS and_CC Discussion_NNP To_TO achieve_VB a_DT better_JJR result_NN ,_, we_PRP trained_VBD three_CD Support_NN V_NN ector_NN Regression_NN models_NNS to_TO predict_VB similarity_NN scores_NNS on_IN different_JJ test_NN sets_NNS ,_, we_PRP used_VBD all_PDT the_DT da_NN -_: tasets_NNS -LRB-_-LRB- except_IN SMT_NNP of_IN 2013_CD ,_, we_PRP got_VBD worse_JJR perfor_SYM -_: mance_NN after_IN added_VBD it_PRP ,_, so_IN we_PRP exclude_VBP SMT_NNP in_IN our_PRP$ final_JJ model_NN -RRB-_-RRB- before_IN 2015_CD as_IN training_NN set_VBN for_IN the_DT first_JJ three_CD test_NN sets_NNS which_WDT are_VBP unseen_JJ data_NNS for_IN the_DT classi_NNS -_: fier_NN ._.
This_DT classifier_NN was_VBD denoted_VBN as_IN Clf-1_NN ._.
For_IN head_NN -_: lines_NNS and_CC images_NNS ,_, all_DT headlines_NNS /_VBP images_NNS data_NNS sets_NNS appeared_VBD before_IN were_VBD used_VBN as_IN training_NN sets_NNS ._.
The_DT trained_JJ classifier_NN was_VBD denoted_VBN as_IN Clf-2_NN and_CC Clf-3_NN respectively_RB ._.
5_CD In_IN terms_NNS of_IN implementation_NN ,_, we_PRP used_VBD Scikit-learn_JJ toolkit_NN -LRB-_-LRB- Pedregosa_NNP ,_, Varoquaux_NNP et_FW al._FW ,_, 2011_CD -RRB-_-RRB- to_TO do_VB the_DT classification_NN and_CC the_DT parameter_NN settings_NNS for_IN three_CD SVR_NNP models_NNS are_VBP shown_VBN in_IN the_DT following_VBG table_NN ,_, we_PRP chose_VBD these_DT parameters_NNS by_IN experiences_NNS ,_, Clf-2_NN and_CC Clf-3_NN used_VBD the_DT same_JJ setting_NN ,_, and_CC a_DT better_JJR result_NN may_MD be_VB achieved_VBN through_IN fine_JJ tuning_NN :_: Table_NNP 3_CD Parameter_NNP settings_NNS of_IN our_PRP$ three_CD classifiers_NNS After_IN the_DT prediction_NN of_IN the_DT similarity_NN scores_NNS of_IN sentences_NNS ,_, we_PRP conducted_VBD a_DT post-processing_JJ step_NN to_TO boost_VB and_CC correct_VB results_NNS ,_, we_PRP truncate_VBP at_IN the_DT extre_NN -_: 5_CD http://scikit-learn.org/stable/_JJ lish_NN text_NN ,_, and_CC ,_, we_PRP need_VBP to_TO compute_VB how_WRB similar_JJ 12_CD STS_NNP English_NNP subtask_NN ._.
Given_VBN two_CD sentences_NNS of_IN Eng_NNP -_: 1_CD and_CC 2_CD are_VBP ,_, returning_VBG a_DT similarity_NN score_NN between_IN 0.0_CD -LRB-_-LRB- no_DT relation_NN -RRB-_-RRB- to_TO 5.0_CD -LRB-_-LRB- semantic_JJ equivalence_NN -RRB-_-RRB- ,_, indicating_VBG the_DT semantic_JJ similarity_NN between_IN two_CD sen_SYM -_: tences_NNS ._.
Data_NNP Set_NNP Ans-for_NNP Ans-stu_NNP Belief_NNP Hdlines_NNP Images_VBZ Mean_NNP All_NNP features_VBZ 0.7381_CD 0.7644_CD 0.7377_CD 0.8521_CD 0.8650_CD 0.8049_CD w/o_NN WordNet-based_JJ 0.7356_CD 0.7516_CD 0.7260_CD 0.8450_CD 0.8560_CD 0.7959_CD w/o_NN Corpus-based_JJ 0.7150_CD 0.7850_CD 0.7389_CD 0.8387_CD 0.8620_CD 0.8032_CD w/o_NN Word2Vec-based_JJ 0.7460_CD 0.7498_CD 0.7366_CD 0.8510_CD 0.8536_CD 0.7989_CD w/o_NN Alignment-based_JJ 0.7278_CD 0.7551_CD 0.7168_CD 0.8355_CD 0.8614_CD 0.7926_CD w/o_NN Literal-based_JJ 0.7175_CD 0.7320_CD 0.7501_CD 0.8240_CD 0.8618_CD 0.7879_CD Table_NNP 4_CD Performance_NNP of_IN different_JJ feature_NN combinations_NNS -LRB-_-LRB- exclude_VB one_CD kind_NN each_DT time_NN -RRB-_-RRB- Table_NNP 5_CD Results_NNS of_IN comparing_VBG the_DT importance_NN of_IN different_JJ kinds_NNS of_IN features_NNS on_IN SemEval_NNP 2015_CD Feature_NNP Set_NNP Ans-for_NNP Ans-stu_NNP Belief_NNP Hdlines_NNP Images_VBZ Mean_NNP All_NNP features_VBZ 0.7381_CD 0.7644_CD 0.7377_CD 0.8521_CD 0.8650_CD 0.8049_CD WordNet-based_JJ 0.6813_CD 0.7252_CD 0.7289_CD 0.7509_CD 0.8352_CD 0.7541_CD Corpus-based_JJ 0.6182_CD 0.6245_CD 0.6652_CD 0.7257_CD 0.8254_CD 0.7043_CD Word2Vec-based_JJ 0.6065_CD 0.7305_CD 0.6904_CD 0.7365_CD 0.8369_CD 0.7381_CD Alignment-based_JJ 0.6675_CD 0.7789_CD 0.6699_CD 0.7891_CD 0.7872_CD 0.7560_CD Literal-based_JJ 0.6666_CD 0.5725_CD 0.5235_CD 0.5493_CD 0.3326_CD 0.5123_CD mes_NNS to_TO keep_VB the_DT score_NN in_IN -LSB-_NNP 0.0_CD ,_, 5.0_CD -RSB-_NNP ,_, and_CC an_DT addi_NN -_: tional_JJ step_NN similar_JJ to_TO the_DT details_NNS in_IN -LRB-_-LRB- Bär_JJ et_FW al._FW ,_, 2012_CD -RRB-_-RRB- ._.
The_DT post-processing_JJ step_NN contributed_VBD a_DT 0.1_CD %_NN im_SYM -_: provement_NN in_IN our_PRP$ overall_JJ performance_NN ._.
Table_NNP 6_CD Performances_NNPS of_IN our_PRP$ model_NN and_CC winning_VBG system_NN on_IN SemEval_NNP 2015_CD STS_NNP test_NN sets_VBZ Table_NNP 4_CD reported_VBD the_DT results_NNS of_IN our_PRP$ method_NN on_IN SemEval_NNP 2015_CD Task_NNP 2a_CD ,_, from_IN which_WDT we_PRP can_MD know_VB that_IN our_PRP$ method_NN outperformed_VBD the_DT winning_VBG system_NN by_IN a_DT big_JJ margin_NN on_IN the_DT headlines_NNS ,_, but_CC only_RB slightly_RB better_RBR on_IN the_DT images_NNS ._.
The_DT reason_NN may_MD because_RB that_IN in_IN the_DT winning_VBG system_NN ,_, images_NNS was_VBD already_RB achieved_VBN a_DT very_RB high_JJ accuracy_NN ,_, but_CC due_JJ to_TO the_DT in_IN -_: complete_JJ use_NN of_IN the_DT semantic_JJ information_NN ,_, did_VBD n't_RB perform_VB as_RB well_RB as_IN in_IN headlines_NNS ._.
As_IN our_PRP$ method_NN used_VBD more_JJR sufficient_JJ features_NNS ,_, our_PRP$ approach_NN achieved_VBD both_DT state-of-the-art_JJ results_NNS on_IN headlines_NNS and_CC images_NNS ._.
The_DT winning_VBG system_NN mainly_RB based_VBN on_IN word_NN alignment_NN ,_, which_WDT guaranteed_VBD very_RB good_JJ gen_NN -_: eralization_NN ability_NN ,_, but_CC much_RB of_IN the_DT semantic_JJ infor_NN -_: mation_NN contained_VBN in_IN the_DT training_NN set_NN was_VBD not_RB used_VBN ,_, while_IN these_DT information_NN can_MD also_RB contribute_VB to_TO the_DT system_NN performance_NN ,_, especially_RB for_IN domain_NN -_: specific_JJ test_NN set_NN ,_, in_IN other_JJ word_NN ,_, our_PRP$ method_NN can_MD be_VB used_VBN to_TO verify_VB this_DT idea_NN ._.
For_IN the_DT first_JJ three_CD datasets_NNS ,_, our_PRP$ method_NN may_MD achieve_VB much_RB better_JJR performance_NN if_IN more_JJR domain-specific_JJ data_NNS was_VBD given_VBN for_IN learn_VBP -_: ing_NN ._.
Overall_RB ,_, our_PRP$ system_NN performed_VBD slightly_RB better_JJR than_IN the_DT wining_NN system_NN in_IN terms_NNS of_IN average_JJ Pearson_NNP correlation_NN ._.
To_TO compare_VB the_DT importance_NN of_IN each_DT kind_NN of_IN fea_NN -_: ture_NN ,_, we_PRP separately_RB exclude_VBP one_CD kind_NN of_IN them_PRP in_IN our_PRP$ model_NN and_CC compare_VB new_JJ model_NN 's_POS performance_NN ._.
The_DT results_NNS are_VBP shown_VBN in_IN Table_NNP 5_CD ._.
And_CC the_DT per_FW -_: formance_NN of_IN using_VBG only_RB one_CD kind_NN of_IN feature_NN showed_VBD in_IN Table_NNP 6_CD ._.
The_DT experiment_NN results_NNS demonstrated_VBD the_DT effec_NN -_: tiveness_NN of_IN our_PRP$ generated_VBN features_NNS ,_, except_IN Liter_NNP -_: based_VBN features_NNS ,_, each_DT kind_NN of_IN other_JJ features_NNS alone_RB could_MD lead_VB to_TO a_DT relatively_RB good_JJ performance_NN ._.
Alt_SYM -_: hough_NN Literal-based_JJ features_NNS did_VBD n't_RB perform_VB well_RB on_IN its_PRP$ own_JJ ,_, exclude_VB it_PRP from_IN our_PRP$ model_NN leads_VBZ to_TO the_DT biggest_JJS decrease_NN in_IN Mean_JJ correlation_NN ,_, which_WDT indi_SYM -_: cated_VBN it_PRP is_VBZ an_DT important_JJ complement_VBP to_TO other_JJ fea_NN -_: tures_NNS ._.
We_PRP also_RB observed_VBD that_IN corpus-based_JJ features_NNS seem_VBP less_JJR effective_JJ compared_VBN to_TO other_JJ features_NNS as_IN they_PRP did_VBD n't_RB perform_VB as_RB well_RB as_IN other_JJ semantic_JJ relat_NN -_: ed_VBD features_NNS and_CC the_DT absence_NN of_IN it_PRP has_VBZ little_JJ impact_NN on_IN the_DT overall_JJ performance_NN ._.
The_DT different_JJ combina_NN -_: tions_NNS of_IN them_PRP boosted_VBD the_DT results_NNS to_TO achieve_VB a_DT high_JJ -_: er_NN correlation_NN ._.
Also_RB ,_, SVR_NNP model_NN played_VBD an_DT Test_NN Set_NNP Winning_VBG team_NN Our_PRP$ System_NNP answers-forums_NNS 0.7390_CD 0.7381_CD answers-students_NNS 0.7725_CD 0.7644_CD belief_NN 0.7491_CD 0.7377_CD headlines_NNS 0.8250_CD 0.8521_CD images_NNS 0.8644_CD 0.8650_CD Weight_NNP Mean_NNP 0.8015_CD 0.8049_CD important_JJ role_NN in_IN our_PRP$ approach_NN ,_, it_PRP provide_VB a_DT good_JJ out-of-sample_JJ generalization_NN as_IN the_DT loss_NN function_NN typically_RB leads_VBZ to_TO a_DT sparse_JJ representation_NN of_IN the_DT decision_NN rule_NN which_WDT makes_VBZ our_PRP$ model_NN more_RBR robust_JJ on_IN novel_NN data_NNS ._.
And_CC we_PRP think_VBP that_IN the_DT appropriate_JJ choice_NN of_IN kernel_NN function_NN in_IN SVR_NNP may_MD also_RB help_VB a_DT lot_NN in_IN the_DT model_NN ._.
The_DT authors_NNS would_MD like_VB to_TO thank_VB the_DT anonymous_JJ reviewers_NNS for_IN their_PRP$ helpful_JJ suggestions_NNS and_CC com_NN -_: ments_NNS ._.
This_DT work_NN is_VBZ supported_VBN by_IN the_DT National_NNP Natural_NNP Science_NNP Foundation_NNP of_IN China_NNP -LRB-_-LRB- 61300114_CD &_CC 61572151_CD -RRB-_-RRB- and_CC Natural_JJ Science_NN Foundation_NNP of_IN Hei_NNP -_: longjiang_NN Province_NNP -LRB-_-LRB- F201132_NNP -RRB-_-RRB- ._.
References_NNS Agirre_NNP Eneko_NNP ,_, &_CC Banea_NNP Carmen_NNP ._.
-LRB-_-LRB- 2015_CD -RRB-_-RRB- ._.
SemEval-2015_NN task_NN 2_CD :_: Semantic_JJ textual_JJ similarity_NN ,_, English_NNP ,_, S-panish_JJ and_CC pilot_NN on_IN interpretability_NN ._.
Paper_NNP presented_VBD at_IN the_DT Proceedings_NNP of_IN the_DT 9th_NNP International_NNP Workshop_NNP on_IN Semantic_NNP Evaluation_NNP -LRB-_-LRB- SemEval_NNP 2015_CD -RRB-_-RRB- ,_, June_NNP ._.
Bär_NNP Daniel_NNP ,_, Biemann_NNP Chris_NNP ,_, Gurevych_NNP Iryna_NNP ,_, &_CC Zesch_NNP Torsten_NNP ._.
-LRB-_-LRB- 2012_CD -RRB-_-RRB- ._.
Ukp_NNP :_: Computing_NNP semantic_JJ textual_JJ similarity_NN by_IN combining_VBG multiple_JJ content_NN similarity_NN measures_NNS ._.
Paper_NNP presented_VBD at_IN the_DT Proceedings_NNP of_IN the_DT First_NNP Joint_NNP Conference_NNP on_IN Lexical_NNP and_CC Computational_NNP Semantics-V_NNP olume_NN 1_CD :_: Proceedings_NNP of_IN the_DT main_JJ conference_NN and_CC the_DT shared_VBN task_NN ,_, and_CC Volume_NN 2_CD :_: Proceedings_NNP of_IN the_DT Sixth_NNP International_NNP Workshop_NNP on_IN Semantic_NNP Evaluation_NNP ._.
Bird_NN Steven_NNP ._.
-LRB-_-LRB- 2006_CD -RRB-_-RRB- ._.
NLTK_NNP :_: the_DT natural_JJ language_NN toolkit_NN ._.
Paper_NNP presented_VBD at_IN the_DT Proceedings_NNP of_IN the_DT COLING/ACL_NNP on_IN Interactive_NNP presentation_NN sessions_NNS ._.
Blei_NNP David_NNP M_NNP ,_, Ng_NNP Andrew_NNP Y_NNP ,_, &_CC Jordan_NNP Michael_NNP I._NNP -LRB-_-LRB- 2003_CD -RRB-_-RRB- ._.
Latent_NN dirichlet_NN allocation_NN ._.
the_DT Journal_NNP of_IN machine_NN Learning_NNP research_NN ,_, 3_CD ,_, 993-1022_CD ._.
Burgess_NNP Curt_NNP ,_, Livesay_NNP Kay_NNP ,_, &_CC Lund_NNP Kevin_NNP ._.
-LRB-_-LRB- 1998_CD -RRB-_-RRB- ._.
Explorations_NNS in_IN context_NN space_NN :_: Words_NNS ,_, sentences_NNS ,_, discourse_NN ._.
Discourse_NN Processes_NNS ,_, 25_CD -LRB-_-LRB- 2-3_CD -RRB-_-RRB- ,_, 211-257_CD ._.
Dumais_NNP Susan_NNP T._NNP -LRB-_-LRB- 2004_CD -RRB-_-RRB- ._.
Latent_NN semantic_JJ analysis_NN ._.
Annual_JJ review_NN of_IN information_NN science_NN and_CC technology_NN ,_, 38_CD -LRB-_-LRB- 1_LS -RRB-_-RRB- ,_, 188-230_CD ._.
Fattah_NNP Mohamed_NNP Abdel_NNP ,_, &_CC Ren_NNP Fuji_NNP ._.
-LRB-_-LRB- 2008_CD -RRB-_-RRB- ._.
Automatic_NNP text_NN summarization_NN ._.
World_NNP Academy_NNP of_IN Science_NNP ,_, Engineering_NNP and_CC Technology_NNP ,_, 37_CD ,_, 2008_CD ._.
Gabrilovich_NNP Evgeniy_NNP ,_, &_CC Markovitch_NNP Shaul_NNP ._.
-LRB-_-LRB- 2007_CD -RRB-_-RRB- ._.
Computing_NNP Semantic_NNP Relatedness_NNP Using_VBG Wikipedia_NNP -_: based_VBN Explicit_NNP Semantic_NNP Analysis_NNP ._.
Paper_NNP presented_VBD at_IN the_DT IJCAI_NNP ._.
Han_NNP Lushan_NNP ,_, Kashyap_NNP Abhay_NNP ,_, Finin_NNP Tim_NNP ,_, Mayfield_NNP James_NNP ,_, &_CC Weese_NNP Jonathan_NNP ._.
-LRB-_-LRB- 2013_CD -RRB-_-RRB- ._.
UMBC_NNP EBIQUITY-CORE_NNP :_: Semantic_JJ textual_JJ similarity_NN systems_NNS ._.
Atlanta_NNP ,_, Georgia_NNP ,_, USA_NNP ,_, 44_CD ._.
Islam_NNP Aminul_NNP ,_, &_CC Inkpen_NNP Diana_NNP ._.
-LRB-_-LRB- 2008_CD -RRB-_-RRB- ._.
Semantic_JJ text_NN similarity_NN using_VBG corpus-based_JJ word_NN similarity_NN and_CC string_NN similarity_NN ._.
ACM_NNP Transactions_NNS on_IN Knowledge_NNP Discovery_NNP from_IN Data_NNP -LRB-_-LRB- TKDD_NNP -RRB-_-RRB- ,_, 2_CD -LRB-_-LRB- 2_LS -RRB-_-RRB- ,_, 10_CD ._.
Le_NNP Quoc_NNP V_NNP ,_, &_CC Mikolov_NNP Tomas_NNP ._.
-LRB-_-LRB- 2014_CD -RRB-_-RRB- ._.
Distributed_VBN representations_NNS of_IN sentences_NNS and_CC documents_NNS ._.
Paper_NNP presented_VBD at_IN the_DT Proceedings_NNP of_IN NIPS_NNP 2013_CD Li_NNP Yuhua_NNP ,_, McLean_NNP David_NNP ,_, Bandar_NNP Zuhair_NNP ,_, O'shea_NNP James_NNP D_NNP ,_, &_CC Crockett_NNP Keeley_NNP ._.
-LRB-_-LRB- 2006_CD -RRB-_-RRB- ._.
Sentence_NN similarity_NN 5_CD Conclusion_NN In_IN this_DT paper_NN ,_, we_PRP presented_VBD our_PRP$ approach_NN to_TO evalu_VB -_: ate_VBD semantic_JJ similarity_NN between_IN short_JJ English_JJ sen_NN -_: tences_NNS ._.
We_PRP employed_VBD a_DT Support_NN Vector_NNP Regression_NNP model_NN combined_VBN with_IN WordNet-Based_JJ features_NNS ,_, Corpus-Based_JJ features_NNS ,_, Word2Vec-based_JJ features_NNS ,_, Binary_NNP Features_NNPS and_CC some_DT other_JJ features_NNS to_TO predict_VB the_DT semantic_JJ similarity_NN score_NN between_IN sentence_NN pairs_NNS ._.
Our_PRP$ experiment_NN results_NNS showed_VBD a_DT high_JJ corre_NN -_: lation_NN with_IN human_JJ annotations_NNS which_WDT outperformed_VBD the_DT top_JJ system_NN in_IN SemEval_NNP 2015_CD task_NN 2a_NN ._.
We_PRP also_RB observed_VBD that_IN our_PRP$ method_NN performed_VBD much_RB better_JJR compared_VBN to_TO winning_VBG system_NN on_IN two_CD test_NN sets_NNS whose_WP$ domain-specific_JJ data_NNS is_VBZ available_JJ for_IN train_NN -_: ing_NN ,_, results_NNS also_RB indicated_VBD that_IN our_PRP$ solution_NN still_RB maintains_VBZ good_JJ generation_NN ability_NN on_IN novel_NN datasets_NNS which_WDT means_VBZ this_DT technique_NN could_MD be_VB well_RB general_JJ -_: ized_VBN to_TO other_JJ data_NNS domains_NNS ._.
While_IN the_DT context_NN of_IN the_DT sentence_NN is_VBZ unavailable_JJ and_CC the_DT information_NN about_IN the_DT tone_NN of_IN sentence_NN was_VBD eliminated_VBN by_IN us_PRP -LRB-_-LRB- most_RBS modal_JJ particles_NNS and_CC punctuations_NNS appeared_VBD in_IN sentences_NNS were_VBD treated_VBN as_IN stop_NN words_NNS in_IN our_PRP$ pro-_JJ cess_NN -RRB-_-RRB- ,_, our_PRP$ model_NN could_MD not_RB distinguish_VB the_DT tone_NN of_IN sentence_NN ,_, for_IN example_NN we_PRP may_MD give_VB a_DT high_JJ similari_NN -_: ty_NN score_NN to_TO a_DT sentence_NN pair_NN consists_VBZ of_IN a_DT declarative_JJ and_CC an_DT imperative_JJ if_IN they_PRP shared_VBD many_JJ words_NNS ._.
This_DT situation_NN was_VBD not_RB considered_VBN in_IN feature_NN generation_NN stage_NN ,_, but_CC will_MD be_VB researched_VBN latter_JJ ._.
Our_PRP$ future_JJ work_NN will_MD include_VB the_DT refinements_NNS of_IN training_NN effec_NN -_: tive_JJ representations_NNS for_IN words_NNS and_CC sentences_NNS on_IN corpus_NN -LRB-_-LRB- LSA_NNP ,_, Word2Vec_NNP and_CC Sent2Vec_NNP -RRB-_-RRB- ,_, the_DT ex_FW -_: pansion_NN of_IN stop_NN word_NN list_NN through_IN adding_VBG proper_JJ selected_VBN domain-specific_JJ stop_NN words_NNS and_CC the_DT re_NN -_: implementation_NN of_IN a_DT well-designed_JJ feature_NN selec_NN -_: tion_NN process_NN to_TO simplify_VB our_PRP$ model_NN ._.
We_PRP hope_VBP that_IN these_DT measures_NNS could_MD be_VB helpful_JJ for_IN improvement_NN ,_, make_VB our_PRP$ model_NN more_RBR robust_JJ and_CC improve_VB our_PRP$ method_NN 's_POS generalization_NN ability_NN as_RB well_RB ._.
Acknowledgments_NNS based_VBN on_IN semantic_JJ nets_NNS and_CC corpus_NN statistics_NNS ._.
Knowledge_NN and_CC Data_NNP Engineering_NNP ,_, IEEE_NNP Transactions_NNS on_IN ,_, 18_CD -LRB-_-LRB- 8_CD -RRB-_-RRB- ,_, 1138-1150_CD ._.
Lin_NNP Dekang_NNP ._.
-LRB-_-LRB- 1998_CD -RRB-_-RRB- ._.
An_DT information-theoretic_JJ definition_NN of_IN similarity_NN ._.
Paper_NNP presented_VBD at_IN the_DT ICML_NNP ._.
Liu_NNP Yang_NNP ,_, Sun_NNP Chengjie_NNP ,_, Lin_NNP Lei_NNP ,_, &_CC Wang_NNP Xiaolong_NNP ._.
yiGou_NNP :_: A_NNP Semantic_NNP Text_NNP Similarity_NNP Computing_NNP System_NNP Based_VBD on_IN SVM_NNP ._.
Paper_NNP presented_VBD at_IN the_DT Proceedings_NNP of_IN the_DT 9th_NNP International_NNP Workshop_NNP on_IN Semantic_NNP Evaluation_NNP ,_, pages_NNS 80-84_CD ,_, Denver_NNP ,_, Colorado_NNP ,_, USA_NNP ._.
Manning_NNP Christopher_NNP D_NNP ,_, Raghavan_NNP Prabhakar_NNP ,_, &_CC Schütze_NNP Hinrich_NNP ._.
-LRB-_-LRB- 2008_CD -RRB-_-RRB- ._.
Introduction_NN to_TO information_NN retrieval_NN -LRB-_-LRB- Vol_NNP ._.
1_LS -RRB-_-RRB- :_: Cambridge_NNP university_NN press_NN Cambridge_NNP ._.
Meng_NNP Lingling_NNP ,_, Huang_NNP Runqing_NNP ,_, &_CC Gu_NNP Junzhong_NNP ._.
-LRB-_-LRB- 2013_CD -RRB-_-RRB- ._.
A_DT review_NN of_IN semantic_JJ similarity_NN measures_NNS in_IN wordnet_NN ._.
International_NNP Journal_NNP of_IN Hybrid_NNP Information_NNP Technology_NNP ,_, 6_CD -LRB-_-LRB- 1_LS -RRB-_-RRB- ,_, 1-12_CD ._.
Metzler_NNP Donald_NNP ,_, Dumais_NNP Susan_NNP ,_, &_CC Meek_NNP Christopher_NNP ._.
-LRB-_-LRB- 2007_CD -RRB-_-RRB- ._.
Similarity_NN measures_NNS for_IN short_JJ segments_NNS of_IN text_NN :_: Springer_NNP ._.
Mihalcea_NNP Rada_NNP ,_, Corley_NNP Courtney_NNP ,_, &_CC Strapparava_NNP Carlo_NNP ._.
-LRB-_-LRB- 2006_CD -RRB-_-RRB- ._.
Corpus-based_JJ and_CC knowledge-based_JJ measures_NNS of_IN text_NN semantic_JJ similarity_NN ._.
Paper_NNP presented_VBD at_IN the_DT AAAI_NNP ._.
Mikolov_NNP Tomas_NNP ,_, Chen_NNP Kai_NNP ,_, Corrado_NNP Greg_NNP ,_, &_CC Dean_NNP Jeffrey_NNP ._.
-LRB-_-LRB- 2013_CD -RRB-_-RRB- ._.
Efficient_JJ estimation_NN of_IN word_NN representations_NNS in_IN vector_NN space_NN ._.
Paper_NNP presented_VBD at_IN the_DT ICLR_NNP ,_, 2013_CD ._.
Miller_NNP George_NNP A._NNP -LRB-_-LRB- 1995_CD -RRB-_-RRB- ._.
WordNet_NNP :_: a_DT lexical_JJ database_NN for_IN English_NNP ._.
Communications_NNPS of_IN the_DT ACM_NNP ,_, 38_CD -LRB-_-LRB- 11_CD -RRB-_-RRB- ,_, 39-41_CD ._.
Narayanan_NNP Srini_NNP ,_, &_CC Harabagiu_NNP Sanda_NNP ._.
-LRB-_-LRB- 2004_CD -RRB-_-RRB- ._.
Answering_VBG questions_NNS using_VBG advanced_JJ semantics_NNS and_CC probabilistic_JJ inference_NN ._.
Paper_NNP presented_VBD at_IN the_DT Proceedings_NNP of_IN the_DT Workshop_NNP on_IN Pragmatics_NNPS of_IN Question_NNP Answering_NNP ,_, HLT-NAACL_NNP ,_, Boston_NNP ,_, USA_NNP ._.
Papineni_NNP Kishore_NNP ,_, Roukos_NNP Salim_NNP ,_, Ward_NNP Todd_NNP ,_, &_CC Zhu_NNP Wei-Jing_NNP ._.
-LRB-_-LRB- 2002_CD -RRB-_-RRB- ._.
BLEU_NNP :_: a_DT method_NN for_IN automatic_JJ evaluation_NN of_IN machine_NN translation_NN ._.
Paper_NNP presented_VBD at_IN the_DT Proceedings_NNP of_IN the_DT 40th_JJ annual_JJ meeting_NN on_IN association_NN for_IN computational_JJ linguistics_NNS ._.
Pedersen_NNP Ted_NNP ,_, Patwardhan_NNP Siddharth_NNP ,_, &_CC Michelizzi_NNP Jason_NNP ._.
-LRB-_-LRB- 2004_CD -RRB-_-RRB- ._.
WordNet_NNP :_: :_: Similarity_NN :_: measuring_VBG the_DT relatedness_NN of_IN concepts_NNS ._.
Paper_NNP presented_VBD at_IN the_DT Demonstration_NNP papers_NNS at_IN hlt-naacl_JJ 2004_CD ._.
Workshop_NNP on_IN New_NNP Challenges_VBZ for_IN NLP_NNP Frameworks_NNPS ,_, pages_NNS 45_CD --_: 50_CD ,_, Valletta_NNP ,_, Malta_NNP ,_, May_NNP 2010_CD ._.
ELRA_NNP Resnik_NNP Philip_NNP ._.
-LRB-_-LRB- 1999_CD -RRB-_-RRB- ._.
Semantic_JJ similarity_NN in_IN a_DT taxonomy_NN :_: An_DT information-based_JJ measure_NN and_CC its_PRP$ application_NN to_TO problems_NNS of_IN ambiguity_NN in_IN natural_JJ language_NN ._.
J._NNP Artif_NNP ._.
Intell_NNP ._.
Res_NNS ._.
-LRB-_-LRB- JAIR_NNP -RRB-_-RRB- ,_, 11_CD ,_, 95-130_CD ._.
Šarić_NNP Frane_NNP ,_, Glavaš_NNP Goran_NNP ,_, Karan_NNP Mladen_NNP ,_, Šnajder_NNP Jan_NNP ,_, &_CC Bašić_NNP Bojana_NNP Dalbelo_NNP ._.
-LRB-_-LRB- 2012_CD -RRB-_-RRB- ._.
Takelab_NNP :_: Systems_NNP for_IN measuring_VBG semantic_JJ text_NN similarity_NN ._.
Paper_NNP presented_VBD at_IN the_DT Proceedings_NNP of_IN the_DT First_NNP Joint_NNP Conference_NNP on_IN Lexical_NNP and_CC Computational_NNP Semantics-Volume_NNP 1_CD :_: Proceedings_NNP of_IN the_DT main_JJ conference_NN and_CC the_DT shared_VBN task_NN ,_, and_CC Volume_NN 2_CD :_: Proceedings_NNP of_IN the_DT Sixth_NNP International_NNP Workshop_NNP on_IN Semantic_NNP Evaluation_NNP ._.
Ștefănescu_NNP Dan_NNP ,_, Banjade_NNP Rajendra_NNP ,_, &_CC Rus_NNP Vasile_NNP ._.
-LRB-_-LRB- 2014_CD -RRB-_-RRB- ._.
Latent_NN semantic_JJ analysis_NN models_NNS on_IN wikipedia_NN and_CC tasa_NN ._.
The_DT 9th_JJ Language_NNP Resources_NNPS and_CC Evaluation_NNP Conference_NNP -LRB-_-LRB- LREC_NNP 2014_CD -RRB-_-RRB- ._.
Sultan_NNP Md_NNP Arafat_NNP ,_, Bethard_NNP Steven_NNP ,_, &_CC Sumner_NNP Tamara_NNP ._.
-LRB-_-LRB- 2014a_JJ -RRB-_-RRB- ._.
Back_RB to_TO basics_NNS for_IN monolingual_JJ alignment_NN :_: Exploiting_VBG word_NN similarity_NN and_CC contextual_JJ evidence_NN ._.
Transactions_NNS of_IN the_DT Association_NNP for_IN Computational_NNP Linguistics_NNP ,_, 2_CD ,_, 219-230_CD ._.
Sultan_NNP Md_NNP Arafat_NNP ,_, Bethard_NNP Steven_NNP ,_, &_CC Sumner_NNP Tamara_NNP ._.
-LRB-_-LRB- 2014b_JJ -RRB-_-RRB- ._.
DLS@CU:_NNP Sentence_NNP Similarity_NNP from_IN Word_NNP Alignment_NNP ._.
Paper_NNP presented_VBD at_IN the_DT Proceedings_NNP of_IN the_DT 8th_JJ International_NNP Workshop_NNP on_IN Semantic_NNP Evaluation_NNP ,_, pages_NNS 241-246_CD ,_, Dublin_NNP ,_, Ireland_NNP Wu_NNP Zhibiao_NNP ,_, &_CC Palmer_NNP Martha_NNP ._.
-LRB-_-LRB- 1994_CD -RRB-_-RRB- ._.
Verbs_NNS semantics_NNS and_CC lexical_JJ selection_NN ._.
Paper_NNP presented_VBD at_IN the_DT Proceedings_NNP of_IN the_DT 32nd_JJ annual_JJ meeting_NN on_IN Association_NNP for_IN Computational_NNP Linguistics_NNP ._.
Pedregosa_NNP Fabian_NNP ,_, Varoquaux_NNP Gaël_NNP ,_, Alexandre_NNP ,_, Michel_NNP Vincent_NNP ,_, Thirion_NNP Bertrand_NNP ,_, Grisel_NNP Olivier_NNP ,_, ..._: Dubourg_NNP Vincent_NNP ._.
-LRB-_-LRB- 2011_CD -RRB-_-RRB- ._.
Scikit-learn_NN :_: Machine_NN learning_NN in_IN Python_NNP ._.
the_DT Journal_NNP of_IN machine_NN Learning_NNP research_NN ,_, 12_CD ,_, 2825-2830_CD ._.
Řehůřek_NNP Radim_NNP ,_, &_CC Sojka_NNP Petr_NNP ._.
-LRB-_-LRB- 2010_CD -RRB-_-RRB- ._.
Software_NNP framework_NN for_IN topic_NN modelling_VBG with_IN large_JJ corpora_NN ._.
Paper_NNP presented_VBD at_IN the_DT Proceedings_NNP of_IN the_DT LREC_NNP 2010_CD Gramfort_NNP
