Experiments on Sarcasm Detection on Twitter: A SurveyAbstractTwitter is the best known microblogging site where people use various hashtags to label their tweets. We have used a corpus contain- ing tweets with ‘#sarcasm’ hashtag for our experiments on sarcasm classification. We studied various lexical and pragmatic features in different combinations and then compared the accuracy and F-score of different machine learning algorithms which includes indroduc- tion and experimentation of two new features (POS tag patterns and POS n-grams). We also studied the effect of changing the ratio of sar- castic and non-sarcastic tweets in the data-set on accuracy and F-score. Using Support Vec- tor Machine and Logistic Regression we at- tained the highest accuracy of 0.877, whereas the highest F-score of 0.86 was obtained using Deep Belief Network.1 IntroductionIn recent years, sarcasm has been widely studied in social media (Gibbs, 1986; Gibbs and Colston, 2007; Kreuz and Glucksberg, 1989; Utsumi, 2000; Nigam and Hurst, 2006; Pang and Lee, 2008; Davi- dov et al., 2010b; Gonza ́lez-Iba ́nez et al., 2011) be- cause of its use in extraction of real sentiment the author is trying to convey. In certain circumstances, people use sentences which may have positve literal meaning but with negative emotions or vice-versa. According to Merriam-Webster dictionary, sarcasm is defined as “The use of words that mean the op- posite of what you really want to say, especially in order to insult someone, to show irritation, or to be funny”.Second AuthorAffiliation / Address line 1 Affiliation / Address line 2 Affiliation / Address line 3 email@domainStudying social media for extraction of intelli- gent information is a challenging task and one of the emerging topics for research. Twitter is a mi- croblogging site in which people share there senti- ments, ideas, opinion in a short sentence(s) of max- imum total length of 140 characters. Twitter cur- rently ranks as one of the leading social networks worldwide, with 288 million monthly active users1. These users often annotate tweets using hashtags like ‘#examsover’, ‘#party’, ‘#sarcasm’. Such hash- tags make possible the creation of various Twitter corpora. We have used such a corpus for training classifiers for sarcasm detection.Sarcasm detection can be seen as a part of the broader sentiment analysis process, which has many reported applications. It is used in various social media monitoring tasks including tracking customer reviews, prediction of poll results etc. Accuracy of such systems can be increased by incorporating sarcasm detection methods. We have developed a framework for sarcasm classification. In this pa- per, we studied various lexical and pragmatic fea- tures in 17 different combinations and then com- pared the accuracy and F-score of different machine learning algorithms. The work reported here, thus, is a survey of sarcasm detection methods, especially in the context of microblogging text. We have used four different algorithms, viz. Naive Bayes, Lin- ear Support Vector Machine (SVM), Logistic Re- gression (LogR) and Deep Belief Network (DBN). We present results for three sets of experiment, first consisting of 117842 regular (non-sarcastic) tweets1 http://www.statista.com/topics/737/ twitter/ 
and the second consisting of 59858 regular (non- sarcastic) tweets. In the third experiment we stud- ied the effect of changing the percentage of regular tweets in the data-set on accuracy and F-score.Our paper includes discussion on previous work in the area of sarcasm classification in Section 1.1. Section 2 describes corpus creation (Section 2.1), steps followed in extraction of features (Section 2.2) and the various classifiers used in this experiment (Section 2.3). In Section 3, we present our experi- mental setting (Section 3.1) and show the evaluation of our approach (Section 3.2). We conclude with a discussion and summary (Section 4) and with an outlook on possible future work (Section 5).1.1 Related WorkDavidov et al. (2010b) used both Twitter and Ama- zon product reviews data-set containing sarcastic sentences. Gonza ́lez-Iba ́nez et al. (2011) used tweets collected using sarcasm hashtags as gold la- bels. Filatova (2012) presented a detailed work on creation of annotated sarcasm corpus using Amazon product reviews. Liebrecht et al. (2013) extracted Dutch tweets using hashtag ‘sarcasme’. Reyes et al. (2013) collected a training corpus based on tweets that consist of ‘#irony’ hashtag in order to train their classifiers on different types of features.Sarcasm is a well studied phenomenon and re- searched in various branches including psychology and linguistics (Gibbs, 1986; Kreuz and Glucksberg, 1989; Utsumi, 2000; Gibbs and Colston, 2007). Var- ious experiments have been conducted studying dif- ferent lexical and syntactic features to recognize sar- casm in text (Nigam and Hurst, 2006; Pang and Lee, 2008; Davidov et al., 2010b; Rajadesingan et al., 2015). Kreuz and Caucci (2007) experimented on lexical features which includes punctuation symbols (e.g.,“!”), intensifiers and interjections (e.g.,“ahh” or “gosh”) in detection of sarcasm.Lukin and Walker (2013) test a new bootstrap- ping method to train the sarcasm and nastiness clas- sifiers in dialogue system to learn lexical N-gram cues associated with sarcasm (e.g., “thanks”, “oh yeah”, “then of course”, “I’am not sure”, etc.) and lexico-syntactic patterns. Reyes et al. (2013) trained the classifier based on tweets with a specific hashtag and also tried distinguishing it from tweets that con- tain hashtag other than ‘#irony’ like ‘#education’,‘#politics’ etc. Carvalho et al. (2009) explored var- ious oral or gestural clues in user comments repre- sented by punctuation symbols (such as emoticons) and other keyboard characters and found them to be more predictive of sarcasm in sentences. Davidov et al. (2010b) presented a semi-supervised learning framework that uses syntactic and pattern based fea- tures in sarcastic sentences on Amazon product re- views data-set. They used predefined list of posi- tive sentiment words which captures words typically found in reviews, for example ‘great’,‘excellent’, ‘exciting’ etc. Filatova (2012) presented a detailed work on creation and analysis of annotated sarcasm corpus using Amazon product reviews. Riloff et al. (2013) used contrast between a positive sentiment referring to a negative situation to identify sarcasm in tweets.Tepperman et al. (2006) worked on sarcasm de- tection in utterances using prosodic and spectral featres (e.g. average pitch, pitch slope etc.) as well as contextual features, e.g. laughter or re- sponse to questions as features. Cheang and Pell (2008; Cheang and Pell (2009) performed studies to identify acoustic features in sarcastic utterances by analyzing various speech features such as speech rate, mean amplitude, amplitude range etc. Research has been conducted to identify and use facial and vo- cal cues in speech, e.g. (Caucci and Kreuz, 2012; Rankin et al., 2009).Gonza ́lez-Iba ́nez et al. (2011) used lexical and pragmatic features for sarcasm detection in tweets collected using sarcasm hashtags as gold labels. They extracted positive and negative emotions from tweets and studied their correlation with sarcasm. Liebrecht et al. (2013) used n-gram features from 1 to 3-grams to build a classifier to detect sarcasm in Dutch tweets. They also made an interesting ob- servation from their most effective n-gram features that people tend to be more sarcastic towards par- ticular topics such as homework, weather, returning from vacation, school, public transport, the church, the dentist etc.Liebrecht et al. (2013) used Winnow classifica- tion method. Gonza ́lez-Iba ́nez et al. (2011) used two standard classifiers which are often used in senti- ment classification: Support Vector Machines with Sequential Minimal Optimization (SMO) and Lo- gistic Regression (LogR). Riloff et al. (2013) used
the LIBSVM (Chang and Lin, 2011) library to train Support Vector Machine classifiers. Davidov et al. (2010b) used k-nearest neighbors (kNN)-like strat- egy for training the classifier. Rajadesingan et al. (2015) used various baseline approches including contrast, hybrid approaches and random, majority and n-gram classifier. Buschmeier et al. (2014) com- pared results of various classifiers, including Linear SVM, Logistic Regression, Decision Tree, Random Forest and Naive Bayes.2 MethodolgoyDevelopment of the sarcasm classification frame- work described in this paper can be subdivided into three different sub-tasks. The first step is corpus cre- ation and the second step is identification and extrac- tion of essential features from the data-set. The last step is to feed the extracted features to various min- ing or machine learning algorithms, which involves learning and choosing the appropriate algorithm(s) that help to mine efficiently the datasets with sub- stantial accuracy.2.1 Corpus CreationWe thank Mathieu Cliche for providing us the dataset2 neccessary for development of our sarcasm detection framework. We also collected data us- ing the Twitter API. We also thank Rajadesingan et al. (2015) and Davide3 for providing us the data containing tweet ids which consist of “#sarcasm” hashtag. Tweets extracted online do contain URLs and username. We have total of 83,495 tweets con- taining sarcasm hashtag and 330692 regular tweets, which were reduced to 25278 and 117842 tweets re- spectively after normalization. We carried out the normalization process in order to eliminate tweets which tend to provide very little information, e.g. tweets with length less than three words. We briefly describe the normalization process as follows:1. Removing all the hashtags including “#sar- casm”2. Removing friends tag (@username) which is used to respond to a tweet posted by some other2 https://github.com/MathieuCliche/Sarcasm_ detector/blob/master/app/twitDB_regular.csv3Natural Language Engineering (NLE) Lab, Technical Uni- versity of Valencia, Spainuser.3. Removingallthekeywordscontainingprefixas “sarcas”. which in particular removes the key- words “sarcasm” and “sarcastic”.4. Removing URLs, e.g. http://abc.com which do not provide any relevant information in the development of our sarcasm detection framework.2.2 FeaturesFirst, we randomly mixed all4 the sarcastic tweets and extracted the features, and then in the first exper- iment, randomly mixed all5 the regular tweets after feature extraction with sarcastic tweet. In the sec- ond experiment, we randomly selected 59858 reg- ular tweets and extracted features. After extracting features form the tweets, we randomly split our data- set into two parts. We used 70 percent of the data-set for the training and used the remaining 30 percent for the testing. The features we extracted are listed below:1. N-gram: Each word or n-gram occurring in the tweet can be used as a binary feature. We have used both unigrams and bigrams as fea- ture. For this, the tweets were tokenized and passed through a stemmer. In one of the ex- perimental setups, we removed the stopwords and then used the n-grams feature and in an- other we extracted n-grams without removing stopwords.2. Hyperbole: This feature indicates occurrence of three positive or negative words as in (Gibbs and Colston, 2007). We have limited this to occurrence of two positive or negative words. In the first step we removed all the stopwords using the NLTK list, which contains 127 stop- words. Then we check for occurrence of two positive sentiment or two negative sentiment words (occurring together) and accordingly we marked hyperbole feature as 1 or 0.3. Readability (Rajadesingan et al., 2015): A syl- lable is a unit of organization for a sequence of425278 sarcastic tweets 5117842 regular tweets  
speech sounds. A syllable in general is made up of a nucleus (often a vowel) with optional ini- tial and final margins (generally consonants). For example, water is composed of two syl- lables (wa + ter). Words can be made up of one or more syllables. The number of sylla- bles in a word is a measure of its ‘complexity’ or ‘difficulty’. As sarcasm is widely acknowl- edged to be hard to read and understand, we adapt standardized readability tests to measure the degree of complexity and understandabil- ity of a tweet. We use as features: number of words, number of syllables, number of sylla- bles per word in the tweet and also readability index derived from the “Flesch-Kincaid Grade Level” formula (Flesch, 1948) (4 features). The formula is given as:value is set to true indicating the presence of the pattern.8. Contrasting Sentiment: SentiStrength6 esti- mates the strength of negative and positive sen- timent in texts (short length), even if the text is written in informal language. This feature is formed using positive and negative sentiment extracted from a sentence using SentiStrength.2.3 ClassifiersAfter preprocessing and extraction of features as de- scribed above, we selected four algorithms to train and build our framework. The classifiers we used are listed below:1. Naive Bayes2. Support Vector Machine 3. Logistic Regression4. Deep Belief Network3 Results3.1 Experimental SetupWe experimented on training the classifier using five different features sets and 12 composite7 features sets, counting to a total 17 sets of features. For Naive Bayes (NB), Logistic Regression (LogR) and Sup- port Vector Machine (SVM), we used the sklearn li- brary, and for Deep Belief Networks (DBN) we used the nolearn library. Both these library are freely available for Python.3.2 Evaluation and AnalysisWe report below the results obtained from three dif- ferent experimental setups.3.2.1 First ExperimentResults for the first experiment in which the num- ber of regular tweets were 117842 is listed in Table 2. In the first experiment using the NB classifier, we obtained highest accuracy of 0.822 using only a single feature, i.e., Hyperbole, but the F-score so obtained was 0. On an average, the accuracy lies be- tween 0.73 to 0.82 using any feature set trained over the NB classifier.6 http://sentistrength.wlv.ac.uk 7Using more than one feature together0.39+11.8−15.59 (1) total words   total sentences  total syllables   total words  4. Interjection (Buschmeier et al., 2014): The in- terjection feature indicates the occurrence of terms like “argh”, “hurrahooops” and “huh”, along with acronyms terms like “lol”. We com- piled a list of 124 interjections.5. Punctuation (Davidov et al., 2010a): It in- cludes the following generic features: (1) Num- ber of “!” characters in the sentence. (2) Num- ber of “?” in the sentence.6. POS n-gram: It is a binary feature similar to n-gram feature after removing stopwords and stemming, with an exception that word in the sentence is combined with its POS tag using ‘ ’. For example:“Oh how I love being ignored.”love NN : 1,ignor NN : 1,oh UH : 17. POS tag pattern: For this feature, we re- moved stopwords from the tweets and then used TextBlob library for POS tagging the sen- tence and counted the frequency of each pat- tern extracted. At the end of processing all the tweets, we selected all the patterns which have frequency greater than 30 and do not oc- cur in non-sarcastic tweets. These patterns are searched in a tweet and if present, the binary     
   Feature Set Code Feature Set  NG N-gram  PNG POS n-gram  RD Readability  PTP POS tag pattern  HB Hyperbole  NG-RD N-gram + Readability  NG-PNG N-gram + POS-Ngram  NGS-RD N-gram (with stopwords) + Readability  NG-PNG-HB N-gram + POS n-gram + Hyperbole  NG-PNG-RD N-gram + POS n-gram + Readability  NG-RD-CS N-gram + Readability + Contrasting Sentiment  NG-HB N-gram + Hyperbole  NG-RD-IJ N-gram + Readability + Interjection  NG-RD-PT N-gram + Readability + Punctuation  NG-RD-IJ-PT N-gram + Readability + Interjection + Punctuation  NG-PTP-RD-IJ-PT N-gram + POS tag pattern + Readability + Interjection + Punctuation  NG-PTP-RD-IJ-PT-CS N-gram + POS tag pattern + Readability + Interjection + Punctuation + Contrasting Sentiment                  Table 1: Feature Set Codes  Features  Naive BayesSVM  Logistic Regression Deep belief Network  Accuracy  F-scoreAccuracy  F-scoreAccuracy  F-scoreAccuracy  F-score NG 0.737  0.5290.873  0.57060.873  0.5670.8638  0.85 PNG 0.82  0.0110.82  00.823  00.8435  0.84 RD 0.822  0.0140.823  00.823  00.813  0.74 PTP 0.821  0.0120.226  0.2870.824  00.81877  0.74 HB 0.8217  00.825  00.825  00.823  0.74 NG-RD 0.742  0.5350.874  0.590.875  0.570.8125  0.74 NG-PNG 0.734  0.5270.874  0.5770.873  0.5670.866  0.86 NGS-RD 0.739  0.5330.874  0.550.875  0.5670.812  0.74 NG-PNG-HB 0.731  0.5280.876  0.5790.875  0.5710.867  0.86 NG-PNG-RD 0.734  0.5290.876  0.5710.876  0.5770.824  0.74 NG-RD-CS 0.749  0.5450.877  0.5660.874  0.5680.808  0.74 NG-HB 0.739  0.5340.876  0.5740.875  0.5680.871  0.86 NG-RD-IJ 0.747  0.5390.87  0.5980.875  0.5670.8244  0.75 NG-RD-PT 0.743  0.5380.877  0.5750.877  0.5710.8225  0.74 NG-RD-IJ-PT 0.746  0.5390.875  0.5710.875  0.5680.824  0.75 NG-PTP-RD-IJ-PT 0.747  0.5360.876  0.6110.877  0.5750.813  0.74 NG-PTP-RD-IJ-PT-CS 0.749  0.5390.874  0.5260.875  0.5630.824  0.74                   Table 2: Comparison of different classification methods using different feature sets when number of sarcastic tweets is 25,278 and regular tweets is 117842.The results are much better with LSVM classi- fier as compared to the NB classifier. Highest ac- curacy reported was 0.877 using features [N-gram, Readability, Punctuation] and F-score obtained was 0.575. Using features [N-gram, POS pattern, Read- ability, Interjection, Punctuation], we obtained an F- score of 0.611 and accuracy of 0.876.LogR results were comparable to LSVM. We ob- tained highest accuracy of 0.877 and F-score of 0.575 using features [N-gram, POS pattern, Read- ability, Interjection, Punctuation], whereas using features [N-gram, POS-N-gram, Readability] we obtained accuracy as 0.876 and F-score as 0.577.The last classifier that we tried was DBN, whichperformed better than NB. Highest accuracy ob- tained was 0.871 using features [N-gram, Hyper- bole] and F-score so obtained was 0.86. On average, the accuracy lies between 0.81 to 0.87 using any fea- ture set trained over DBN classifier.3.2.2 Second ExperimentIn the second experimental setup, we randomly selected 59858 regular (non-sarcastic) tweets. Re- sults for the second experiment are listed in Table 3. In the first experiment in this setup, using the NB classifier, we obtained highest accuracy of 0.774 using a single feature, i.e., POS n-gram, but the F- score so obtained was 0.635. On an average, accu-
  Features  Naive BayesSVM  Logistic Regression Deep belief Network  Accuracy  F-scoreAccuracy  F-scoreAccuracy  F-scoreAccuracy  F-score NG 0.754  0.670.817  0.6580.817  0.6560.811  0.81 PNG 0.774  0.6350.795  0.5910.795  0.5990.784  0.78 RD 0.681  0.2360.3  0.460.702  00.705  0.58 PTP 0.705  00.708  00.708  00.704  0.58 HB 0.705  00.71  00.712  00.699  0.57 NG-RD 0.762  0.6770.823  0.6680.822  0.6640.701  0.58 NG-PNG 0.754  0.6690.824  0.6720.825  0.6710.812  0.81 NGS-RD 0.756  0.6740.825  0.670.823  0.6610.705  0.58 NG-PNG-HB 0.752  0.6660.819  0.6680.82  0.6650.808  0.81 NG-PNG-RD 0.757  0.6730.824  0.6680.824  0.6720.809  0.81 NG-RD-CS 0.753  0.6690.824  0.6570.825  0.6680.7  0.58 NG-HB 0.752  0.6670.821  0.6650.822  0.6650.809  0.8 NG-RD-IJ 0.763  0.6780.825  0.6680.822  0.6660.703  0.58 NG-RD-PT 0.76  0.670.822  0.6670.823  0.6690.706  0.58 NG-RD-IJ-PT 0.755  0.6740.822  0.660.82  0.660.706  0.58 NG-PTP-RD-IJ-PT 0.756  0.6710.789  0.6790.82  0.6690.699  0.58 NG-PTP-RD-IJ-PT-CS 0.763  0.6750.822  0.6710.822  0.6660.705  0.58                   Table 3: Comparison of different classification methods using different feature sets when number of sarcastic tweets is 25,278 and regular tweets is 59,858.racy lies between 0.70 to 0.78 using any feature set trained over the NB classifier.The result on the LSVM classifier are again much better as compared to the NB classifier. Highest ac- curacy reported was 0.825 using features [N-gram8, Readability] and F-score obtained was 0.67. The same accuracy was obtained using [N-gram, Read- ability and Interjection] as features, whereas using features [N-gram, POS-pattern, Readability, Inter- jection, Punctuation], we obtained highest F-score of 0.679 and accuracy of 0.789.LogR results were also again comparable to LSVM. We obtained the highest accuracy of 0.825 and F-score of 0.671 using features [N-gram, POS n-gram].The results on the DBN classifier were better than NB. Highest accuracy obtained was 0.812 using fea- tures [N-gram, Hyperbole] and F-score so obtained was 0.81. On an average, accuracy lies between 0.68 to 0.82 using any feature set trained over DBN clas- sifier.3.2.3 Third ExperimentIn the third setup, we varied the percentage of reg- ular (non-sarcastic) tweets in the data-set from 46% to 82% and calculated accuracy and F-score at ev- ery interval of 2% increase in non-sarcastic tweets in the data-set. For this we selected four combina-8With stopwordstions of features from the features sets and used two classifiers, viz. NB and LSVM.As shown in Figure 1, we got initial accuracy of 0.745 and F-score of 0.773 using NB, whereas us- ing LSVM we got accuracy of 0.658 and F-score of 0.573 when using POS tag pattern, POS n-gram, Readability and Contrasting Sentiment as features. As we increased the percentage of regular tweets to 82% in the data-set, F-score dropped to 0.375 and accuracy increased to 0.826. But in the case of LSVM we see a zig-zag pattern both in case of F-score and accuracy. LSVM attains highest ac- curacy when percentage of regular tweets was in- creased to 82%, but the lowest F-score reported was 0.305 when percentage of regular tweets was 70%.For the Figure 2, we see that F-score followed the same pattern and decreased from 0.803 to 0.548 and 0.691 to 0.418 in case of NB and LSVM, respec- tively, when we used a combination of POS tag pat- tern, n-gram, Readability and Contrasting Sentiment as features. As we increased the percentage of regu- lar tweets to 82% in the data-set, accuracy dropped to 0.752 when NB was used and increased to 0.859 in the case of LSVM. F-score follows the same see zig-zag pattern in the case of LSVM.Again for the Figure 3, we see that F-score fol- lowed the same pattern as above and decreased from 0.803 to 0.538 and 0.784 to 0.556 in case of NB and LSVM, respectively when n-gram and Readability 
    Figure 1: Plot of accuracy and f-score obtained using combination of POS tag pattern, POS n-gram, Readabil- ity and Contrasting Sentiment as featureswere used as features. As we increased the percent- age of regular tweets to 82% in the data-set, accu- racy dropped to 0.742 when NB was used and in- creased to 0.874 in case of LSVM. In the case of LSVM, F-score and accuracy attains a local minima when percentage lies between 65-70%.In the fourth combination of feature set using Readability, N-gram and Contrasting Sentiment as features, we see that NB follows same pattern as above two cases and the value of accuracy and F- score gradually dropped as percentage of regular tweets increased as shown in Figure 4. Highest F- score reported was 0.804 and 0.795 in case of NB and LSVM, respectively, when percentage was 46%. In the case of LSVM, we again see several local min- ima both in the case of F-score and accuracy. LSVMFigure 2: Plot of accuracy and f-score obtained using POS tag pattern, n-gram, Readability and Contrasting Sentiment in the feature setattains highest accuracy of 0.874 and lowest F-score of 0.569 when percentage of regular tweets was in- creased to 82%.4 ConclusionWe experimented with the task of sarcasm classifica- tion using various lexical and pragmatic features us- ing the Twitter data. We tested 17 features using four different algorithms, using which we get accuracy ranging from 0.3 (using Readability feature in the case when regular tweets were 59,858) to 0.877. Lo- gistic Regression and SVM gave nearly the same re- sults, but using Deep belief Networks, we got high- est F-score of 0.86. We selected four combinations of features from the feature set and conclude that in- creasing the percentage of regular tweets increases
    Figure 3: Plot of accuracy and f-score obtained using n- gram and Readability in the feature setaccuracy but decreases F-score in case we use SVM but in the case of NB, accuracy also decreased with the F-score for the three combinations of features tested. In this paper, we tried two new features (POS n-gram and POS tag pattern), which performed sat- isfactorily in the case of SVM and LogR, but very good in the case of NB and DBN. We tried various other feature sets and reported the results on them for the four classifiers. The best results were ob- tained when we used all the features together. It can also be concluded that the two new features help in sarcasm detection.5 Future WorkWe look forward to conduct further experimenta- tions for refinement of the features by includingFigure 4: Plot of accuracy and f-score obtained using n- Gram, Readability and Contrasting sentiment in the fea- ture setacoustic and other lexical and pragmatic features. New features can be tried to further improve the efficiency of the classifier. We will try including amazon data-set and increase the number of sarcas- tic and regular tweets so as to enrich the dataset. The preprocessing step can be refined to remove re- peated characters from word like “happpppy”. We will try other algorithms like k-nearest neighbour (KNN) and various other algorithms based on neural networks.ReferencesKonstantin Buschmeier, Philipp Cimiano, and Roman Klinger. 2014. An impact analysis of features in a classification approach to irony detection in product
reviews. ACL 2014, page 42.Paula Carvalho, Lu ́ıs Sarmento, Ma ́rio J Silva, andEuge ́nio De Oliveira. 2009. Clues for detecting irony in user-generated contents: oh...!! it’s so easy;-). In Proceedings of the 1st international CIKM workshop on Topic-sentiment analysis for mass opinion, pages 53–56. ACM.Gina M Caucci and Roger J Kreuz. 2012. Social and paralinguistic cues to sarcasm. online 08/02/2012, 25:122, February.Henry S Cheang and Marc D Pell. 2008. The sound of sarcasm. Speech communication, 50(5):366–381.Henry S Cheang and Marc D Pell. 2009. Acoustic mark- ers of sarcasm in cantonese and english. The Jour- nal of the Acoustical Society of America, 126(3):1394– 1405.Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010a. Enhanced sentiment learning using twitter hashtags and smileys. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, pages 241–249. Association for Computational Lin- guistics.Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010b. Semi-supervised recognition of sarcastic sentences in twitter and amazon. In Proceedings of the Four- teenth Conference on Computational Natural Lan- guage Learning, pages 107–116. Association for Computational Linguistics.Elena Filatova. 2012. Irony and sarcasm: Corpus gen- eration and analysis using crowdsourcing. In LREC, pages 392–398.Rudolph Flesch. 1948. A new readability yardstick. Journal of applied psychology, 32(3):221.Raymond W Gibbs and Herbert L Colston. 2007. Irony in language and thought: A cognitive science reader. Psychology Press.Raymond W Gibbs. 1986. On the psycholinguistics of sarcasm. Journal of Experimental Psychology: Gen- eral, 115(1):3.Roberto Gonza ́lez-Iba ́nez, Smaranda Muresan, and Nina Wacholder. 2011. Identifying sarcasm in twitter: a closer look. In Proceedings of the 49th Annual Meet- ing of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume 2, pages 581–586. Association for Computational Lin- guistics.Roger J Kreuz and Gina M Caucci. 2007. Lexical in- fluences on the perception of sarcasm. In Proceedings of the Workshop on computational approaches to Fig- urative Language, pages 1–4. Association for Compu- tational Linguistics.Roger J Kreuz and Sam Glucksberg. 1989. How to be sarcastic: The echoic reminder theory of verbalirony. Journal of Experimental Psychology: General,118(4):374.Christine Liebrecht, Florian Kunneman, and AntalVan den Bosch. 2013. The perfect solution for de- tecting sarcasm in tweets #not. In Proceedings of the 4th Workshop on Computational Approaches to Sub- jectivity, Sentiment and Social Media Analysis, pages 29–37, Atlanta, Georgia, June. Association for Com- putational Linguistics.Stephanie Lukin and Marilyn Walker. 2013. Really? well. apparently bootstrapping improves the perfor- mance of sarcasm and nastiness classifiers for online dialogue. In Proceedings of the Workshop on Lan- guage Analysis in Social Media, pages 30–40.Kamal Nigam and Matthew Hurst. 2006. Towards a ro- bust metric of polarity. In Computing Attitude and Af- fect in Text: Theory and Applications, pages 265–279. Springer.Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and trends in infor- mation retrieval, 2(1-2):1–135.Ashwin Rajadesingan, Reza Zafarani, and Huan Liu. 2015. Sarcasm detection on twitter: A behavioral modeling approach. In Proceedings of the Eighth ACM International Conference on Web Search and Data Mining, WSDM ’15, pages 97–106, New York, NY, USA. ACM.Katherine P Rankin, Andrea Salazar, Maria Luisa Gorno- Tempini, Marc Sollberger, Stephen M Wilson, Dani- jela Pavlic, Christine M Stanley, Shenly Glenn, Michael W Weiner, and Bruce L Miller. 2009. De- tecting sarcasm from paralinguistic cues: anatomic and cognitive correlates in neurodegenerative disease. Neuroimage, 47(4):2005–2015.Antonio Reyes, Paolo Rosso, and Tony Veale. 2013. A multidimensional approach for detecting irony in twit- ter. Lang. Resour. Eval., 47(1):239–268, March.Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalindra De Silva, Nathan Gilbert, and Ruihong Huang. 2013. Sarcasm as contrast between a positive sentiment and negative situation. In Proceedings of the Conference on Emphirical Methods in Natural Language Process- ing(EMNLP 2013), pages 704–714.Joseph Tepperman, David R Traum, and Shrikanth Narayanan. 2006. ” yeah right”: sarcasm recogni- tion for spoken dialogue systems. In INTERSPEECH. Citeseer.Akira Utsumi. 2000. Verbal irony as implicit display of ironic environment: Distinguishing ironic utterances from nonirony. Journal of Pragmatics, 32(12):1777– 1806.