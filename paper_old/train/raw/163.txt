 1AbstractSentence similarity computing has been widely used in automatic summarization, question-answer (QA) system, machine translation (MT) and so on. This paper proposes a hybrid pragmatic sentence similarity computing method integrating morphologic features, syntactic features and semantic features. We construct a Chinese Paraphrase Corpus to do evaluation experiments of our proposed method and comparison with baseline systems. Experimental results show that our method can effectively improve the precision and recall of sentences similarity.Introductionbased models to improve the estimates of existing language models. Lv et al. (2003) and Li et al. (2005) offer a sentence similarity calculation method which consists of morphological similarity and word order similarity. Yang et al. (2012) offers a vector space model using TF-IDF features. These methods are usually based on surface morphological information of each word of sentences, only considers the characteristic of words. Liu et al. (2007) present a novel method to measure similarity between sentences by analyzing parts of speech and using Dynamic Time Warping (DTW) technique. However, sentence similarity calculation using information of similar vocabulary is different from semantics. It can’t be handled appropriately.The second category is developed by many researchers and it quantifies similarity relationships based on information in the manually crafted thesaurus, for instance, WordNet and HowNet, etc. Richardson et al. (1994) propose the use of WordNet as a knowledge base in an information retrieval task. The application areas range from information filtering and document retrieval to multimedia retrieval and data sharing in large scale distributed database systems. Li (2002) and Liu (2002) calculate similarity and correlation between words based on Chinese Thesaurus- Tongyici Cilin and HowNet, and then calculate the correlation between sentences. Kong et al. (2011) calculates the similarity between sentences based on HowNet. Though methods of this category compensates for the defects of similarity computing methods based on characteristics of words to a certain extent, it ignores the deep syntactic structure of a sentence. Simultaneously, the accuracy of similarity calculation depends on the completeness of the semantic dictionary. Therefore, this method has certain limitation.Moreover, a basic idea of sentence similarity is usually defined as a combination of semantic similarity and word order similarity or others.The third category is based on semantic dependency structure. Achananuparp et al. (2008) proposes a hybrid question similarity approach that combines semantic, syntactic and question typeA Hybrid Pragmatic Sentence Similarity Computing MethodSentence similarity computing is a core research point in natural language processing, which is a measure method of similarity between sentences. Sentence similarity measures play an increasingly important role in text related research and applications in areas such as text classification, information retrieval, machine translation, automatic summarization and automatic question answering system, etc. These applications show that the computation of sentence similarity has become a generic component for each research project involved in knowledge representation and discovery.The sentence similarity computing method generally consists of four levels: surface morphological similarity, syntactic similarity, semantic similarity and pragmatic similarity, in which the pragmatic similarity is the most important approach and it is quite difficult to realize at present. Meanwhile, the traditional methods can be roughly divided into three categories:The first category is based on the characteristics of words. In many Information Retrieval systems, the word co-occurrence methods are often used as the “bag of words” method. Dagan et al. (1999) proposes a general method for using similarity
 similarity. Semantic and syntactic information is measured by taking into account word similarity, word ordering and parts of speech information. Li et al. (2003) calculates sentence similarity by combining semantic information and syntactic dependency structure information. Mao (2013) adopts the method of semantic extension to calculate similarity between sentences. These previous methods can remedy the sentence structure defect of others methods based on surface morphological information and semantic dictionary.In recent years, many research results show that similarity computing of sentence integrating multi- features of a variety of characteristics of a sentence, which is better than that based on the calculation method of a single feature. Zhao et al. (2005) propose a method using multi-features of the weighted value of keywords feature, semantic distance and syntactic dependency structure pairs to calculate sentence similarity. Liu et al. (2008) propose an approach integrating an improved edit distance algorithm and dependency grammar. Wu et al. (2014) offer a method according to a combination of the Hierarchical Networks of Concepts (HNC) theory and dependency grammar. Xiao et al. (2014) combine neural network and combined semantics.Each singular type of the overall sentence similarity calculation methods still keeps some space to be improved.In this paper, we propose a hybrid pragmatic sentence similarity calculation method integrating the characteristics of morphological words, semantic thesaurus and syntactical dependency structural information. The morphological similarity calculation adopts keyword features with deleting stop noun list. The semantic similarity calculation is carried out using HowNet. The syntactic similarity calculation employs dependency structural features which is not only include the calculation of the similarity of effective collocation pairs of keywords, but also the similarity between non-keywords, as a result, it ensures the comprehensiveness and reliability of syntactic similarity calculation method. The experimental results show that our method can ensure the balance between recall and accuracy, and provide effective and stable results.The remainder of this paper is organized as follows: the proposed approach is introduced in Section 2. The experimental results and associateanalysis are given in Section 3. Finally, conclusions are drawn in Section 4.2 Proposed MethodIn this section, we introduce several approaches of the different level of sentence similarity between a pair of sentences with different features. These are intended as several samples of each methodology that might be considered for our proposed method.The outline of our proposed method is shown in Figure 1. We adopt morphologic features, syntactic features and semantic features to realize the processing of sentence similarity calculation. With the integration processing, sentence similarity calculation can be optimized using different machine learning approaches. In the following sections, we will give detailed report of each processing respectively. Morphologic FeaturesSyntactic Features Semantic FeaturesIntegration ProcessingSimilarity Output        2.1ThesaurusFigure 1. OutlineMorphologic Features based MethodMost of technical calculation of sentence similarity calculate a score S( S1 , S2 ) between a querysentence S and a candidate sentence S , intended 12to seize numerically the extent to which they convey the same information. The objective is to be able to calculate S( S 1 , S2 ) for each sentence ofa certain collection and know that when the score S is maximized, the sentence S2 has a high degree ofsimilarity to the query sentence S 1 .There are many measures of traditional sentence similarity calculation method using morphologic features. For instance, Word overlap measures, TF- IDF measures, Relative-frequency measures and N-gram model measures, etc. The morphologic
 features are usually adopted to do calculation using keyword of sentence, word order, part-of-speech (POS) or N-grams.In order to re-weight the count features, one of the simplistic approaches is very common to use the TF-IDF transform. TF means term-frequency while TF-IDF means term-frequency times inverse document-frequency.In this stage, we just adopt keyword and POS as features to calculate the morphologic sentence similarity. We delete each word of stop list, and then to use the vector space model (VSM) to calculate cosine similarity between two sentences. We will try to improve the algorithm with other methods in the future work.Here, given two sentencesS1 andS2 , remove stop words and get valid words of the twosentences to form vector spaceV  {X1,X2,...,Xn},in which repeated words are merged. Then, to calculate the frequency or probability of each word of the vector space. The function of morphologicsimilarity calculation is shown in Formula (1):semantic similarity calculation method based on the multilayer structure of HowNet and multiple modified relation of sememe. On the one hand, we refer to the method of Liu et al. (2012). And on the other hand, we combine with part of speech (POS) to enhance the accuracy of similarity calculation.Figure 1. Concept description of “tap”Liu et al. (2012)’s method considers not only the hyponymic relations of sememe, but the multiple modified relation of sememe. The method treats child nodes with the same description relationship that under the same node as the same collection.Based on the modality of the concept structure tree, shown in Figure 1, the similarity computation method of the i-th relationship of node a and node b is shown in (2): Sim (S,S)V V ii nn  (1)n word 1 21 2i1 2   Where , S 1V and V denote the vector space of each sentence. i1 i12 iiand S2 mean a pair of sentences,12The frequency of a word X i in sentenceexpressed as i , and i in sentence S2 . 2.2 Semantic Features based MethodS 1 isFrequently-used semantic dictionary incorporates HowNet, WordNet, Tongyici Cilin, etc. HowNet is a knowledge database that describes concepts of English and Chinese words. It reveals relationship between concepts and their attributes. WordNet is an English lexical semantic web that expresses relationship of synonyms. Tongyici Cilin is a Chinese Thesaurus of synonyms.In this stage, we use HowNet as the semantic dictionary. Among the existed methods, the method based on the structure of HowNet sememe tree to compute semantic similarity is extensively adopted. Liu et al. (2002) utilize the distance of sememe in the sememe structure tree to compute semantic similarity. Jiang et al. (2008) take into account the depth of sememe in the sememe structure tree. Liu et al. (2012) put forward a rela_i 1 2 s 1 2 11 NSim (S,S)2 N Sim (S,S)Sim(S,S,depth) (2)s,node 1 2   i1   Where, N means N different kinds of relationships in the sememe tree, depth is the setting detail level parameter,Sims (S1,S2,depth) denotes the similarity based on sememe structure and multiple modified relation.Similarity computation method is shown in (3): Sim (S , S , depth) s12 StructSim(S,S),if depth0 (3) 12 Sim(S,S,depth1) Modify(S,S),if depth0depth s12 modify 12Where, StructSim(S ,S ) represents the structure12similarity between sememe, Modify(S1,S2) denotes
 multiple modified relation similarity between sememe, and Modify(*)  Sims,node (S1, S2 ) .(5):words with multiple POS respectively, and calculate the max value of their similarity. w and v denote number of words with multiple POS in sentence S1 and S2 respectively. SimG _ E(*)representsthe POS similarity of two sentences. It means converting the word similarity to the property describe similarity of a certain part of speech of words.According to these methods, to calculate the similarity using HowNet concept multilayer structure and multiple modified relation of sememe of each word in the pair of sentences. And then detecting whether there are words contain more than one POS and computing similarity on the level of POS. The similarity computation method based on semantic dictionary is shown in (6):Sim (S,S )Sim (S,S )Sim (S,S )(6) Hownet 1 2 struct 1 2 G_E 1 2Where, Simstruct (S1,S2) represents sentencesimilarity based on HowNet concept multilayer structure and multiple modified relation of sememe.With the sememe similarity of the node, we can compute the concept similarity of the root node bottom-up, and then compute word similarity with (4):Sim(W,W) max Sim(C,C )(4) w 1 2 i1..n, j1..m c 1i 2 jWhere, C1i denotes the i-th concept of word W1 ,C2j denotes the j-th concept of word W2 , andSimc(*)represents computing the similarity of twoconcepts. With word similarity Sim (W ,W ) of all w 1i 2jwords in two sentences, we can obtain similarity Sim(S1,S2) of two sentences by weighted sum ofword similarity.According to the method of Liu et al. (2012), itdoes not take full advantage of part of speech (POS) in the HowNet, which may create ambiguity when selecting words with multiple part of speech. For instance, word "culture" has lots of part of speech in Chinese. Its sememe properties in HowNet is shown in Table 1.Table 1: Sememe property of word "culture" Consider the following two sentences in Chinese:S1 :中国是一个拥有多民族的国家。( S 1 :China is a multi-ethnic country.)S2 : 我们不能摒弃我国的传统文化。(S2 :We cannot abandon the traditional cultureof our country.)When computing the semantic similarity of “中国(China)” and “文化(culture)”, the similarity is 0.77 according to method of (Qun et al., 2002). It is due to the same property of {Avalue| 属 性 值 , Attachment|归属} and the neglect of part of speech in a specific context. This results in the deviation of the result from our common cognitive.In this stage, we adopt multiple POS as features aim to progress the performance of our method. We do sentence segmentation processing with word multiple POS, and then do syntax analysis tow,v1i 2jobtain sentence dependency structure with POS.The similarity computation formula is shown inSimG_E(S1,S2)Where, P1i and P2j mean the property of twoi1, j1maxSim(P,P )(5)   Part of SpeechPropertyadjn nAvalue|属性值,Attachment|归属, Literature|文Knowledge|知识Mental|精神denotes sentence similarity of sentence contains words with multiple part ofSimG_E (S1,S2)speech.  and  are parameters, and     1,which can be optimized by greedy algorithm. It is the sum of semantic similarity by Liu et al. (2012)'s method.2.3 Syntactic Features based MethodSentence similarity calculation of traditional dependency structure based method are usually computed by the similarity degree between effective match pairs of dependency structure from dependency trees. The so-called effective match pairs is composed of the core verb and words directly depends on it. Zhao et al. (2005) propose a method to calculate sentence similarity using weight value of effective match pairs. Liu et al. (2008) use edit distance as an auxiliary of semantic dependency. Wu et al. (2014) not only consider the effective match pairs, but use
 secondary notional words and function words as reference standard of semantic similarity.In this stage, we propose a semantic dependency similarity method that combine immediate constituent set of head and non-immediate constitute set. The immediate constituent set of head is a collection of core word (verb) in the dependency tree and its directly connected nouns, adjectives and adverbs. The non-immediate constituent set is a word set that aside from immediate constituent set of head. We employ the syntactic analysis tool of Stanford University (the Stanford Parser) to analyze the syntax of the sentence. For example, the dependency tree of Chinese sentence "教授举办一场学术报告(Professor holds an academic report.)" is shown in Figure 2.Figure 2: Dependency tree of example sentenceAs shown in Figure 2, the root node of the dependency tree is "举办 (holds)" the subjective of "教授 (Professor)" and the objective of "报告 (report)" are directly connected with the root node. It constitutes of the skeleton of the sentence. Although we can rely on the key words of a sentence to understand the general meaning of a sentence, the remaining words also affect the whole meaning more or less. Some sentences only consist of immediate constituent words, such as the Chinese sentence " 牛 吃 草 。 (Cows eat grass.)", which does not contain non-immediate constitute words. We consider two sets — keyword set and non-keyword set. In this paper, keyword set is represented by N and non-keyword set is represented by E. The similarity computation method based on semantic dependency is shown in (7):Simsemantic (S1,S2 )  mSim(N , N )  nSim(E , E ), if E  0 12 12 (7) Sim(N1,N2) , if E0Where, Simsemantic(S1,S2) represents semantic similarity of two sentences,Sim(N1,N2)denotes semantic similarity of keyword set andSim(E1,E2)denotes semantic similarity of non-keyword set. Parameters satisfy 0  n  m  1 and m  n  1.With regard to semantic similarity of keyword set, we refer to Wu et al. (2014)'s method which is shown in (8):1 pSim(N1,N2) Sim(h ,h )(8) p 1i2i i1p  count(H1  H2  H1  H2 )Where,species number of different grammatical functionsdenotes of keyword set in two sentences. h and hrepresent keywords with the same grammaticalfunction. On account of the number of keywords inthe two sentences do not always equal, whenh or h  setSim(h,h )(isavery 1i 2i 1i 2ismall positive number).For the similarity computation of keyword set,we adopt the idea similar to bipartite graph to calculate similarity of all non-keywords in the sentence. The similarity of non-keyword sets E1 and E2 are obtained from the average similarity of all the keywords. Computation method is shown in (9):Sim(a ,a ) 1i 2jSim(E,E ) i,j (9) 1 2 max(p,q)Where, a and a represent words in non- 1i 2jkeyword set E1 and E2 respectively. p and q are the number of words in E1 and E2 .2.4 Integration ProcessingAs mentioned above, we propose a hybrid pragmatic sentence similarity computing method. We use Linear interpolation to do integration processing to combine the sentence similarity of using morphologic features, syntactic features and semantic features. The formula of sentence similarity calculation is shown as:Sim(S,S ) 12aSim (S ,S )bSim (S ,S )cSim (S ,S )(10) word 1 2 Hownet 1 2 semantic 1 2Where, a, b and c are parameters and satisfy a+b+c=1 The greedy algorithm can be used to to optimize these parameters according to the preliminary experiments.For each similarity ofSim (S ,S ) , Sim (S ,S ) and word 1 2 Hownet 1 21i 2i  
 Simsemantic (S1 , S2 ) , it must be normalized ensure each value within the interval [0,1].tothe recall performance. Nevertheless, precision and recall maintain the relationship of constrained. Once guaranteed the precision, high recall rate can not be guaranteed. For this reason, the F -measure is introduced to reflect the comprehensive performance of precision and recall rate. Considering precision and recall rate are equally important, parameter  in the formula values 1.3.3 Results and AnalysisWe use greedy algorithm to optimize the three features in sentence similarity equation of multiple feature combination of the linear interpolation according preliminary experiments. We get the optimal value  0.13, 0.42, 0.45.We evaluate seven kinds of method, including traditional TF-IDF, traditional method based on HowNet, method based on semantic dependency information, method based on edit distance and dependency grammar( EDIT & DEPEND ) ,3 3.1Experiment and Analysis Data setIn the English sentence similarity computing, we often employ Microsoft Research Paraphrase Corpus (MSR) as the basic corpus. However, there is no standard one in Chinese sentence similarity computation. Consequently, manual annotation is needed to process corpus.We imitate the MSR mode building a small Chinese Paraphrase Corpus. Based on the background of question-answer(QA) system, we collect different types of interrogative sentences on Baidu Knows for the recent one month. We judge artificially whether the semantics of each sentence in the data set are equivalent. We stipulate that 1 indicates two sentences are similar, while 0 indicates not. The introduction of Chinese Paraphrase Corpus is shown in Table 2.Table 2:Introduction of Chinese Paraphrase Corpus3.2 EvaluationWe employ three evaluation index of sentence similarity: precision, recall and F-measure. The definition of the three evaluation index refered to Liu(2013) is demonstrated as follows:precision  (TP) (TP  FP)(11)recall(TP) (TPFN)(12) Fmeasure((1)precisionrecall) (precisionrecall)(13)Where, TP is the number of sentences that forecast as similar and actually similar. FP is the number of sentences that forecast as similar but actually not. FN is the number of sentences that forecast as dissimilar while actually similar. Precision is the ratio of sentence number of forecast as similar and the total sentence number. It evaluates the precision of algorithm. Recall is the ratio of sentence number of forecast as similar and sentence number of actually similar. It evaluatescompositive method based HowNet(HOWNET_C),method(HYBRID1) and our method. Compositive method based on HowNet means to replace the part of semantic similarity algorithm based on semantic dictionary of our method with the classic semantic similarity calculation method of Liu(2002), and then calculate the compositive sentence similarity. While, Hybrid1 method is the method of Zhao et al, (2005). It is a similar method to ours and we consider it as a baseline. The results are shown in Table 3.on Hybrid1 TypeData setTraini Test ng set setTotally Semantically Equivalent Semantically not Equivalent3000 192510752807 193 1503 422730 345 AlgorithmPrecision RecallF- measureTF-IDF HOWNET SEMANTIC EDIT& DEPEND Hybrid1 HOWNET_ C OUR METHOD65.23 80.21 70.10 90.76 71.25 87.3580.12 91.32 81.03 91.85 81.06 92.1583.02 93.5671.95 79.10 78.4885.35 86.10 86.2587.98   Table 3: Experiment ResultsIntuitively, for contrasting the effect of these algorithms, the histogram of precision, recall and F-measure of each method are shown in Figure 3.
  Figure 3: Algorithm effectiveness comparisonAs shown in Table 3 and Figure 3, We can find that TF  IDF method is not good because it just uses surface morphologic features, while ignores syntactic and semantic information. The precision and recall of HOWNET and SEMANTIC are better than TF  IDF method. It shows that semantic and structure features can effectively improve the performance.. The method ofEDIT & DEPEND provides better effectiveness, as it combines the edit distance and dependency grammar.Compared with baseline methods, It shows that our method can get the best accuracy. with precision and recall. The F-measure of our proposed method is 87.76%, It shows that our method can get the Feasibility.4 Conclusions and Future WorkIn this paper, we propose a hybrid pragmatic sentence similarity calculation method integrating the characteristics of morphological words, semantic thesaurus and syntactical dependency structural information.Moreover, This paper summarizes the methods of sentence similarity calculation of domestic and abroad. We sum up three kinds of traditional similarity calculation methods and analyze the limitations of the algorithm performance.We construct a Chinese Paraphrase Corpus to do experiments. The experimtental results show that our method can improve the performance of sentence similarity calculation. In our future work, we will try to challenge how to resolve the problem of domain adaptation. and the actual application in short text.ReferencesAchananuparp P, Hu X, Zhou X, et al. (2008). Utilizing sentence similarity and question type similarity to response to similar questions in knowledge-sharing community. Proceedings of QAWeb 2008 Workshop.Dagan I, Lee L, and Pereira FCN. (1999). Similarity- Based Models of Word Cooccurrence Probabilities. Machine Learning, 34(1-3):34.Fei, H. (2013). Chinese short sentence similarity calculation based on tree-structure corpus. Computer Applications & Software, 30(8), 18-8.Jiang, M., Xiao S., & Wang W. (2008). AnImproved Word Semantic Computation Based on HowNet. of Chinese information, 22(5) :84-89.Similarity JournalKong, S., & Wang, Y. (2011). Research of Text Topic Sentence Extraction Algorithm Based on Sentence Similarity Research. Journal of the China Society for Scientific and Technical Information, 30(6).Lewis W D. (2011). Measuring conceptual distance using WordNet: the design of a metric for measuring semantic similarity. Coyote Papers, 12.Li, B., Liu, T., & Qin, B. (2003). Chinese Sentence Similarity Calculation Based on Semantic Dependency. Application Research of Computers, (12):15-17.Li, M. Q., Li, J. Z., & Wang, Z. Y. (2005). Semantic Analysis and Structured Language Models. Journal of Software, 16(9): 1523-1524.Li, R., Wang, Z., Li, S., Liang, J., & Baker, C. (2013). Chinese sentence similarity computing based on frame semantic parsing. Journal of Computer Research & Development, 50(8), 1728-1736.Li, S. (2002). Research of relevancy between sentences based on semantic computation. Computer Engineering & Applications, 38(7), 75-78.Liu, B., Lin, H., & Zhao, J. (2008) Chinese Sentence Similarity Computing Based on Improved Edit- distance and Dependency Grammar. Computer Application and Software, 25(7): 33-34.Liu, H. (2013). Ontology Based Sentence Similarity Measurement. Computer Science, 40(1): 251-256.Liu, J., Xu, J., & Zhang, Y. (2012). Semantic similarity calculation method Based on hownet concept of multilayer structure and the original complex modified relation.Liu, Q., Li, S. (2002). Lexical Semantic Similarity Calculation Based on HowNet.
 Liu, X., Zhou Y. and Zheng. R. (2007). Sentence similarity based on dynamic time warping. In Semantic Computing, ICSC2007. International Conference on, pp. 250-256.Lv, X. Q., Ren, F. L., & Huang Z. D. (2003). Sentence Similarity Model and the Most Similar Sentence Search Algorithm. Journal of Northeastern University, 24(6):531-534.Mao, Y. (2013). Chinese Information Retrieval System Design and Implementation Based on the Semantic Extension. University of Electronic Science and Technology of China.Richardson, Ray, A. Smeaton, and John Murphy. Using WordNet as a knowledge base for measuring semantic similarity between words. (1994): 09.Rui, W., & Neumann, G. (2007). Recognizing Textual Entailment Using Sentence Similarity Based On Dependency Tree Skeletons. RTE '07 Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing(pp.36--41).Wu, Z., Wang, Y. (2014). New Measure of Sentences similarity Based on Hierarchical Network of Concepts Theory and Dependency Parsing. Computer Engineering and Applications, 50(3): 97- 102.Xiao, H., Fu, L., and Ji, D. (2014). Neutral Language Model and Semantic Compositionality Model in Semantic Similarity. Computer Engineering and Applications.Yang, Q. Q., & Dong, X. U. (2012). Study of english sentence similarity conputing in tf-idf method. Computer Knowledge & Technology.Yin, Y., & Zhang, D. (2014). Sentence similarity computing based on relation vector model. Computer Engineering & Applications.Zhao, Y., Qin, B., & Liu, T. (2005). Sentence Similarity Computing Based on Multi-Features Fusion