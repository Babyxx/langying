AbstractHow can we retrieve meaningful infor- mation from a large and sparse graph? Tra- ditional approaches focus on generic clus- tering techniques and discovering dense cumulus in a graph network, however, they tend to omit an interesting pattern such as the paradigmatic relations. In this paper, we propose a novel graph clustering tech- nique modelling the relations of a node us- ing the paradigmatic analysis. We exploit node‚Äôs relations to extract its existing sets of signifiers. The newly found clusters rep- resent a different view of a graph, which provides interesting insights into the struc- ture of a sparse graph network. Our pro- posed algorithm PaC (Paradigmatic Clus- tering) for clustering graphs uses paradig- matic analysis supported by a asymmetric similarity, in contrast to traditional graph clustering methods, our algorithm yields worthy results in tasks of word sense dis- ambiguation. In addition we propose a novel paradigmatic similarity measure. Ex- tensive experiments and empirical analysis evaluate our algorithm on synthetic and re- al data.IntroductionSecond AuthorAffiliation / Address line 1 Affiliation / Address line 2 Affiliation / Address line 3        email@domainrepresenting real groups, each cluster is character- ized by a larger density of inner links than outer links; the main problem is defining what a cluster is and how to identify it. Fortunato and Castellano (Santo et al., 2012) have identified three main cat- egories to define communities; Local definition, Global definitions and Vertex similarity. A local definition is when communities are grouped to- gether based on their vertices and their immediate neighbours, disregarding the rest of the graph. A global definition is when communities are grouped together based on the analysis of a community with respect to the graph as a whole. Lastly, vertex similarity is defined as: communities that are grouped together based on their vertices' similari- ties, where a quantitative criterion is chosen to evaluate the similarities between each pair of verti- ces. The main problem in this case is identifying adequate measures of similarity.Yang and Leskovec (2012) have identified 13 main measurements for clustering which are based on internal and external connectivity, a combina- tion of internal and external connectivity and net- work model. All algorithms analyse inner and out- er links in several ways and measure the degree of correctness of the different communities which are found.This paper is organized as follow; in the next section we will discuss the Paradigmatic approach for graph clustering. The PaC algorithm is present- ed in section 3. In section 4 a set of tests are pre- sented, from toy tests to real world tests and finally in section 6 we state the conclusions.Data from different domains, such as social net- works, bioinformatics, graph network and natural processing language can be modelled as graphs, and each graph contains communities or clustersParadigmatic Clustering
2 Graph Clustering Using Paradigmatic StructureWe propose another sub structural property be- tween any two vertices in a graph, a paradigmatic property. A sub structure in which vertices are equivalent if they are connected the same way or similarly, this being vertex similarity according to Fortunato and Castellano‚Äôs definition (Santo et al., 2012). We extended concept with an asymmetric similarity coefficient, uJaccard, section 2.2.Paradigmatic analysis seeks to identify the vari- ous paradigms (or existing sets of signifiers) which manifest the content of data. This aspect of struc- tural analysis involves the discovery of the exist- ence of 'underlying' similarity paradigms. Para- digmatic analysis involves comparing and con- trasting each of the signifiers that are present in the data. The commutation test can be used in order to identify distinctive signifiers and to define their significance; determining whether a change on the level of the signifier leads to a change on the level of the signified (Saussure et al., 1959). The para- digmatic approach is common in linguistics, social networks and others (Tejada, 2010).There are two kinds of equivalences, structurally equivalent where two vertices in a graph are struc- turally equivalent if they share many of the same network neighbours. Regular equivalence is more subtle, two regularly equivalent vertices do not necessarily share the same neighbours, but they do have neighbours who are themselves similar (Lor- rain et al., 1971; White et al., 1983).PaC focuses on paradigmatic vertices‚Äô similari- ties, vertices are similar if they are equally shared or have a similar number of edges connected to the same set of vertices directly or indirectly.In this paper we present a novel graph clustering algorithm based on a paradigmatic approach sup- ported by a bipartite graph (Fern et al., 2004) and enhanced by modularity. PaC (Paradigmatic Clus- tering) algorithm is fast and it isn‚Äôt complex, its time complexity is O(n avg(degree)2), it‚Äôs also suitable for large graphs, highly parallelized and incremental. PaC is enhanced with a graph‚Äôs modularity; the use of modularity is done from top to bottom, after an initial cluster has been con- structed.2.1 Paradigmatic StructuresParadigmatic analysis is a process that identifies entities which are not related directly but are relat- ed by their properties, the relatedness among other entities and whether they are interchangeable (Saussure et al., 1959). In language the reason why we tend to use morphologically unrelated forms in comparative oppositions is to emphasize the se- mantics which is done by substitution and transpo- sition of words with a similar signifier. Similarity is not defined by a syntactic set of rules but rather by the use of the language. In some cases this use is not grammatically or syntactically correct but it is commonly used.We defined the signifier as the similarity among vertices of the same cluster, not all members of a cluster have the same similarity of relatedness. This is due to the fact that a member of a cluster might belong to more than one cluster with differ- ent similarities.2.2 ParadigmaticSimilaritySome vertices are paradigmatically exchangea- ble if they rely on the same set of edges and their sub structures are equivalent to a certain degree. For example in some contexts you could use the word family to refer to your friends and vice versa, the words are not related syntactically or semanti- cally but in some context are common to inter- change the words to extend the meaning. But the similarity from word family to word friend is dif- ferent to the similarity from word friend to word family, this is because the usage that people gives to the words. We address the similarity in section 2.2. Words family and friend do not need to be linked directly by an edge, they might be linked among other vertex or vertices, be able to identify which two words are used interchangeable is the focus of the paradigmatic approach.PaC will use a structural equivalence to cluster vertices. Particularly the Unilateral Jaccard Simi- larity (Santisteban et al., 2015) define by:|Va ‚à© Vc| uJaccard(Va, Vc) = . (1)|eùíπgeùìà(Va)| |Va ‚à© Vc| uJaccard(Vc,Va)=|eùíπgeùìà(Vc)| . (2) 
2.3 AlgorithmIn this section we will present a novel algorithm for clustering graphs, and a Paradigmatic Cluster- ing algorithm.We propose to use a bipartite graph, to achieve fast and easy paradigmatic analysis. Our focus is to only compare rows that have at least one column in common, which reduces the space of comparison. Our approach will anchor a row to its closest and most similar row, if we want to extend the cluster- ing level we could anchor a row to more than one row based on a degree of similarity. PaC algorithm can process rows or columns seamlessly, resulting in a bi-clustering algorithm. PaC yielded better results in a sparse matrix due to the low level of connectivity. By reason of its non-dependency it can be parallelized.The input graph has to be bipartite graph, if the graph is not a bipartite; we transform it on the flight, this is simple task as we start working with a matrix in which column and row will form the two sets of a bipartite graph, knowing that a word might be in both sets we will label as set col-word- xy and row-word-xy. PaC can be applied to the set columns or set rows, getting a binary clustering.Algorithm 1 : PaC algorithm// The graph needs to be bipartite// The graph needs to be an undirected graph 1:procedure PaC (graph, threshold)2: for each vertex in the graph3: similaritiesPairs[ ] = uJaccard (Vi,Vn,)4: // uJaccard two levels out, all possible paths 5: // among Vi and Vn6: sort similaritiesPairs[ ]7: group vertices with similar structural8: equivalence & uJaccard score.9: grouping up to a threshold10: clusters [ ] = groups found11: return clusters[ ]// the grouping is done by selecting the elements // up to a particular thresholdPaC will use modularity to optimize the found clusters; the use of modularity is not intensive since it is a top to bottom approach. PaC will re- calculate the threshold used to identify the cluster cut; in each iteration the modularity can be im- proved providing a better clustering. The initial cluster found might be or not of interest for the application, but for general purpose and compari-son we try to optimize the clustering. The cluster will depend on how similar the clusters need to be, modularity will help to find the optimal.Algorithm 2 : Clustering// we start with modularity of 0// we start with a threshold = 0.5// new modularity is nQ = 0// last modularity is lQ = -1// factor will be the step of reduction = 0.011:procedure clus (graph, clusters, threshold)  2:3:4:5:6:7:// threshold is a map <cluster id, threshold > // some clusters will be improvedwhile nQ > lQ lQ = nQthreshold[ ] = threshold - factor cluster[ ] = PaC(graph, threshold) nQ = modularity (graph, clusters)return cluster[ ]// based on its assigned threshold The uJaccard function has been presented at 2.4, and graph modularity introduced by Newman- Girvan (Newman et al., 2004) is given by:ùëòùëÑ = ÔøΩ(ùëíùëñùëñ ‚àí ùëéùëñ2) (3) ùëñ=1  Where eii = % edges in module i (probability edge is in module i), ai = % edges with at least 1 end in module i (probability a random edge would fall into module i).The first run produces a set of clusters, but are those clusters the optimal cut? To measure if the produced clusters are the optimal cut we use the graph modularity presented by Newman and Gir- van (Newman et al., 2004), modularity will be run after PaC, to find out if it has been improved, if not, we stop otherwise we run PaC again. Since the approach is top-down, the calculation of modulari- ty is not intensive like other algorithms; rather it is used a few times to optimize PaC.The time complexity of our algorithm PaC is O(n avg(degree)2). We can point out that PaC al- gorithm is highly parallelized because its similarity calculation does not depend on a previous calcula- tion. Lastly it is incremental since we can add addi- tional vertices and calculate only the vertices in- volved and not the whole graph. 
3 Experimental EvaluationWe evaluated the algorithm on toy graphs, synthet- ic data sets, NLP dataset and real data sets. To prove its efficiency the PaC is compared with the normalized mutual information bench mark in syn- thetic graphs (Danon et al., 2005).3.1 Toy TestingThe use of PaC in toy graphs, as show in Table 2, it is clear that the desired cluster from those graphs are obtained by PaC, moreover the state of the art algorithms also find the same clusters. We com- pare PaC against 6 clustering techniques present in iGgraph (Csardi et al., 2006), Walktrap Communi- ty, Spinglass Community, Leading Eigenvector Community, Fastgreedy Community, Label Propa- gation Community, and Multilevel Community.Table 1: toy graphs clustered using PaCThe toy graphs in table 2 are just simple exam- ples to test PaC in particular cases, hence PaC is able to identify cluster properly, in particularly in some special cases like the third toy graph in which cluster 4,5,6 has been identify correctly, also last toy graph in which 2 cluster are identify as expected.3.2 Real Data SetsZachary's karate club: The network represents the pattern of friendships among members of a karate club at a North American University (Newman, 2003). Football: A network of American football games. (Girvan et al., 2001). Dolphins: A dolphin social network, New Zealand (Lusseau, 2003). Les Miserables: The network of interactions between major characters in Victor Hugo‚Äôs sprawling novel of crime and redemption (Knuth, 1993).To measure our findings with a ground-truth graph, we have used the Normalized Mutual In- formation - NMI (Fano, 1961). A measure of simi- larity of partitions borrowed from information the- ory, which has proven to be reliable (Danon et al., 2005). Used by Fortunato Santo (Santo, 2010) to compare two graphs and get a score of the differ- ences between two graphs. NMI of 1 shows equal graphs, 0 shows completely different graphs. Our findings are shown in table 3, column PaC, against the baseline column C and the similarity measure in column NMI, we have demonstrated that PaC provides good results.Table 2. Results on real world datasets produced by PaC. From left to right, the number of vertices N, edges E, Normalized Mutual Information NMI, actual number of communities C (base line) and PAC number of communities.PaC is able to identify almost correctly clusters in the Football graph with 0.9 NMI result, similarly with Les Miserable graph with 0.878, but no so well with Dolphins graph where PaC identifiby 5 cluster out of 2; on the Zachary graph PaC has identify 4 out 5, but this is a particular case since the original clustering is 2 but further paper refer up to 4 cluster with some particular vertices that might belong to different cluster.3.3 Synthetic Data SetsWe have tested PaC, based on a benchmark graph with a built-in community structure. Standard benchmarks, such as the Girvan and Newman, do not account for important features of real networks,       (1,2,3,4) (5,6,7)(1,2,3) (4,8,9,10) (5,6,7)         N     E      C     NMI  PaC      Zachary‚Äôs karate club 34 78  2 0.6524  Dolphins  62   159    2   0.527 5    Football   115     613      12     0.907  15      Les Miserables   77   254     11   0.878 12           (1,2,3) (4,5,6) (7,8,9,10)(1,2,3,4,5,6) (7,8,9) (10,11,12,13)            (0,1,2,3,4,5,6) (7,8,9,10,11,12) (13,14,15,16) (17,18,19,20)(1,2,3,4,5,6) (7,8,9,10,11,12,13,14)         
like the fat-tailed distributions of vertex degree and community size (Lancichinetti et al., 2008). There- fore, we have used a different class of benchmark graphs, in which the distributions of vertex degree and community size are both power laws, with tuneable exponents.To compare the communities found by PaC against a ground-truth network, we use the Nor- malized Mutual Information (Santo, 2010). For our testing we followed the settings used by Lancichi- netti and Fortunato (Lancichinetti et al., 2009).To validate the performance of PaC, we com- pare it against 6 clustering techniques present in iGgraph (Csardi et al., 2006), Walktrap Communi- ty (wc), Spinglass Community (sc), Leading Ei- genvector Community (lec), Fastgreedy Communi- ty (fc), Label Propagation Community (lpc), Multi- level Community (mc).PaC out performs mc, fc, lec, sc but this is not the same case for wc and lpc. The wc has a quad- ratic running time O(n 2 log n) (Pons et al., 2005) which is not appropriate for very large graphs in comparison, our PaC running time is O(n avg(degree)2). The lpc has a similar running time as PaC. The differences in performance are due to the fact that each algorithm uses different similari- ty measures while lpc use a symmetric measure and PaC use an asymmetric coefficient, hence we cannot compare them only based on the outcomes, and we have to consider the type of similarity they use. PaC uses a paradigmatic approach focusing in NLP .The mixing parameter in figure 3,4 and 5 is the degree of a cluster is mix with other clusters, 0 means cluster completed separated, where 0.5 means that 50% of the edges in a cluster are linked to other clusters.Figure 1: Comparison of PaC with wc, sc, lec, fc, lpc, mc on synthetic data set. The number of verti- ces is N = 5000, each point corresponds to 100 graph realizations. PaC clearly outperform sc, lec, fc.Figure 2: Comparison of PaC with wc, sc, lec, fc, lpc, mc on synthetic data set. The number of verti- ces is N = 1000, each point corresponds to 100 graph realizations. PaC clearly outperform sc, lec, fc, mc.Figure 3: Comparison of PaC with wc, sc, lec, fc, lpc, mc on synthetic data set. The number of verti- ces is N = 5000, each point corresponds to 100 graph realizations. PaC clearly outperforms sc, lec, fc, mc.3.4 NLPTestingOne of the challenging tasks in NLP is word sense disambiguation, making this more complex if we use a non-supervised technique. We use PaC to solve this problem and we use an empirical analy- sis to assess our results. PaC will need a graph of words.   
To build a graph of words we source it from three corpuses, Google n-gram corpus (Franz et al., 2006), The British National Corpus, version 3, BNC XML Edition (The British National Corpus, 2007) and en.wikipedia database backup dumps, each corpus will form an independent set of test- ing.Table 3: Size of the used corpusTo construct a graph of words we start by con- structing a bi-grams model, it was straight forward for Google n-gram as it was in fact a bi-gram da- taset. We cleaned up the text on the en.wikipedia and the BNC corpus, and then we run a sentence segmentation script against text in order to extract sentences. Once the sentences were identified we parse it to discover POS (part of speech), we used the BLLIP parser at T50 (McClosky et al., 2006). Once we have the parsed sentences we remove stop words (first 1000 more common words) and we apply a lemmatization process to each remain- ing word by doing a look up on WordNet (Fell- baum, 2005); with a parsed and lemmatized sen- tence and its POS we start building bi-grams of the following forms, verb-verb, noun-noun, verb-noun, noun-verb, adverb-verb, verb-adverb, adjective- noun, noun-adjective. We use a window of size n to the left and/or right of a the working word to build the bi-grams; after a few experiments we use n = 3. This being an extension work of Justeson and Katz (Justeson et al., 1995) and Patrick Pantel (Patrick, 2003).We select two collocation measures from the state of the art, Log-likelihood Ratios LLR (Dun- ning, 1993). LLR has been selected because it is a good approach to sparse data and it is able to iden- tify rare bi-grams with a low frequency.The second collocation measure we selected is the Pointwise Mutual Information (PMI) between two events. In information theory, mutual infor- mation refers to the mutual information between two random variables rather than between two re- lated events. It is a bad measure of dependence because it does not dependent on the frequency. PMI is bad with sparse data, some words that only occur once, but appear together, score very high PMI. PMI is also bad with word dependence, whentwo words are perfectly dependent of each other, whenever one occurs; the other occurs, so the rarer the word is, the higher the PMI it gets. Bouma (Bouma, 2009) shows several normalized variants of PMI in order to make PMI less sensitive to a low frequency. We have selected the Normalized Pointwise Mutual Information to the power of cube (NPMI3) as it reduces the score for rare words.We calculate LLR and NMPI3 for each bi-gram, given a word we align the result from LLR and NMPI3 forming a feature vector of the given word, we use the top 1/3 of the characteristic vector as its collocation ratio is very low and is meaningless on the other 2/3.We build a few graphs of word, one for each case (verb-verb, noun-noun, verb-noun, noun-verb, adverb-verb, verb-adverb, adjective-noun, noun- adjective) for BNC, en.wikipedia and Google n- gram.Figure 4: word ‚Äúchair‚Äù extracted from en.wikipedia (noun-noun), it shows the largest cluster.For word sense disambiguation we do not clus- ter the whole graph, instead we get a subset, the subset which is related to a given word. The sub graph is constructed as follows, from a given word we get it‚Äôs feature vector and load it to the graph, then per each word in the feature vector we load it ÃÅs feature vector to the graph. In a similar way to Fern and Brodley approach (Fern et al., 2004). Then we cluster the sub graph using PaC to obtain clusters of senses of a given word. Corpus   Size      bi-grams   Words     Vocabulary     Google 2- gram 1.5GB  314,843,401 629,298,853 5,827,772 BNC   539MB      842,162,318   98,537,224     354,374     Wikipedia  11GB       1,459,026,075   3,990,248     
Figure 5: word ‚Äúchair‚Äù extracted from BNC (noun- noun), it shows the largest clusters.Figure 6: word ‚Äúchair‚Äù extracted from en.wikipedia (noun-verb) it shows largest cluster.PaC produces clusters that represent word‚Äôs senses, as shown in figures 6 to 8, the uJaccard similarity shows how well related the words are. Edge thickness denoted the collocation measure. We notice that each corpus shows different cluster as their contexts are different, as in the case of chair. The results are worthy to consider. Compar- ing with a base line such as WordNet or other re- pository will be cumbersome and not productive because each corpus is completely different and from a different context (Agirre et al., 2004).Table 4: Senses/cluster for word apple, producedby PaC using en.wikipedia. The cluster related to the fruit is presented and other clusters are grouped by its senses such as music.macintosh , os, macos, mac, desktop      notebook , laptop, asus, baterry  wireless, usb, memory, remote      release , announce, calendar, make  store, product, user    imac , emac, intel, macbook      cranberry , pectin, cinnamon  juice, cider, martini      software, final, pro, power, hardware, advisor, video, quick- time  music, cinema, aria, garage band, book, deal, vacation, dvd, lossless, itunes, ipod, insight, ipods, archos    valley, spring, orchard, avery, tree, blossom, newton, loop, airport    support, core, developer, weblog, inspiration, iie, iigs    powerbook, ibook, powermac, xserve     punish, forgive, almighty, forbid, instruct, spake, ordain      play, free, man, smile, soldier, live, tv  incarnate, bless, forsaken, sake, fear, manifest      told, knew, father, hear  send, save, word, rulesnet      damned, damn, awful, ente  mode, speed, command, channel      brought, bring, giveth, meant, reveal, bring, make, awfuls, gave  desire, set, choose, expect, require, dwell, change, raise, in- tend, accord    government, church, grant, design, exist, heal, lead       Figure 7: word ‚Äúsail‚Äù extracted from BNC (noun- verb) shows largest clusters.Table 5: Senses for the word god, produced by PaC using Google bi-grams; it is clear that each cluster represents different aspects related to god.PaC produces clusters that represent word- senses, as shown in Table 5 and 6, the uJaccard similarity shows how well related the words are. Google bi-grams corpus does not have any syntac- tic information or POS, we just use its frequency to get words collocation and apply PaC. It is interest- ing to observe that even with lack of information we get very meaningful word-senses.3.5 ConclusionWe have shown how community detection can be approached using a paradigmatic analysis, in which the main idea is to find how similar two non-adjacent vertices are. Paradigmatic analysis is the process that identifies entities which are not related directly, but are similar by their properties and their relatedness among other entities, similar Figure 8: word ‚Äúgod‚Äù extracted from BNC( noun- noun) shows largest cluster.
to the structural equivalence. We introduce a new graph property, a paradigmatic property defined as the set of vertices that are similar if they have a structural equivalence and are interchangeable by substitution and transposition.We also present a novel clustering algorithm PaC, which finds clusters with similar structural equivalence. The structural equivalence is found by using uJaccard. PaC is highly parallelized, in- cremental and it‚Äôs running time is O(n avg(degree)2). Results show that PaC is able to find word-senses, as well as clusters on a ground- truth network.AcknowledgmentTO DOReferencesAgirre, E., Alfonseca, E., & De Lacalle, O. L.: Approx- imating hierarchy-based similarity for WordNet nom- inal synsets using topic signatures.: In Proceedings of GWC-04, 2nd global WordNet conference, pp. 15- 22, 2004.Biggs, Norman.: Algebraic graph theory: Cambridge university press, 1993.Bouma, G.: Normalized (pointwise) mutual information in collocation extraction.: Pro-ceedings of GSCL, 31- 40, 2009.Clauset, Aaron, Newman, Mark EJ, & Moore, Cristo- pher.: Finding community structure in very large networks.: Physical review E, 70(6), 066111, 2004.Csardi, Gabor, & Nepusz, Tamas.: The igraph software package for complex network re-search.: InterJour- nal, Complex Systems, 1695-5, 2006.Danon, Leon, Diaz-Guilera, Albert, Duch, Jordi, & Arenas, Alex.: Comparing community structure iden- tification.: Journal of Statistical Mechanics: Theory and Experiment, 2005.Dunning, Ted.: Accurate methods for the statistics of surprise and coincidence.: Computa-tional linguis- tics, 19(1), 61-74, 1993.Fano, R M.: "chapter 2". Transmission of Information: A Statistical Theory of Communi-cations.: MIT Press, Cambridge, MA. ISBN 978-0262561693, 1961.Fellbaum, Christiane.: WordNet.: Encyclopedia of Lan- guage and Linguistics, Second Edition, Oxford: Elsevier, 665-670, 2005.Feng, Jing, He, Xiao, Hubig, Nina, Bohm, Christian, & Plant, Claudia. Compression-based Graph Mining Exploiting Structure Primitives. Paper presented at the Data Mining (ICDM), 2013 IEEE 13th Interna- tional Conference, 2013.Fern, Xiaoli Zhang, & Brodley, Carla E. Solving cluster ensemble problems by bipartite graph partitioning. Paper presented at the Proceedings of the twenty-first international conference on Machine learning, 2004.Fortunato, Santo, & Castellano, Claudio. Community structure in graphs Computational Complexity (pp. 490-512): Springer, 2012.Fortunato, Santo. Community detection in graphs. Phys- ics Reports, 486(3), 75-174, 2010.Franz, Alex, & Brants, Thorsten. All our N-gram are Belong to You. Google Machine Translation Team, 20, 2006.Geva, Guy, & Sharan, Roded. Identification of protein complexes from co-immunoprecipitation data. Bioin- formatics, 27(1), 111-117, 2011.Girvan, Michelle, & Newman, Mark EJ. Community structure in social and biological networks. arXiv preprint cond-mat/0112110, 2001.Justeson, J.S. and Katz, S.M. Technical terminology: some linguistic properties and an al-gorithm for iden- tification in text. In Natural Language Engineering, 1:9-27, 1995.Knuth, Donald Ervin. The Stanford GraphBase a plat- form for combinatorial computing (Vol. 37): Addi- son-Wesley Reading, 1993.Lancichinetti, Andrea, & Fortunato, Santo. Community detection algorithms: a comparative analysis. Physi- cal review E, 80(5), 056117, 2009.Lancichinetti, Andrea, Fortunato, Santo, & Radicchi, Filippo. Benchmark graphs for testing community detection algorithms. Physical review E, 78(4), 046110, 2008.Lorrain, Francois, & White, Harrison C. Structural equivalence of individuals in social networks. The Journal of mathematical sociology, 1(1), 49-80, 1971.Lusseau, David. The emergent properties of a dolphin social network. Proceedings of the Royal Society of London. Series B: Biological Sciences, 270(Suppl 2), S186-S188, 2003.
Malliaros, Fragkiskos D, & Vazirgiannis, Michalis. Clustering and community detection in directed net- works. A survey. Physics Reports, 533(4), 95-142, 2013.McClosky, D., Charniak, E., & Johnson, M. Reranking and self-training for parser adapta-tion. In Proceed- ings of the 21st International Conference on Compu- tational Linguistics. Association for Computational Linguistics, 2006.Newman, Mark EJ, & Girvan, Michelle. Finding and evaluating community structure in networks. Physi- cal review E, 69(2), 026113, 2004.Newman, Mark EJ. The structure and function of com- plex networks. SIAM review, 45(2), 167-256, 2003.Newman, Mark. Networks: an introduction. Oxford University Press, 2010.Patrick Pantel.: Clustering by Committee. Ph.D. Disser- tation. Department of Computing Science, University of Alberta, Canada, 2003.Pons, P., & Latapy, M. Computing communities in large networks using random walks. In Computer and In- formation Sciences-ISCIS 2005 (pp. 284-293). Springer Berlin Heidel-berg, 2005.Saussure, Ferdinand de. Course in general linguistics. ed. C. Bally and A. Sechehaye. Translated by Wade Baskin. New York: Philosophical Library, 1959.Strogatz, Steven H. Exploring complex networks. Natu- re, 410(6825), 268-276, 2001.Tejado Carcamo, Javier. Construccion Automatica De Un Modelo De Espacio De Palabras Mediante Rela- ciones Sintagmaticas Y Paradigmaticas. PhD disser- tation, Mexico, 2010.The British National Corpus, version 3. BNC XML. Edition. Distributed by Oxford Uni-versity Compu- ting Services on behalf of the BNC Consortium, 2007.White, Douglas R, & Reitz, Karl P. Graph and semi- group homomorphisms on networks of relations. So- cial Networks, 5(2), 193-234, 1983.Yang, Jaewon, & Leskovec, Jure. Defining and evaluat- ing network communities based on ground-truth. Pa- per presented at the Proceedings of the ACM SIGKDD Workshop on Mining Data Semantics, 2012.Zachary, Wayne W. An information flow model for conflict and fission in small groups. Journal of an- thropological research, 452-473, 1977.Santisteban, Julio. Unilateral Jaccard Similarity Coeffi- cient. SIGIR Workshop on Graph Search and Be-yond ‚Äô15 Santiago, Chile Published on CEUR-WS, 2015