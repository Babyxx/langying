Discourse Relation Recognition by Comparing Various Units of Sentence Expression with Recursive Neural NetworkAbstractWe propose a method for implicit discourse relation recognition using a recursive neural network (RNN). Many previous studies have used the word-pair feature to compare the meaning of two sentences for implicit dis- course relation recognition. Our proposed method differs in that we use various-sized sentence expression units and compare the meaning of the expressions between two sen- tences by converting the expressions into vec- tors using the RNN. Experiments showed that our method significantly improves the accu- racy of identifying implicit discourse relations compared with the word-pair method.1 IntroductionDiscourse relation recognition is a technique to iden- tify the type of discourse relation between two sen- tences. Because discourse relation contributes to the coherence of sentences, it has potential applications in many natural language processing (NLP) tasks. For example, in text summarization, it makes sum- mary documents more consistent by using discourse relations and structures (Gerani et al., 2014). Simi- larly, in conversational systems (Higashinaka et al., 2014), discourse relations can help the system select contextually appropriate system utterances.Discourse relations are categorized into explicit and implicit relations. Explicit relations have a dis- course marker such as a connective, making them easy to identify with a high degree of accuracy (Pitler and Nenkova, 2009). Implicit discourse rela- tions, in contrast, have no discourse marker betweensentences. Previous studies have proposed many methods for implicit discourse recognition, among them reasoning-based (Sugiura et al., 2013) and pattern-based (Saito et al., 2006) methods. Many of these earlier studies (Marcu and Echihabi, 2002; Lin et al., 2009; Pitler et al., 2009; Wang et al., 2012; Lan et al., 2013; Biran and McKeown, 2013; Ruther- ford and Xue, 2014) focused on using word pairs or their derivative features. For example, take the two following sentences:A1 : I like summer. B1 : I prefer winter.In this case, we can easily identify the relation as “comparison” by focusing on the word pair “sum- mer - winter”. However, there is emerging evidence that word pairs may no longer have a role to play in implicit discourse relation recognition (Park and Cardie, 2012). This is because identification is not always possible by using just word pairs. When we consider the following sentences,A2 : I got soaked by the sudden rain yesterday. B21: Did you forget your umbrella at the office? B22: The rain was so heavy that my umbrella wasuseless.discourse A2 − B21 and A2 − B22 have different relations. discourse A2 − B21 is causal relation: B21 explains the reason for A1, and A2 − B22 is expansion relation: B22 is a supplemental explana- tion about the “sudden rain” in A2. Nevertheless, the same word pair “soaked - umbrella” can be ex-     
   9(%A473 D('A89@ 8) D%948C@ (E99(@@487 C74A@ C@473 A4( iiii (7A(7'(    ('A89@ii (7A(7'(    ('A89@8(%AC9( '9(%A487 )987 D('A89@ )89 9(6%A487 9('8374A487     (7A(7'(   (7A(7'(  $E7%74' 886473$4))(9(7'(  ('A898(%AC9(   ('A898yhvsvr  h $4@'8C9@( (6%A487 b%'(6               Figure 1: Overview of proposed discourse relation recognition.SII Tpr&$#00#4###G##36 9@@ @@8999 8A 8 @@999@ @89@ A9@@9@98A300#4###G##36 9@@ @@8999 8A 8 @@999@ @89@3         Thpvp Tprqrrqrp pr&$#  00#  4#  ##G# #36 9@@ @@8999 8A 8 @@999@ @89@ A9@@9@98A3Trrpr9rrqrpTrtr  ir 00#4#38A 8 @@999@ @89@3&$###G##4###G##36 9@@ @@8999 8A 8 @89@3                &$# 00#3A9@@9@98A3 3@@999@34#38A 8 @89@336 9@@ @@8999 A9@@9@98A3##G##36 9@@ @@89993                    tracted for both cases, making little contribution to relation recognition. If we can use pairs of longer expressions such as “I got soaked - forget your um- brella” and “I got soaked by the sudden rain - so heavy that my umbrella was useless ”, it will be eas- ier to perform relation recognition because the units employed are more specific and distinguishing of discourse relations.This paper proposes a novel method for implicit discourse relation recognition that compares various expression units between two sentences. The small- est units of a sentence expression are words, and the largest are the entire sentence. To consider various expression units, we turn to a recursive neural net- work (RNN) based approach. The RNN is the neural network based method to create vectors of various expression units on the basis of the syntactic struc- ture of a sentence and has been applied to various NLP tasks (Socher et al., 2011; Li et al., 2014; Liu et al., 2014). Here, we employ the RNN based ap- proach for implicit discourse recognition and show that our proposed method significantly outperforms the word pair based approach.In this paper, we demonstrate through experi- ments using Japanese conversational data that our method can improve the estimation performance of implicit discourse relation recognition more than the conventional word pair method. In the following sections, we first describe our proposed method us- ing the RNN with Japanese sentences in Section 2. Section 3 explains the experiments we performed on implicit discourse recognition in Japanese dialogue, and we discuss the results in Section 4. Finally, we conclude in Section 5.2 Discourse relation recognition by comparing various units of sentence expressionsFigure 1 shows an overview of the proposed method using various units of expressions in a sentence to identify implicit discourse relations. First, we input sentences to the RNN. The RNN then creates vectors of various expression units on the basis of the input syntactic structures in a bottom-up fashion. Next, we create a feature vector by comparing vectors ofXq &$ # 00 # 4 # ##G# # Figure 2: RNN structure in Japanese dependency structure.
various units of expression. The discourse relation is identified by a discriminative classifier such as a support vector machine (SVM). In this section, we explain how the RNN works, describe how the vec- tors are created by the RNN, and show how to create the feature for the classifier from vectors.2.1 Recursive neural networkThe RNN is a kind of deep neural network created by applying the same set of weights recursively over a structure. The RNN has a binary tree structure, and its framework computes the representation for each parent iteratively in a bottom-up fashion on the basis of its children. We assume that word vectors c1 , c2 , and c3 have N dimensions. Each word is given vec- tors in advance by word embeddings (e.g., word2vec (Mikolov et al., 2013a)). Segment vectors are cre- ated by combining word vectors from left to right in each segment. The c1 and c2’s parent representation vector p1 is computed asp1 =f(We[c1;c2]+be) (1)where [c1;c2] is the 2N-dimension concatenation vector of c1 and c2, We is the N × 2N encoding matrix, be is the N-dimension encode bias vector, and f denotes an element-wise activation function (we use tanh). The next parent representation vector p2 , which has children p1 and c3 , is computed in the same way by an input concatenation vector [p1 ; c3 ] and encoding parameters We and be.2.2 Creating vectors of various expression units using the RNNThe RNN creates vectors of various expression units during the process of creating a sentence vector. Our approach compares the meaning of two sentences by using these interim vectors. In this subsection, we introduce a method for extracting vectors of various expression units by the RNN for Japanese sentences.Figure 2 shows the RNN structure based on Japanese dependency structure. Japanese sentences have dependency structures made up of bunsetsu segments (bunsetsu is a Japanese expression unit comprising one or more content words with zero or more function words). We obtain the syntac- tic structures of sentences by Japanese dependency parsing. Refer to (Kudo and Matsumoto, 2003) for how Japanese dependency parsing works in general.We create segment vectors by combining word vectors. The sentence vector is the root vector of the RNN created at the end of the combining pro- cess. In this paper, we construct an RNN tree struc- ture on top of the Japanese dependency structure. In Japanese, dependency relationships are generally di- rected from left to right, so we constantly combine segment vectors from the right-most segment to ob- tain the segment vector, as in the example shown in Fig. 2.Because Japanese dependency structures are not a binary tree, there are some vectors that are not used in the process of creating the sentence vector. For example, the vectors of the expressions “I got soaked yesterday” and “I got soaked by rain” are not created in the process of creating the sentence vector in Fig. 2. Since these vectors have an independent meaning and can be useful, in our proposed method, we use all the vectors (including ones that do not lead to the sentence vector) in the RNN structure for discourse relation recognition as we describe in the following section.2.3 Feature creation from vectors for discourse relation recognitionIf sentences 1 and 2 have n and m vectors, respec- tively, we have to create a feature vector consider- ing n × m patterns. However, the feature vector for the classifier must be fixed-length although the num- ber of vectors extracted from a sentence changes dy- namically depending on the number of words and on the syntactic structure. Therefore, we need to create a fixed-length feature vector without dependence on the number of vectors. The simplest approach to do this is to use a concatenation of sentence vectors as the feature vector. However, this way does not al- low us to directly compare the meaning of interme- diate expression units. Here, we create fixed-length feature vectors by dynamic pooling and difference vectors as follows:Dynamic PoolingDynamic Pooling (DP) (Socher et al., 2011) is a method to create fixed-length features using the similarity between two vectors (Fig. 3). First, we create a similarity matrix between the vec- tors within the two sentences. The similarity between two vectors is computed with cosine
(7A(7'(                                                                                                           &RPSXWH WKH  VLPLODULW\ IRU HDFK  FRPELQDWLRQ RI  YHFWRUV  6SOLW XS WKH  VLPLODULW\ PDWUL[ E\  ZLQGRZ VL]H6LPLODULW\ PDWUL[  6                        (7A(7'(                                                                                                                                                                                          similarity, as follows:W              ([WUDFW RQH  VLPLODULW\ IURP  HDFK ZLQGRZVFigure 3: Overview of Dynamic Pooling.Difference vectorsv1 · v2 sim(v1, v2) = |v1||v2|(2)Recent studies of word embeddings such as word2vec (Mikolov et al., 2013b) have re-vealed that difference vectors are meaningful. In the well-known word2vec example, the vec- tor operation “king - man + woman = queen” holds. That is, the difference vector “king - man ” represents the information of kingship. Fol- lowing this insight, we use the difference vec- tor in the hope that it can capture some rela- tions between sentences. The difference vector is computed by subtracting two vectors, v1 and v2,v1 − v2 diff(v1,v2)= |v1 −v2| (3)where vectors v1 and v2 denote vectors cre- ated by the RNN. In this paper, we utilize the mean vector of all difference vectors created by a combination of all the vectors (i.e., vectors that correspond to all the cells in the matrix S in Fig. 3) of two sentences as a feature vector.3 ExperimentWe performed experiments using a Japanese conver- sational corpus. First, we explain the dataset used where v1 and v2 denote vectors extracted from sentences 1 and 2, respectively. The row and column order of the matrix is placed depth-first in the RNN tree, right-to-left. Specifically, ma- trix element s00 , which is the first element of similarity matrix S, is the degree of similarity between the left-most word vectors in each sen- tence.In DP, the similarity matrix is split up into asub-matrix by a grid window. The size of thegrid window is computed depending on pool-ing size np. If sentences 1 and 2 have N andM vectors, respectively, the grid window sizeis [ N ] × [M ]. We extract a maximum similar- np npity value element in each sub-matrix to create a pooled matrix. This pooled matrix is con- sistently fixed-length because the grid window size dynamically varies depending on sentence length. Similarity information between two sentences is consolidated into a fixed-length feature by the DP.   
  @ 647(     @9(%6(9  @                $8 E8C (9476 %6'84864' '(D(9%3(@ 47 E8C9 (%46E 64)(   @  @ 647(     @9(%6(9      '877('A4D( '%A(389E G7964'4A  '6%@@ ( @iGui   AE9( G7@A%7A4%A487  9(6   7%96(9     889 (E%796(                     G 8)A(7 (9476 74978))4'(  @  @ 647(     @9(%6(9   @  '877('A4D( '%A(389E G7964'4A   '6%@@  ui Gi9(i    AE9(  %C@(   9(6   7%96(9        ('%C@(              GA 4%@ % 9()9(@4473 A%@A(  @  @ 647(     @9(%6(9   @            $8 E8C (9476 X%9%7(@( %6(   @  @ 647(     @9(%6(9      '877('A4D( '%A(389E G7964'4A   '6%@@  uh@Gui   AE9(  87A9%@A  9(6   7%96(9     CA                  G 9%9(6E (9476 X%9%7(@( %6(   @  @ 647(     @9(%6(9             4A@ A%@A( 4@ @8 C749C(  '877('A4D( '%A(389E (E964'4A   '6%@@  ui Gi9(i    AE9(  %C@(  9(6          ('%C@(   '877('A4D(  @  Figure 4: Discourse relation corpus from Japanese dialogue.  Utterance 2  Relation                 (I often drink Smirnoff Ice.)  Implicit         EXPANSION   Instantiation           (It has a refreshing taste.)  Implicit       CONTINGENCY Cause              (I rarely drink Japanese sake.)  Implicit       COMPARISON Contrast           (Because I think its taste is so unique.)  Explicit       CONTINGENCY CauseUtterance 1Connective     (For example)       (Because)      (But)      (Because) Table 1: Examples of utterance pairs and discourse relations extracted from Fig. 4.               (Do you drink alcoholic bever- ages in your daily life?))                   (I often drink Smirnoff Ice.)                 (I often drink Smirnoff Ice.)for the experiment. Next, we describe our experi- mental methodology and comparative methods. Fi- nally, we present the experimental results.3.1 DatasetIn this paper, we focus on conversational dialogue because we want sophistication of dialogue analysis by using discourse relations.The annotation framework follows the Penn Dis- course Treebank (PDTB), a corpus of English texts from the Wall Street Journal in which the relations                (I rarely drink Japanese sake.) between abstract objects in discourse are annotated (Prasad et al., 2008). The PDTB has four classes (CONTINGENCY, COMPARISON, EXPANSION, and TEMPORAL) and 16 types of discourse rela- tion within its hierarchical structure. In the PDTB, the discourse relations are decided with connectives: “because” , “and”, “but”, and so on. If a discourse marker (e.g., a connective) is written clearly in ei- ther target sentence, the discourse relation is cate- gorized as Explicit. Discourse relations without any discourse marker are called Implicit.
We annotated PDTB-style discourse relations to the Japanese conversational dialogue corpus created by Higashinaka et al. (2014). Figure 4 shows the annotated Japanese conversational dialogue corpus. We provide connective tags to each utterance if they have a connective. Connective elements have five at- tributes: category, which denotes discourse relation category and can be either explicit or implicit; class, which includes the four discourse relations; type, which denotes detailed relation types; rel, which de- notes an utterance line number that has a discourse relation; and marker, which denotes the connective appropriate for discourse relation if the relation is Implicit. Table 1 gives a tabular view of the utter- ance pairs from Fig. 4.Note that there is another dialogue corpus anno- tated with PDTB-style discourse relations (Tonelli et al., 2010); however, they focus on the design of the corpus and do not tackle the problem of discourse relation recognition.3.2 Experimental method and resultsWe evaluate our proposed approach using the anno- tated conversational dialogue corpus. We created an implicit discourse relation classifier using an SVM with training data consisting of utterance pairs that have an explicit discourse relation. Explicit relations are more certain than implicit relations, so explicit relational data have been used as training data (Pitler and Nenkova, 2009).We performed the evaluation by classifying three discourse relations (CONTINGENCY, COMPARI- SON , and EXPANSION) using classifiers. Here, we do not use the TEMPORAL relation class be- cause far fewer utterance pairs have a relation to TEMPORAL than the other relations. Training data consisted of 5,000 utterance pairs for each relation. Test data were utterance pairs that have an implicit discourse relation, with each relation containing 500 utterance pairs by random sampling.We evaluated our proposed method along with several comparative methods. All the methods de- rive features for two sentences to be classified by the SVM. The features used by the methods are de- scribed as below.• Comparative methodsWord pairThe word pair feature is a basic feature for discourse recognition. Input sentences are split into words by a morphological analyzer MeCab1 (we used this analyzer throughout the paper). We create word pair tokens from the combination of words between two sentences. Finally, the word pair feature is created by creating word- pair appearance frequency vectors.Vector centroidWe create a sentence vector by computing the centroid of all word vectors in the sen- tence and use the vector as a feature. Here, word vectors are given by the word2vec model created using Japanese Wikipedia data. Note that the word centroid vector reflects the whole meaning of the sentence without syntactic structure or word order.RNN sentenceThe RNN sentence feature is the root node vector of the RNN structure. Parameters of the RNN are trained with data consist- ing of 100,000 utterances from the afore- mentioned dialogue corpus. The sentence vector differs from the word centroid vec- tor in that it includes the information of syntactic structure.• Proposed methodsRNN + DPThe RNN+DP feature is a concatenation vector with the RNN sentence vector and Dynamic Pooling vector (window size: 5).RNN + DP + diffThe RNN+DP+diff feature is a concatena- tion vector with the RNN sentence vector, Dynamic Pooling, and a difference vector.Figure 5 shows the results of the overall classi- fication accuracy and McNemar’s testing, and Ta- ble 2 shows the implicit discourse classification per- formance for each discourse relation by using pre- cision, recall, and F-score. As can be seen in Fig. 5, our proposed method (RNN + DP + diff) had the1 http://taku910.github.io/mecab/ 
    CONTINGENCYPrecisionCOMPARISON         PrecisionRecallF-scoreRecallF-scorePrecisionRecall         0.38 0.39 0.420.600.38 0.260.320.400.260.22 0.43 0.66                 0.410.450.41 0.410.460.38 0.320.41 0.430.41 0.40 0.470.460.480.29 0.300.360.400.340.36 0.370.370.410.36          0.390.410.53 0.60           Word pairVector centroid RNN sentence RNN + DPRNN + DP + diffUtterance 1Table 2: Implicit discourse classification scores. Example of correct classification by all methodsEXPANSION F-score0.28 0.42 0.47 0.45 0.49Predicted relation COMPARISON EXPANSIONCONTINGENCY EXPANSION                  (Snowboarding is hard for me.)            ( I like variety shows.)          (I’m good at skiing too!)                (What type of TV programs do you like?)Example of correct classification by RNN + DP + diff             (I went to an amuse-ment park yesterday.)                   (Where do you learn your makeup tech- niques?)                           (My favorite band performed played a live show there.)               (I learn them by reading magazines.)                                                                                         (0.38) is very close to that of pure chance (0.33). We separately checked the inter-annotator agree- ment of discourse relation relation annotation and found that the accuracy of human (taking another annotator’s annotation as gold standard) is 0.67. If the upper bound is 0.67, then our proposed method (0.43) achieves 64% accuracy relative to human per- formance, which is a lot higher than 57% accuracy (0.38) of Word pair, showing our contribution to implicit discourse relation recognition.We show examples of the discourse relation recognition results between two Japanese utterances in Table 3. The upper two examples show utterance pairs that were classified correctly by all methods, while the two examples at the bottom were correctly classified by only the (RNN + DP + diff) method.4 DiscussionThe accuracy and McNemar’s testing results indi- cate that our proposed approach (RNN + DP + diff) outperformed the word-pair and sentence vector ap- proach, demonstrating that our approach, with its use of various units of expression, is more effective than the approach based on word pair and sentences.      "':RUG 3DLU  #!9HFWRU &HQWURLG  "(511 6HQWHQFH  #511   '3  #"511   '3   GLIITable 3: Examples of discourse relation recognition between two utterances.      Figure 5: Comparison of classification accuracy.highest accuracy (accuracy = 0.43), and the results of McNemar’s testing reveal a significant difference between the (Word pair) and (RNN + DP + diff) methods (p = 0.0046, p < 0.001) and between the (RNN sentence) and (RNN + DP + diff) methods (p = 0.0077, p < 0.001). In contrast, the differ- ence between the (Vector centroid) and (RNN + DP + diff) methods was only marginally significant (p = 0.12).The accuracy of the baseline method Word pairUtterance 26pphp
               G 646( @8''(9 A4%7 '%@('%66               G 646( @8''(9 A4%7 '%@('%66       G 646( @8''(9          G 646( @8''(9       G 646( '%@('%66         G 646( '%@('%66            G 646( '%@('%66 A4%7 @8''(9              G 646( '%@('%66 A4%7 @8''(9Figure 6: Visualization of RNN sentence vectors.            G 646( '%@('%66 A4%7 @8''(9   Figure 7: Visualization of word centroid vectors. Note that the vector “I like soccer more than baseball” overlaps with the vector “I like baseball more than soccer.”.In the example in Table 3, the inputs classified correctly by all methods were identified by extract- ing the characteristic content words from each ut- terance. For example, in the first example, the re- lation is identified as COMPARISON by extracting the pair “skiing - snowboarding”. In contrast, in the last example, while the relation is difficult to identify as EXPANSION by extracting the pairs “makeup - magazine” or “makeup - learn”, we can identify the relation by extracting the expression pairs “your makeup techniques - by reading a magazine”. By taking advantage of the various units of expression in a sentence, our approach appropriately identifies the discourse relation between two sentences.Our experimental results show that the RNN vec- tors are not always superior to word centroid vectors because there are cases where it is not necessary to consider syntax. Sometimes, word pairs are better suited for obtaining the generic topic of a sentence. However, we also found that implicit discourse rela- tion recognition requires to detect slight differences in expressions in sentences. For example, Figs. 6 and 7 compare RNN vectors and word-centroid vec- tors in the visualization of vector space. The sen- tences “I like baseball more than soccer.” and “I like soccer more than baseball.” are in different places in Fig. 6. If the first sentence is “I like soccer.” and the second sentence is “I like soccer more than base- ball.”, the discourse relation between two sentences is EXPANSION (I like soccer. Moreover, I like soc- cer more than baseball.). However, if the second sentence is “I like baseball more than soccer.”, the most appropriate discourse relation is COMPARI- SON (I like soccer. But I like baseball more than soccer.). The RNN vectors are able to capture these different structures, enabling our proposed method to recognize discourse relations more precisely.5 ConclusionWe proposed an implicit discourse relation detection method using various units of expressions between two sentences. All expressions are converted into vectors by the RNN and then applied to Japanese de- pendency structures. Experimental results showed that our approach performs better than the conven- tional word-pair features method. This paper is the first to show that various expression units in sen- tences are effective for implicit discourse relation recognition.Our future work is to enable more feature selec- tion using intermediate expression vectors and to consider applications for dialogue systems. Current dialogue systems have problems that they choose a contextually inappropriate utterance for the user in- put. Since two utterances with a discourse relation can be coherent, we expect the quality of utterance selection to be increased by selecting an utterance that has a discourse relation with the user utterance.                      G 646( @8''(9 A4%7 '%@('%66        G 646( @8''(9          G 646( @8''(9              G 646( @8''(9 A4%7 '%@('%66       G 646( '%@('%66              G 646( '%@('%66 A4%7 @8''(9       G 646( '%@('%66  
Referencesternational Conference on Learning Representations(ICLR 2013).Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013b. Linguistic regularities in continuous space word representations. Proc of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies (NAACL-HLT 2013), pages 746–751.Joonsuk Park and Claire Cardie. 2012. Improving im- plicit discourse relation recognition through feature set optimization. Proc of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL 2012), pages 108–112.Emily Pitler and Ani Nenkova. 2009. Using Syntax to Disambiguate Explicit Discourse Connectives in Text. Proc of the Joint Conference of the 47th Annual Meet- ing of the ACL and the 4th International Joint Confer- ence on Natural Language Processing of the AFNLP Short Papers (ACL-IJCNLP 2009), pages 13–16.Emily Pitler, Annie Louis, and Ani Nenkova. 2009. Au- tomatic Sense Prediction for Implicit Discourse Rela- tions in Text. Proc of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Asian Federa- tion of Natural Language Processing (AFNLP 2009), pages 683–691.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt- sakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber. 2008. The Penn Discourse TreeBank 2.0. Proc of the sixth international conference on Lan- guage Resources and Evaluation (LREC 2008).Attapol Rutherford and Nianwen Xue. 2014. Discov- ering implicit discourse relations through brown clus- ter pair representation and coreference patterns. Proc of the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2014), pages 645–654, April.Manami Saito, Kazuhide Yamamoto, and Satoshi Sekine. 2006. Using phrasal patterns to identify discourse re- lations. Proc of the 2006 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies (NAACL-HLT 2006), pages 133–136.Richard Socher, Eric H. Huang, Jeffrey Pennin, Christo- pher D Manning, and Andrew Y. Ng. 2011. Dy- namic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection. Proc of Advances in Neural Information Processing Systems (NIPS 2011), pages 801–809.Jun Sugiura, Naoya Inoue, and Kentaro Inui. 2013. Rec- ognizing Implicit Discourse Relations through Abduc- tive Reasoning with Large-scale Lexical Knowledge. Proc of the 1st Workshop on Natural Language and Automated Reasoning (NLPAR 2013), pages 76–87.Or Biran and Kathleen McKeown. 2013. Aggregated word pair features for implicit discourse relation dis- ambiguation. Proc of the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013), pages 69–73.Shima Gerani, Yashar Mehdad, Giuseppe Carenini, Ray- mond T. Ng, and Bita Nejat. 2014. Abstractive sum- marization of product reviews using discourse struc- ture. Proc of the 2014 Conference on Empirical Meth- ods in Natural Language Processing (EMNLP 2014), pages 1602–1613.Ryuichiro Higashinaka, Kenji Imamura, Toyomi Me- guro, Chiaki Miyazaki, Nozomi Kobayashi, Hiroaki Sugiyama, Toru Hirano, Toshiro Makino, and Yoshi- hiro Matsuo. 2014. Towards an Open Domain Con- versational System Fully Based on Natural Language Processing. Proc of the 25th International Conference on Computational Linguistics (COLING 2014), pages 928–939.Taku Kudo and Yuji Matsumoto. 2003. Fast methods for kernel-based text analysis. Proc of the 41st Annual Meeting of the Association for Computational Linguis- tics (ACL 2003), pages 24–31.Man Lan, Yu Xu, and Zhengyu Niu. 2013. Leveraging Synthetic Discourse Data via Multi-task Learning for Implicit Discourse Relation Recognition. Proc of the 51st Annual Meeting of the Association for Computa- tional Linguistics(ACL 2013), pages 476–485.Jiwei Li, Rumeng Li, and Eduard Hovy. 2014. Re- cursive deep models for discourse parsing. Proc of the 2014 Conference on Empirical Methods in Natu- ral Language Processing (EMNLP 2014), pages 2061– 2069.Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009. Recognizing Implicit Discourse Relations in the Penn Discourse Treebank. Proc of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP 2009), pages 343–351.Shujie Liu, Nan Yang, Mu Li, and Ming Zhou. 2014. A recursive recurrent neural network for statistical ma- chine translation. Proc of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014), pages 1491–1500.Daniel Marcu and Abdessamad Echihabi. 2002. An Un- supervised Approach to Recognizing Discourse Rela- tions. Proc of the 40th Annual Meeting on Associa- tion for Computational Linguistics (ACL 2002), pages 368–375.Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient Estimation of Word Represen- tations in Vector Space. Proc of the Workshop at In-
Sara Tonelli, Giuseppe Riccardi, Rashmi Prasad, and Ar- avind Joshi. 2010. Annotation of Discourse Relations for Conversational Spoken Dialogs. Proc of the Sev- enth International Conference on Language Resources and Evaluation (LREC 2010), pages 19–21.Xun Wang, Sujian Li, Jiwei Li, and Wenjie Li. 2012. Im- plicit discourse relation recognition by selecting typi- cal training examples. Proc of the 24th International Conference on Computational Linguistics (COLING 2012), pages 2757–2772.