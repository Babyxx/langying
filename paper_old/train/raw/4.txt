A Classifier-based Preordering Approachfor English-Vietnamese Statistical Machine TranslationAbstractReordering is of essential importance problem for phrase based statistical machine translation (SMT). In this paper, we propose an approach to automatically learn reordering rules as pre- processing step based on a dependency parser in phrase-based statistical machine transla- tion for English to Vietnamese. Inspired from (Lerner and Petrov, 2013) using classifier- based preordering approach, we used depen- dency parsing and rules extracting from train- ing the features-rich discriminative classifiers for reordering source-side sentences. We eval- uated our approach on English-Vietnamese machine translation tasks, and show that it out- perform the baseline phrase-based SMT sys- tem.1 IntroductionPhrase-based statistical machine translation (Koehn et al., 2003; Och and Ney, 2004) is the state-of-the- art of SMT because of its power in modelling short reordering and local context. However, with phrase- based SMT, long distance reordering is still prob- lematic. The reordering problem (global reordering) is one of the major problems. In recent years, many reordering methods have been proposed to tackle the long distance reordering problem.Many solutions to the reordering problem have been proposed, e.g syntax-based model (Chiang, 2005), lexcicalized reordering (Och and Ney, 2004), tree-to-string methods (Zhang et al., 2007). Chiang (Chiang, 2005) shows significant improvement by keeping the strengths of phrases, while incorporat- ing syntax into SMT. Some approaches have beenapplied at the word-level (Collins et al., 2005). They are particularly useful for language with rich mor- phology, for reducing data sparseness. Other kinds of syntax reordering methods require parser trees , such as the work in (Quirk et al., 2005; Collins et al., 2005; Huang and Mi, 2010). The parsed tree is more powerful in capturing the sentence structure. However, it is expensive to create tree structure, and building a good quality parser is also a hard task. All the above approaches require much decoding time, which is expensive.The approach we are interested in here is to bal- ance the quality of translation with decoding time. Reordering approaches as a preprocessing step (Xia and McCord, 2004; Xu et al., 2009; Genzel, 2010; Lerner and Petrov, 2013) is very effective (improve- ment significant over state of-the-art phrase-based and hierarchical machine translation systems and separately quality evaluation of reordering models).Inspiring this preprocessing approach, we have proposed a combine approach which preserves the strength of phrase-based SMT in local reordering and decoding time as well as the strength of inte- grating syntax in reordering. Firstly, we use depen- dency parsing for preprocessing with training and testing. Second, we use the features discriminative classifiers from data training to extract rules which are learnt automatically from parallel corpus to the dependency tree. Beside these rules, we can use ex- tracted features in source-side that directly predict the target-side word order to be applied as a pre- processing step in phrase-based machine translation. The experiment results from English-Vietnamese pair showed that our approach achieves significant
improvements over MOSES which is the state-of-the art phrase based system.The rest of this paper is structured as follows. Sec- tion 2 reviews the related works. Section 3 briefly introduces classifier-based Preordering for Phrase- based SMT. Section 4 describes and discusses the experimental results. And, conclusions are given in Section 5.2 Related worksThe difference of the word order between source and target languages is major problems in phrase-based statistical machine translation.Preordering (reordering-as-preprocessing) is ap- proach for tacking the problem, which modifies the word order of an input sentence in a source language to have the word order in a target language (Firgure 1).Figure 1: A example of preordering for English- Vietnamese translation.Many preordering methods using syntactic infor- mation have been proposed to solve the reordering problem. (Collins et al., 2005; Xu et al., 2009) pre- sented a preordering method which use manually created rules on parse trees and linguistic knowl- edge for a language pair is necessary to create such rules. Other preodering methods using automatically create reodering rules or utilize statistical classifier have been studied (Xia and McCord, 2004; Li et al., 2007; Yang et al., 2012; Lerner and Petrov, 2013; Jehl et al., 2014)(Collins et al., 2005) developed a clause detection and used some handwritten rules to reorder words in the clause. Partly, (Xia and McCord, 2004; Habash, 2007) built an automatic extracted syntactic rules.(Xu et al., 2009) described method using depen- dency parse tree and a flexible rule to perform the reordering of subject, object, etc... These rules were written by hand, but (Xu et al., 2009) showed that an automatic rule learner can be used.(Genzel, 2010; Lerner and Petrov, 2013) de- scribed method using discriminative classifiers to directly predict the final word order. (Jehl et al., 2014) present a simple preordering approach for ma- chine translation based on a feature-rich logistic re- gression model to predict whether two children of the same node in the source-side parse tree should be swapped or not. (Jingsheng Cai, 2014) introduces a novel pre-ordering approach based on dependency parsing for Chinese-English SMT. (Hoshino et al., 2015) proposed a simple procedure to train a dis- criminative preordering model. The main idea is to obtain oracle labels for each node by maximizing Kendall’s τ of word alignments. (Nakagawa, 2015) present an efficient incremental top down parsing method for preordering based on Bracketing Trans- duction Grammar.Compared with theses approaches, our work has a few differences. Firstly, we aim to develop the phrase-based translation model to translate from En- glish to Vietnamese. Secondly, we extracted a set rules from English-Vietnamese parallel corpus by using SVM classification model (Wang, 2005) in Weka tools (Hall et al., 2009) with lexical and syn- tactic features. Thirdly, we use the automatic rules that directly predict target-side word as a preprocess- ing step in phrase-based machine translation. As the same with (Xia and McCord, 2004; Habash, 2007), we also apply preprocessing in both training and de- coding time. 3 3.1Classifier-based Preordering for Phrase-based SMTPhrase-based SMTIn this section, we will describe the phrase-based SMT system which was used for the experiments. Phrase-based SMT, as described by (Koehn et al., 2003) translates a source sentence into a target sen-
tence by decomposing the source sentence into a se- quence of source phrases, which can be any contigu- ous sequences of words (or tokens treated as words) in the source sentence. For each source phrase, a target phrase translation is selected, and the target phrases are arranged in some order to produce the target sentence. A set of possible translation candi- dates created in this way is scored according to a weighted linear combination of feature values, and the highest scoring translation candidate is selected as the translation of the source sentence. Symboli- cally,(Koehn et al., 2003) used the following distortion model (reordering model), which simply penalizes nonmonotonic phrase alignment based on the word distance of successively translated source phrases with an appropriate value for the parameter α:d(ai − bi−1) = α|ai−bi−1−1| (2) 3.2 Classifier-based PreorderingIn this section, we describe the learning a model that can transform the word order of an input sentence to an order that is natural in the target language. En- glish is used as source language, while Vietnamese is used as target language in our discussion about the word orders.For example, when translating the English sen- tence:I ’m looking at a new jewelry site.to Vietnamese, we would like to reorder it as:I ’m looking at a site new jewelry.And then, this model be used in combination with translation model.We use the dependency grammars and the dif- ferences of word order between English and Viet- namese to create the set of the reordering rules. We part-of-speech (POS) tag and parse the input sen- tence, producing the POS tags and head-modifier dependencies shown in Figure 2. Traversing the de- pendency tree starting at the root to reordering. We determine the order of the head and its children (in- dependently of other decisions) for each head word and continue the traversal recursively in that order. In the above example, we need to decide on the or- der of the head "looking" and the children "I", "’m", and "site.".The words in sentence are reordered by new sequence learned from training data using multi- classifier model. We use SVM classification model (Wang, 2005) that supports multi-class prediction. The class labels are corresspond to reordering se- quence, so it is enable to select the best one from many possible sequences.3.2.1 FeaturesThe features are extracted based on dependency tree includes POS tag and alignment information. We traverse the tree from the top, in each family we create features with such information:ntˆ= argmax λifj(s,t,a)t,a(1)i=1when s is the input sentence, t is a possible output sentence, and a is a phrasal alignment that specifies how t is constructed from s, and tˆis the selected out- put sentence. The weights λi associated with each feature fi are tuned to maximize the quality of the translation hypothesis selected by the decoding pro- cedure that computes the argmax. The log-linear model is a natural framework to integrate many fea- tures. The baseline system uses the following fea-tures:• the probability of each source phrase in the hy- pothesis given the corresponding target phrase.• the probability of each target phrase in the hypothesis given the corresponding source phrase.• thelexicalscoreforeachtargetphrasegiventhe corresponding source phrase.• the lexical score for each source phrase given the corresponding target phrase.• the target language model probability for the se- quence of target phrase in the hypothesis.• the word and phrase penalty score, which allow to ensure that the translation does not get too long or too short.• the distortion model allows for reordering of the source sentence.The probabilities of source phrase given target phrases, and target phrases given source phrases, are estimated from the bilingual corpus.
  • The head’s POS tag,• The first child’s POS tag, the first child’s syn-tactic label.• The second child’s POS tag, the second child’s syntactic label.• The third child’s POS tag, the third child’s syn- tactic label.• The fourth child’s POS tag, the fourth child’s syntactic label.• The sequence of head and its children in source alignment• The sequence of head and its children in target alignment. It is class label for SVM classifier model.Feature DescriptionT The head’s POS tag1T The first child’s POS tag1L The first child’s syntactic label2T The second child’s POS tag2L The second child’s syntactic label3T The third child’s POS tag3L The third child’s syntactic label4T The fourth child’s POS tag4L The fourth child’s syntactic labelO1 The sequence of head and its children in source alignment O2 The sequence of head and its children in target alignment.Table 1: Set of features used in training data from corpus English-Vietnamesemulti-class prediction and can therefore be used to select one out of many possible permutations. The learning algorithm produces a sparse set of features. In our experiments the our models have typically only a few 50K non-zero feature weights English- Vietnamese language pairs.When extracting the features, every word can be represented by its word identity, its POS-tags from the treebank, syntactic label. We also include pairs of these features, resulting in potentially bilexical features.3.2.2 Training Data for PreorderingIn this section, we describe a method to build training data for a pair English to Vietnamese. Our purpose is to reconstruct the word order of input sentence to an order that is arranged as Vietnamese words order.For example with the English sentence in Figure 2:I ’m looking at a new jewelry site.is transformed into Vietnamese order:I ’m looking at a site new jewelry.For this approach, we first do preprocessing to en- code some special words and parser the sentences to dependency tree using Stanford Parser (de Marn- effe and D.Manning, 2006; Cer et al., 2010). Then, we use target to source alignment and dependency tree to generate features. We add source, target align- ment, POS tag, syntactic label of word to each node in the dependency tree. For each family in the tree, we generate a training instance if it has less than and                Figure 2: A example with POS tags and dependency parser.The feature is built for "site, a, new, jewelry" fam- ily in Figure 2:NN, DT, det, JJ, amod, NN, nn, 1230, 1023We limited ourself by procesing families that have less than five children based on counting total fam- ilies in each group: 1 head and 1 child, 1 head and 2 children, 1 head and 3 children, 1 head and 4 chil- dren ... We found out that the most common families appear (80%) in our training sentences is less than and equal four children.We train a separate classifier for each number of possible children. In hence, the classifiers learn to trade off between a rich set of overlapping features. List of features is given in table 1.We use SVM classification model (Wang, 2005) in the WEKA tools (Hall et al., 2009) that supports multi-class prediction. Since it naturally supports
  Algorithm 1 Extract rules  input: dependency trees of source sentences and alignment pairs;output: set of automaticaly rules;for each family in dependency trees of subset and alignment pairs of sentences dogenerate feature (pattern + order) ;end forBuild model from set of features;for each family in dependency trees in the rest of the sentences dogenerate pattern for prediction;get predicted order from model;add (pattern, order) as new rule in set of rules;end for                           Algorithm 2 Apply rule  input: source-side dependency trees , set of rules; output: set of new sentences;for each dependency tree dofor each family in tree dogenerate patternget order from set of rules based on pattern apply transformend forBuild new sentence;end for                 equal four children. In case, a family has more than and equal five children, we discard this family but still keep traversing at each child.Each rule consists of: pattern and order. For ev- ery node in the dependency tree, from the top-down, we find the node matching against the pattern, and if a match is found, the associated order applies. We arrange the words in the English sentence, which is covered by the matching node, like Vietnamese words order. And then, we do the same for each chil- dren of this node. If any rule is applied, we use the order of original sentence. These rules are learnt au- tomatically from bilingual corpora.The our algorithm’s outline is given as Alg. 1 and Alg. 2Algorithm 1 extracts automatically rules with in- put including dependency trees of source sentences and alignment pairs.Algorithm 2 proceeds by considering all rules af- ter finish Algorithm 1 and source-side dependency trees to build new sentence.3.2.3 Classification ModelThe reordering decisions are made by multi-class classifiers (corespond with number of permutation: 2, 6, 24, 120) where class labels correspond to per- mutation sequences. We train a separate classifier for each number of possible children. Crucially, we do not learn explicit tree transformations rules, but let the classifiers learn to trade off between a richset of overlapping features. To build a classification model, we use SVM classification model (Wang, 2005) in the WEKA tools (Hall et al., 2009). The result follows are obtained using 10 folds-cross vali- dation.We apply them in a dependency tree recursively starting from the root node. If the POS-tags of a node matches the left-hand-side of a rule, the rule is ap- plied and the order of the sentence is changed. We go through all children of the node and matching rules for them from the set of automatically rules.Table 2 gives examples of original and pre- processed phrase in English. The first line is the original English: " I’m looking at a new jew- elry site .", and the target Vietnamese reordering " Tôi đang xem một trang web mới về nữ_trang .". This sentences is arranged as the Vietnamese order. Vietnamese sentences is the output of our method. As you can see, after reordering, original English line have the same word order: " I ’m looking at a site new jewelry ." in Figure 1.4 ExperimentIn this section, we present our experiments to trans- late from English to Vietnamese in a statistical ma- chine translation system. In hence, the language pairs chosen is English-Vietnamese. We used Stan- ford Parser (Cer et al., 2010) to parse source sen- tence (English sentences).We used dependency pars- ing and rules extracting from training the features- rich discriminative classifiers for reordering source- side sentences. The rules are extracted automatically by learnt in English-Vietnamese parallel corpus and
   Order  1,0,2,3    2,1,0,3    2,1,0   PatternNN, DT, det, JJ, amod, NN, nn NNS, JJ, amod, CC, cc, NNS, con NNP, NNP, nn, NNP, nnExampleI ’m looking at a new jewelry site . →I ’m looking at a site new jewelry . it faced a blank wall .→ it faced a wall blank .it ’s a social phenomenon .→ it ’s a phenomenon social .    Table 2: Examples of rules and reorder source sentences  Number children of head Number1 79142 2 40822 3 26008 4 15990 5 7442 6 2728 7 942 8 307 9 83Table 3: Statistical number of family onthe dependency parser of English examples. Final, they used to reorder source sentences. We evalu- ated our approach on English-Vietnamese machine translation tasks with three system in table 5, and show that it can outperform the baseline phrase- based SMT system.We give some definitions of our experiments:• Baseline: use the baseline phrase-based SMT system using distance-based default reordering model in Moses toolkit.• Manual Rules: the phrase-based SMT systems applying manual rules 1.• AutoRules:thephrase-basedSMTsystemsap- plying automatically rules.4.1 Implementation• We used Stanford Parser (de Marneffe and D.Manning, 2006; Cer et al., 2010) to parse source sentence and apply to preprocessing source sentences (English sentences).• We used classifier-based preordering by us- ing SVM classification model (Wang, 2005)1Improving English-Vietnamese Statistical Machine Trans- lation Using Preprocessing Dependency Syntactic, introduced in (Tran et al., 2015).This system use manual rules.   • •4.2DescriptionFamily has 1 children Family has 2 children Family has 3 children Family has 4 children Family has 5 children Family has 6 children Family has 7 children Family has 8 children Family has 9 childrencorpus English-Vietnamesein Weka tools (Hall et al., 2009) for train- ing the features-rich discriminative classifiers to extract automatically rules from English- Vietnamese parallel corpus. These automaticly rules applied for reordering words in English sentences according to Vietnamese word order.We implemented preprocessing step during both training and decoding time.Using the SMT Moses decoder (Koehn et al., 2007) for decoding.Data set and Experimental SetupFor evaluation, we used an English-Vietnamese cor- pus (Nguyen et al., 2008), including about 54642 pairs for training, 500 pairs for testing and 200 pairs for development test set. Table 4 gives more statistical information about our corpora. We con- ducted some experiments with SMT Moses Decoder (Koehn et al., 2007) and SRILM (Stolcke, 2002). We trained a trigram language model using inter- polate and kndiscount smoothing with 89M Viet- namese mono corpus. Before extracting phrase ta- ble, we use GIZA++ (Och and Ney, 2003) to build 
   CorpusSentence pairs  Training SetDevelopment Set  Test Set  General55341  54642200  499  English  Vietnamese Training     Sentences 54620 Average Length 11.2  10.6 Word 614578  580754 Vocabulary 23804  24097 Development     Sentences 200 Average Length 11.1  10.7 Word 2221  2141 Vocabulary 825  831 Test     Sentences 499 Average Length 11.2  10.5 Word 5620  6240 Vocabulary 1844  1851                Table 4: Corpus StatisticalTable 5: Our experimental systems on English-Vietnamese parallel corpus   Name Description  Baseline Manual RulesAuto Rules         Phrase-based systemPhrase-based system with corpuswhich is preprocessed using manual rulesPhrase-based system with corpus which is preprocessed using automatically learning rules        word alignment with grow-diag-final-and algorithm. Besides using preprocessing, we also used default reordering model in Moses Decoder: using word- based extraction (wbe), splitting type of reordering orientation to three class (monotone, swap and dis- continuous – msd), combining backward and for- ward direction (bidirectional) and modeling base on both source and target language (fe) (Koehn et al., 2007). To contrast, we try preprocessing the source sentence with some handwritten rules and automati- cally rules, which is described in 3.2.1.4.3 BLEU scoreThe result of our experiments in table 6 showed our applying transformation rule to process the source sentences. in this method, we can find out various phrases in the translation model. So that, they enable us to have more options for decoder to generate the best translation.Table 7 describes the BLEU score (Papineni et al., 2002) of our experiments. As we can see, by applying preprocess in both training and decoding, the BLEU score of our best system increase by 0.42point "Baseline + AR" system) over "Baseline sys- tem". Improvement over 0.42 BLEU point is valu- able because baseline system is the strong phrase based SMT (integrating lexicalized reordering mod- els).We also carried out the experiments with manual rules (Tran et al., 2015). Using automatically rules help the phrased translation model generate some best translation. Besides, the result proved that the effect of applying transformation rule on the depen- dency tree when the BLEU score is higher than base- line systems. Because, the cover of manual rules is better than automatically rules on corpus. We can extract more and better phrase tables. However, in our experimental, we need conduct with larger cor- pus and quality of corpus better to extract automati- cally rules which can cover many linguistic reorder- ing phenomena on corpus. Finally, the BLEU score of using monotone decoder increase by 0.42 when we use classifier-based preprocessing for English- Vietnamses parallel corpus. As, the default reorder- ing model in baseline system is lower than in this
 experiment2.4.4 AnalysisAccording to typical differences of word order be- tween English and Vietnamese, we have created a set of automatically rules for reordering words in English sentence according to Vietnamese word or- der and types of rules including noun phrase, adjec- tival and adverbial phrase, preposition. Table 3 gives statistical families have larger or equal 4 children in our corpus. Number of children in every family has limited 4 children in our approach. So in target lan- guage (Vietnamese), number of child in every family is common.We compared result experiment between the phrase-based SMT systems applying manual rules with the phrase-based SMT systems applying auto- matically rules. Because the manual rules have good quality (Xia and McCord, 2004; Habash, 2007), the phrase-based SMT systems applying manual rules is better than the phrase-based SMT systems applying automatically rules. In this paper, we believe that the quality of the phrase-based SMT systems applying automatically rules will better when we have a better corpus. Beside, the quality of phrase-based SMT can be improved if we combine automatically learned rules with manual rules.5 ConclusionIn this study, a preprocessing approach based on a dependency parser is presented. We used classifier- based preordering by using SVM classification model (Wang, 2005) in Weka tools (Hall et al., 2009) for training the features-rich discriminative classi- fiers to extract automatically rules from English- Vietnamese parallel corpus and apply these rules for reordering words in English sentence according to Vietnamese word order.We evaluated our approach on English- Vietnamese machine translation tasks. The ex- periment results showed that our approach achieved statistically improvements in BLEU scores over a state-of-the-art phrase-based baseline system. Improvement over 0.42 BLEU point is valuable2The reordering model in the monotone decoder is distance based, introduced in (Koehn et al., 2003). This model is a de- fault reordering model in Moses Decoder (Koehn et al., 2007)Name Baseline Manual Rules Auto RulesSize of phrase-table 1152216 1231365 1213401    Table 6: Size of phrase tables  System Baseline Manual Rules Auto RulesBLEU (%) 36.97 37.71 37.26   Table 7: Translation performance for the English- Vietnamese taskbecause baseline system is the strong phrase-based SMT.Our rules are automatically which learnt from cor- pus and can cover many linguistic reordering phe- nomena. We believe that such reordering rules ben- efit English-Vietnamese pair languages.In the future, we plan to investigate along this di- rection and extend the rules to languages other. We believe that this is the important step in trying im- proving SMT systems and that might lead to a wider adoption of them. We would like to evaluate our method with tree with higher and deeper syntactic structure and larger size of corpus.We attempt to create more efficient preordering rules by exploiting the rich information in depen- dency structures.ReferencesDaniel Cer, Marie-Catherine de Marneffe, Daniel Juraf- sky, and Christopher D. Manning. 2010. Parsing to stanford dependencies: Trade-offs between speed and accuracy. In 7th International Conference on Lan- guage Resources and Evaluation (LREC 2010).David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43rd Annual Meeting of the Association for Com- putational Linguistics (ACL’05), pages 263–270, Ann Arbor, Michigan, June.M. Collins, P. Koehn, and I. Kucerová. 2005. Clause re- structuring for statistical machine translation. In Proc. ACL 2005, pages 531–540. Ann Arbor, USA. 
Bill MacCartney de Marneffe and Christopher D.Manning. 2006. Generating typed depen- dency parses from phrase structure parses. In In the Proceeding of the 5th International Conference on Language Resources and Evaluation.Dmitriy Genzel. 2010. Automatically learning source- side reordering rules for large scale machine transla- tion. In Proceedings of the 23rd International Con- ference on Computational Linguistics, COLING ’10, pages 376–384, Stroudsburg, PA, USA. Association for Computational Linguistics.N. Habash. 2007. Syntactic preprocessing for statisti- cal machine translation. Proceedings of the 11th MT Summit.Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The weka data mining software: An update. SIGKDD Explor. Newsl., 11(1):10–18, November.Sho Hoshino, Yusuke Miyao, Katsuhito Sudoh, Kat- suhiko Hayashi, and Masaaki Nagata. 2015. Discrimi- native preordering meets kendall’s τ maximization. In Proceedings of the 53rd Annual Meeting of the Associ- ation for Computational Linguistics and the 7th Inter- national Joint Conference on Natural Language Pro- cessing (Volume 2: Short Papers), pages 139–144, Bei- jing, China, July. Association for Computational Lin- guistics.Liang Huang and Haitao Mi. 2010. Efficient incremental decoding for tree-to-string translation. In Proceedings of the 2010 Conference on Empirical Methods in Natu- ral Language Processing, pages 273–283, Cambridge, MA, October. Association for Computational Linguis- tics.Laura Jehl, Adrià de Gispert, Mark Hopkins, and Bill Byrne. 2014. Source-side preordering for transla- tion using logistic regression and depth-first branch- and-bound search. In Proceedings of the 14th Confer- ence of the European Chapter of the Association for Computational Linguistics, pages 239–248, Gothen- burg, Sweden, April. Association for Computational Linguistics.Eiichiro Sumita Yujie Zhang Jingsheng Cai, Masao Utiyama. 2014. Dependency-based pre- ordering for chinese-english machine translation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics.Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceed- ings of HLT-NAACL 2003, pages 127–133. Edmonton, Canada.Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin,and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of ACL, Demonstration Session.Uri Lerner and Slav Petrov. 2013. Source-side classifier preordering for machine translation. In EMNLP, pages 513–523.Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li, Ming Zhou, and Yi Guan. 2007. A probabilistic approach to syntax-based reordering for statistical machine trans- lation. In ANNUAL MEETING-ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, volume 45, page 720.Tetsuji Nakagawa. 2015. Efficient top-down btg parsing for machine translation preordering. In Proceedings of the 53rd Annual Meeting of the Association for Com- putational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 208–218, Beijing, China, July. Association for Computational Linguistics.Thai Phuong Nguyen, Akira Shimazu, Tu Bao Ho, Minh Le Nguyen, and Vinh Van Nguyen. 2008. A tree-to-string phrase-based model for statistical ma- chine translation. In Proceedings of the Twelfth Con- ference on Computational Natural Language Learning (CoNLL 2008), pages 143–150, Manchester, England, August. Coling 2008 Organizing Committee.Franz J. Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.Franz J. Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417–449.K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002. 2002. Bleu: a method for automatic evaluation of ma- chine translation. In Proc. of the 40th Annual Meet- ing of the Association for Computational Linguistics (ACL), pages 311–318. Philadelphia, PA, July.Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed phrasal smt. In Proceedings of ACL 2005, pages 271– 279. Ann Arbor, Michigan, USA.Andreas Stolcke. 2002. Srilm - an extensible lan- guage modeling toolkit. In Proceedings of Interna- tional Conference on Spoken Language Processing, volume 29, pages 901–904.Viet Hong Tran, Vinh Van Nguyen, and Minh Le Nguyen. 2015. Improving english-vietnamese statistical ma- chine translation using preprocessing dependency syn- tactic. In Proceedings of the 2015 Conference of the Pacific Association for Computational Linguistics (Pa- cling 2015), pages 115–121 (pdf).Lipo Wang. 2005. Support Vector Machines: theory and applications, volume 177. Springer Science & Busi- ness Media.
Fei Xia and Michael McCord. 2004. Improving a statis- tical mt system with automatically learned rewrite pat- terns. In Proceedings of Coling 2004, pages 508–514, Geneva, Switzerland, Aug 23–Aug 27. COLING.Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz Och. 2009. Using a dependency parser to improve smt for subject-object-verb languages. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the As- sociation for Computational Linguistics, pages 245– 253, Boulder, Colorado, June. Association for Com- putational Linguistics.Nan Yang, Mu Li, Dongdong Zhang, and Nenghai Yu. 2012. A ranking-based approach to word reordering for statistical machine translation. In Proceedings of the 50th Annual Meeting of the Association for Com- putational Linguistics: Long Papers-Volume 1, pages 912–920. Association for Computational Linguistics.Yuqi Zhang, Richard Zens, and Hermann Ney. 2007. Chunk-level reordering of source language sentences with automatically learned rules for statistical machine translation. In Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 1–8.