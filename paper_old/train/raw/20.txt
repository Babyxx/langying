Hindi Dialog system using PurposeNet-based OntologyAbstractIn this paper, we propose a question answering cum dialog system in Hindi, in the domain of MMTS (Multi-Modal Transport System), us- ing an Ontology, developed on an architecture based on PurposeNet. Apart from retrieving answers from the knowledgebase, this paper focuses on effectively removing the linguistic gap between the input query and the knowl- edgebase, which is in another language. In ad- dition to answering the traditional factoid (sin- gle word and list-based) answers, we have dis- cussed methods to answer why and how ques- tions, as well.1 IntroductionMMTS1 train service connects a large population, with a varied demographics, across Hyderabad, ev- eryday. Till now, a single template based timing in- formation extraction is possible. But, for other spe- cific information, one needs to either remember the complete MMTS route or extract information from the tabular enlistment of train timings on various routes. A significant population in Hyderabad speak Hindi2, hence we decided to implement a Hindi dia- log system on the local-train timing domain.This paper provides a detailed explanation of cre- ating a text-based dialog system which takes natu- ral language questions in Hindi, extracts the ques- tion type and question arguments, processes it to1Local train system in the twin cities of Hyderabad and Se- cunderabad, India (http://www.mmtstraintimings.in/)2 http://en.wikipedia.org/wiki/Demographics   of   Hyderabadcreate intermediate queries based on some predeter- mined template, extracts information from a self- built knowledgebase and generates the answer in Hindi. The OWL (Web Ontology Language) based ontology along-with the pellet reasoner (Sirin et al., 2007) is used to get answers of the intermediate queries. The ontology is already studied and all the relevant intermediate queries are created. These pre- determined queries encompass the complete knowl- edge present in the knowledgebase. This approach is similar to the one used by Sneiders (2002).The prime focus of the paper is to bridge the gap between a conceptual ontology and query in any language. The ontology is supposed to be a con- ceptual and not linguistic representation of the do- main. Even if the ontology would have been created in Hindi or in any language, the results would have been same. The ontology was manually created, but the purpose of this ontology is not only to aid in a Hindi dialog system but is manifold.The proposed system clearly gives a major contri- butions in the following areas:• Given a question, how can one find out the an- swer in a cross-lingual set-up.• MostoftheQAsystemsreportedinliteraturetar- get factoid questions, while we have targeted all the types of question possible in Hindi. Major emphasis is given for ”how” and ”why” (causal) questions, for which we have come up with a graph-based algorithm as well. This algorithm can be extended to any resource which provides the information of sub-action, precondition and outcomes (or similar action frame with a slight modification in the algorithm). 
• A domain-oriented open-source knowledge-base is created and uploaded for free use. (Upload link cannot be mentioned to ensure anonymity)Maybury (2003) has mentioned that accessing causal (why) questions is more difficult than how questions and how question is more difficult than factoid (when, where, which, who, what) questions. PurposeNet (Sangal et al., 2013) architecture helps in achieving the task of answering reasoning based questions. By creating a DAG (directed acyclic graph) of the actions and conditions, we can easily traverse the ontology to find the answers for reason- ing based questions (We have considered why and how as reasoning based questions).2 Literature ReviewFor the past decade, both corpus-based (open do- main) (Ravichandran and Hovy, 2002) and ontology based approaches (closed domain) (Ferra ́ndez et al., 2009) have been tried for developing QA systems. Though, not a lot of work has been done for build- ing Ontology based QA system, general purpose QA systems (Vargas-Vera and Motta, 2004; Lopez et al., 2005) have been developed. These systems exten- sively use NLP tools for processing questions and the query is in English. These systems use rdf (Re- source Description Framework) ontologies as their data resources, but their indexing mechanism, which indexes all the entities and relations, loses the effect of using a structural resource in entirety.Indian languages are a late entrant in the field of QA systems. Kaur and Gupta (2013), in their re- cent study have compared various works carried in the domain of QA for Indian languages. Corpus- based QA system for natural language questions in Hindi has been targeted in the recent work of Sahu et al. (2012). Some other efforts in Indian languages are Gupta and Gupta (2013) for Punjabi and Bandy- opadhyay (2012) for Bengali.Sekine and Grishman (2003) is one of the earli- est attempts to build a QA system for Hindi. The proposed system works in a multi-lingual setting. It takes English queries as input, translates non- functional words to Hindi to determine the answer from Hindi corpus (news corpus), and the retrieved answers are translated to English. The amount of er- rors propagated through translation severely affectsthe QA quality.Previous works which have tackled why questionsinclude that of Oh et al. (2013) in which the au- thors exploit the intra- and inter-sentential clausal relation. Higashinaka and Isozaki (2008) have pro- posed a machine learning based method for recog- nizing causal patterns from corpus. No significant work has been reported for a robust reasoning QA system using ontology.Ontology based dialogue system has been dis- cussed in Milward and Beveridge (2003). One of the earliest works for dialogue system for Indian languages has been carried on in Telugu (Reddy and Bandyopadhyay, 2006). They have also tack- led the problem of coreference and anaphora reso- lution by the dialogue manager, although the same has not been implemented by us. Though, the dia- logue systems are nearly always supplemented by a speech module (Jurafsky and James, 2000; Cole et al., 1997), we are not targeting for such a system.3 Question distribution in HindiIn Hindi, we primarily find 8 question types includ- ing kab (when), kitna (how many), kaise (how), kyun (why), kahan (where), kaun (who), kya (what) and kis (which). As discussed below, the reduplication of each of these question words is possible, some- times there is a change in the requirement of answer type. In this section we briefly explain the various answers expected by each question type3. Though Kumar et al. (2003) also give a brief overview of question types in Hindi, their approach is too partic- ular for answer retrieval from running text and para- graphs.3.1 Question Type with the suitable answer type1. single answer questions: The answers to these questions are mostly singular and these can be extracted directly or by simple aggregation over the ontology.(a) kab (when): Inquiring time of occurrence of certain event.(b) kitna/kitne/kitni (how many): The answer to this question is a numeric entity. Though the3Due to paucity of space, We are not showing examples of all the questions types, but we have shown wherever necessary 
following are exceptions to the rule.• kitne baje (what time): Inquiring time ofoccurrence of certain event.• kitni der/kitna samay (what duration):Requires the duration for an event to occur.(c) kaun (who/which): The answer to this ques-tion is a noun, mostly physical nouns.(d) kis (who/which): The answer to this questionis a noun, mostly physical nouns.(e) kahan (where): The answer to this question is a place. The place can be general or specific,even proper noun.(f) kya (what): This is the most complicatedquestion type. This can either have a boolean answer or a factoid answer:• kya (Can, Does, Is, etc.): The answer ofthis can be a yes or no. For example,(1) kya agli train chadh jaun? can next train board do?Can I board the next train?• kya (what): This can be either a direct or a description type question.(2) MMTS train kya hai? MMTS train what is?What is MMTS train?(3) MMTS train ka samay kya hai? MMTS train of time what is?What is the time of the MMTS train?2. list answer questions: The answer to these ques- tions is an empty or non-empty list.(a) kab kab (when all): The answer to these questions is a list of timings.(b) kitni kitni der/kitne kitne samay mein: An- swered as a list of duration for some event.(c) kaun kaun (who all/which all): The answer includes a list of nouns. This can be found in 2 major forms in Hindi.• kaun kaun X (who all): Here X is a verb. The answer to this question type is a set of nouns performing X.• kaun kaun se X (which all X): It is an- swered as a list of Xs. Here X is a noun.(d) kahan kahan (where all): The answer to this question is a list of place.(e) kis kis: The answer to this question is a list of entities mostly noun.(f) kya kya (what all): A list of descrip- tion/entities is answer to these questions.3. reasoning answer questions: The answers are not direct, instead some reasoning needs to be done with the existing knowledge.4.Questions without question word: These are imperative questions. The answers to these ques- tions vary differently.(4) hyderabad se lingampally jane Hyderabad from Lingampally going wali sabhi gadiyon ke samay batayein.all trains time inform.Inform train timings of all the trains go- ing from Hyderabad to Lingampally.Implications: Apart from these, certain implica- tive questions can also be asked which actually do intend to ask something else.Target Question Types5.3.2(a) kyun/kyun kyun (why): The answer demands reasons for happening or not happening of an event. The answer to this is the reason for the happening of certain event.(b) kaise/kaise kaise (how): The method for do- ing certain action is the answer to these ques- tions. The answer for this is the method of doing that event.All the questions have been taken literally and im- plications have yet not been covered. We have not targeted questions without question word. Addition- ally, due to the high ambiguity for questions with kya (what, does, can, is, etc.) question word, we have planned to tackle this using dialog mechanism for interacting with humans in case of ambiguity. Al- though it has not been described in this paper, we have planned an expert system interface for the kyun (why) and kaise (how) questions.4 Ontology creationAccording to Minock (2005), the following condi- tions should be satisfied for a restricted domain:• It should be circumscribed. • It should be complex.• It should be practical.
Keeping the above in mind, we chose the MMTS train-timing domain as it satisfies all the above cri- teria. To ease answering kyun (why) and kaise (how) questions, we chose PurposeNet as its struc- ture helps in expanding multiple actions and con- ditions. Mayee et al. (2011) attempts to inform the complexity in the action semantics involved in PurposeNet architecture. The action frame in Pur- poseNet contains:• precondition: the conditions before certain ac- tion can happen.• sub-actions: the actions without which an action cannot happen. These actions combine together to form the main action.• outcome: the result of an action.• thematic roles: various roles performed by ob-jects for an action.These features help us a lot when deciding the an-swers for why and how kind of questions. The pro- cedure will be described in section 6.2.Multiple attempts to automatically populate Pur- poseNet have already been carried out in the past (Mayee et al., 2010; Singh and Srivastava, 2011). Although, none of these have been clear as to how to automatically populate the entire Ontology using web resources or a corpus. We decided to manu- ally create the ontology using the online data on the MMTS website. The ontology has been created us- ing OWL (Web Ontology Language)(McGuinness et al., 2004) using Protege. Pellet reasoner runs on this ontology, thus giving us a data resource for retriev- ing query answers.5 System DesignOur design involves the following ideology to ensure the generalization of the methodology:• Multilingual architecture: Although systems other than that of Hindi have not been tested and their specific issues are not handled, the architec- ture of our QA system is designed in such a way that it should work for most of the Indo-Aryan languages.• Domain independence: Our QA system can be extended to various domains like medicine, tourism, transport, etc. However, some more set of specific rules (domain oriented) need to be in- tegrated to cover a larger group of factoid ques-tions in other domains.Figure 1 shows the dialogue system. It is com-prised of a various sub-modules each of which is de- scribed as follows.5.1 Dialog ManagerThis is the major backbone of the dialogue system. The dialogue manager contributes to a variety of functionalities in a dialogue system.1. User interface (UI): This module handles the user interaction. Both to and fro dialogues are delivered by this module.(a) inform user: The module deals with inform- ing the user about the answers or the details of the response the dialogue manager is expect- ing. As the QA system is in Hindi, we have provided a complete Hindi based interface to the users. The user is either prompted for a clearer input (in case of certain ambiguity) or given the answer in Hindi.(b) accept response: This module accepts re- sponses from the user.2. Back-end processing: Some of the functions of this part include acquiring questions, identi- fying incorrect/irrelevant questions, identifying whether the kya type of question is factoid or boolean, argument completion wherever possible and delivering the answer.(Refer Section 6.4).5.2 Language differenceOne of the major challenges in this work was that the ontology was created in English and the questions were given in Hindi. We addressed these issues by:• Devanagari (script for Hindi) digits in a given query is converted to Latin numbers. eg. ”2  e 3   e” is deemed as ”2  e 3   e” for the query.• A dictionary of mapping of proper nouns of the Ontology in Hindi was created. eg. ”    u a  e  ai  a aa ” is deemed as ”Falaknuma  e Hyderabad” for the query.• A mapping of the answer terms from Hindi to English along-with the reverse of above men- tioned dictionary was created. eg. When the an- swer contains the term ”Next train”, it is mapped to ”a      e ” in the final answer. Same is done from question to answer as well.
    Argument Completorno5.3 Extract argumentsThough Hindi is a free word order language, it pro- vides perfect syntactic cues in many cases whereas at some places it makes an ambiguous phrase or sen- tence. We use these syntactic cues for extracting the arguments of the question.• postpositions. eg se (from) specified after a word (which is in the list of stations) is the source sta- tion, whereas se(from) specified after a time for- mat specifies the time from which the user is in- terested in. Similar cues can be found for desti- nation station.• pehli (first) and agli (next) keywords are used for the first (train) search whereas pichli (last from the current time) and aakhri (last in the day) specify the last (train) search.• whether arrival or departure is being talked about • which is the primary entity being inquired.5.4 Identify question typesRegular expression is matched for all the the ques- tion types explained (Section 3). If the question type is identified as a reasoning type question, the control is transferred to the reasoning module, else the fac- toid module solves the query.5.5 Create SPARQL queryFor every question type mentioned in section 3, we have a predefined SPARQL template with prede- fined argument structure. The argument structure is completed by the arguments extracted (section 5.3).Following is a template to find out all the arrival times (calculated by subtracting halt-duration from the departure time) (Line 1) given, the start station (Line 3,4), end station (Line 5,6) and the name of the station at which the time has to be found (Line 7,8).1. SELECT (xsd:time(?deptime) - xsd:duration(?hlt) AS ?arrTime)2. WHERE {3. ?s :source ?src.4. FILTER ( regex ( str(?src), ”depstation”) ). 5. ?s :destination ?dest.6. FILTER ( regex ( str(?dest), ”arrstation”) ). 7. ?s :atStation ?st.8. FILTER ( regex ( str(?st), ”atstation”) ). 9. ?s :stationHaltTime ?hlt.}Natural Language (NL) Question          boolean / factoid questionkyacorrect / incorrect (irrevant) questionyes Extract ArgumentsAIduetnot-icfyomQpuleste- AtirognumTyepnetsfactoid / kya / reasoning questionreasonFull parse the questionExtract mainverb and the assosiated themeTackle reasoning based questions      factoidfactoid    no arguments complete?  provide argumentsyes       Local Word Grouper on the sentenceCreating sparql query using argumentsExtract fac- toid answers   Ontology        Create an- swer templateFill the answer template template with the extracted answer (NL answer)   Translate the answer to HindiFigure 1:of the dialogue system. Blocks in dark color (blue) represent the parts of the QA system, parts in light color (light green) represent the components of the dialogue system, parts with the dotted line box (dark green) represent parts of reasoning question type handler whereas parts within solid box (red) repre- sent factoid type handler. White trapezium repre- sents resource.Figure shows the the complete structure
 Extract action      5.6 Create answer templateUsing the question type, we create predefined an- swer templates, with predefined argument structure. These are plain Hindi sentences with blanks in be- tween to be filled in by the oncoming answers. The entities which we receive and decide at run-time are replaced using the dictionary created in section 5.2. The answer we extract from the above steps is filled in that argument structure and the answer is sent as an output to users.6 ApproachGiven a natural language question in Hindi, our target is to give back natural language answer in Hindi. For this, we initially extract the arguments and then the question type from the question. If the question type is kyun(why) or kaise(how) we send the query to a separate module handling reasoning- based questions (refer section 6.2), otherwise we send it to a module handling factoid questions. We then extract answers from a pellet reasoner which runs over an OWL ontology. Using the question type, we create answer template and after we get the answers from the reasoner, we fill the answer tem- plate, thus returning natural language answers to the user. In case of ambiguities or incompleteness, we ask the user back for the information required.6.1 Handling factoid questionsWe run local word grouper on all the questions to group all the relevant information together and then extract all the arguments and the question type. We select the equivalent sparql query and fill in the ar- guments. The sparql queries templates (section 5.5) are used to extract direct answers from the ontology. OWL and pellet APIs are used for this purpose.6.2 Tackle reasoning questionsCurrently, we have targeted only two reasoning based questions, kyun(why) and kaise(how). These question types inquire about the happening or not happening (the reasoning or steps) of any specific action. In the further sections we explain how to tackle these questions in the PurposeNet framework. PurposeNet framework contains action frame which quite vividly explains the requirements of an action.yesany more sub- action?Find Sub-actionFind Precondition      Pellet reasoner of OWL ontologyFigure 2:around an action.To get completed, an action needs to satisfy all its preconditions. Additionally it needs to have com- pleted all its sub-actions. Those sub-actions again have to satisfy conditions and sub-actions, thus con- tinuing the sequence, till we reach basic actions.6.2.1 DAG(Directed acyclic graph)Satisfying its sub-actions as well as preconditions is the requirement for an action. Similarly, for a pre- condition P, actions which have their outcome as P may or may not be the requirement of P. To answer reasoning based questions, we need to create a net around the main action consisting of its sub-actions and preconditions and the same way for them too by depth first traversal (DFS) of the ontology. Figure 2 shows the steps for creating DAG around an action. We first check the list of sub-actions and precondi- tions. If an action has some sub-actions they are considered for the next cycle. If preconditions are present, we extract those actions which have their outcome as this precondition.Now, this gives us the structure of a graph. If the graph is cyclic anywhere, we can’t give a fixed or- der of steps, wherever a cycle occurs. Considering the resultant will be a DAG, we apply topological sort on this created DAG. Now with the results of this DAG, we can proceed one action after another to ask/confirm the steps for a given action.Answering reasoning questions uses sparql queries (section 5.5) as an internal subroutine, though for this problem we only have sparql queries answering direct answers. For example list of precondition, outcome and subaction for an action. As soon as the question enters this module we run a full parser on the question to identify the main verb. This verb and its arguments would be neededany more precon-dition? yes Figure shows how to construct a DAGFind action for which this condition is outcome  
  outcome have Ticket subaction subaction outcomePay Moneyoutcome precondtion paid Moneyaction. We proceed according to the algorithm 1.        Algorithm 1: Tackle kaise questions   1 Identifythemainaction(kharid(buy)).2 Identifythetheme(ticket(ticket)).3 CreateDAGaroundthemainaction.4 TopologicalsortthisDAG.5 Startfromthebeginningtodisplayallthebase actions (not having any sub-actions) on the way.    precondition have Money  (a) Figure shows the ontological structure around the ac- tion BuyMMTSTicket  Buy MMTS TicketPay Receive Money Tickethave Ticket           have paid Money MoneySo the output of question 5 would be: (b) Figure shows the DAG created around the action 1. Pay MoneyBuyMMTSTicket2. Receive Ticket 3. haveTicket.  Buy MMTS TicketPay Receive Money Tickethave Ticket    6.2.3(6)Tackle kyun questions kyun ticket nahi kharida jawhy ticket not to buy happening? Why am I not able to buy tickets?raha hai?     6.2.2(5)Tackle kaise questionsticket kaise kharidein?tickethow tobuy? How to buy a ticket?So the output of question 6 would be: 1. Do you have money?2. Did you pay money?3. Did you receive ticket?The user would automatically know where has he not done the correct action.One very interesting question that arises is, sup- pose a user hasn’t performed the action of receiving ticket (Receive Ticket) then is it relevant to ask him whether he has money (have Money)? As we can see, in a bigger Ontology where a DAG can be very huge, it would be nearly impossible to ask/suggest such a large number of irrelevant questions. We can make this more intuitive and interactive using an ex- pert system interface by asking only the most rele- vant questions first.have paid Money MoneyBuy MMTS Ticket  (c) Figure shows the topologically sorted order of DAGFigure 3: Figure shows the various steps in extract- ing information from ontology to having a sorted or- der of events using DAG.in creating the DAG.An Example: We consider an action Buy MMTS Ticket. Figure 3a shows how the ontology around this action is configured. Applying our technique of constructing a DAG, we find all the sub-actions and preconditions over which this action is depen- dent and construct edges. We do that for all the sub-actions hierarchically. Now we pick up the pre- condition paid Money and find out that its outcome for pay Money so we create an edge for the same. We repeat this process till we have traversed our complete graph. The completed DAG is shown in figure 3b. Figure 3c depicts the topologically sorted order of events, i.e. if the events are dependent on each other, then the events occurring later cannot occur before a previously enlisted event.We have to do the following operations until we find the main reason of not being able to do the above specified action according to the algorithm 2.  Algorithm 2: Tackle kyun questions   1 Identifythemainaction(kharid(buy)).2 Identifythetheme(ticket(ticket)).3 CreateDAGaroundthemainaction.4 TopologicalsortthisDAG.5 Startfromthebeginningtodisplaybaseactions and non-outcome states. We have to identify the steps for doing a specificReceive Ticket
6.3 Handling kya questionskya questions can be ambiguous as discussed earlier (Section 3.1). The dialog manager asks the user if the question is boolean or factoid types.in the question), which can be used when the query is incomplete or ambiguous.The system identifies the arguments and if any essential argument is missing, the dialog manager asks the users to provide that argument. The prompt given to user is like sawaal adhoora: kripya gan- tavya sthan bataen (question incomplete: please provide the destination station). The user then pro- vides the arguments which are again handled by the dialogue manager.6.4.2 Handling incorrect or irrelavant questionsAlthough this module is not very effective, it han- dles trivial issues, like:• a question without a question word• nouns not being identified in the domain.7 Experiments and ResultsAs no previous work has been reported in this do- main for Hindi till date, we do not have a bench- mark to evaluate our system. Regarding the eval- uation with other ontology based QA systems, the pipeline for QA is nearly same. The coverage of our system with respect to the number of types of questions is much broader than those systems which handle only factoid questions, because in addition to factoid questions, we also handle how and why ques- tion types.We have collected questions from 5 different users, informing them about the domain scenario. Though, it was not a compulsion, they were re- quested to adhere to these guidelines:• the primary domain was train timing domain• single sentence queries were encouraged• The questions should contain one and only onequestion word.We collected 66 questions4, with repetitions, from the 5 users. For a fair evaluation, the test queries were collected after the rules were framed. Hindi questions were entered in Devanagari script and an- swers/dialogue interactions were also given in Hindi in Devanagari script. Table 1 explains the distribu-4The primary focus of the work is to address a larger variety in terms of question types, than the state-of-the-art QA systems. Yet we acknowledge that we should have had a larger testing data for more confident results.• •If the question is factoid type, it is treated exactly as the other factoid types questions and send for processing to other modules.If the question is boolean type, it is treated as a factoid question and if the result from ontology is a not null value, we formulate the answer as ”Yes”. For eg.(7) kya 5 baje ke baad koi MMTS what 5 o’clock after any MMTS train Falaknuma train Falaknuma se Lingampally jaati hai? from Lingampally go is?(a) Does any train go from Falaknuma to Lingam- pally after 5 o’ clock?(b) What is the MMTS train that goes from Falaknuma to Lingampally after 5 o’clock?This question’s literal meaning is the Sentence 7(a), but we consider the question as its ambigu- ous equivalent Sentence 7(b). We then send this sentence to the pipeline and if the answer result by the ontology is not null, we declare that the answer of the question would be Yes else the an- swer would be No.Handling various problems encountered in the QA system6.4In this section we try to cover 2 of the major issues we have not addressed till now. One major setback which the QA system suffered from was the way it handled incomplete questions. The other issue was to identify and notify an incorrect or out of domain question, which is also handled here.6.4.1 Handling incomplete questionsThe sentence given in natural language may or may not contain the complete information. Some information might be implicit by the user and hence it is suitable to assume them. Some of the argu- ments are compulsory for every query while some might be taken as optional and certain assumptions can be made. For a majority of questions, source station and destination station are the two explicit arguments which this system wants, for everything else, we have kept a suitable default value (eg. cur- rent time is taken by default if time is not specified 
  Question Type   # ques.  Stage I Stage II  Stage IIIStage IV   Stage V  alluniq  %  %  %  %   %    Factoid (single answer)    kab/kitne baje kitni derkitni Xkaunkahan      18 9 3 1 7        7 5 3 1 5  9 2 0 0 4        39.5  9 2 0 0 4        39.5  15 4 2 1 5        71.1  15 4 2 1 5        71.1  16 6 3 1 7           86.8   kya (bool. type) kya (other)   6 1  4 1  0 0  0  0 0  0  0 0  0  5 1  85.7  6 1   100    Factoid (list answer)   kab kabkitni kitni der kaun kaun se X kaun kaun X kahan kahan      1 2 2 2 8        1 1 1 2 5  1 0 0 0 3        26.7  1 0 0 0 3        26.7  1 1 2 0 7        73.3  1 1 2 0 7        73.3  1 2 2 0 8           86.7   Reasoning questions  kaise/kaise kaise kyun/kyun kyun   3 1  3 1  0 0  0  2 1  75  2 1  75  2 1  75  3 1   100  Others no question word  22  0  0  0  0  2 100 Total   6643  1928.8  2233.3  4162.1  4771.2  59 89.4 Weighted Avg.     13.2  28.2  43.9  61.0   94.7         Table 1: This table shows the question distribution according to the question type and the number of correctly answered questions by the system at every stage. Stage I shows the results of raw QA system dealing only in factoid questions. Stage II shows the results after adding reasoning system in place. Stage III shows results when local word grouper is added in the pipeline. Stage IV shows the introduction of kya questions with the help of dialog systems. Stage V shows the results after adding argument completer module in the dialog system as well as the pipeline.tion of these questions with the question type ex- plained in the section 3.100 80 6040Tota20 0Figure 4: Figure shows how various stages have affected the accuracy of various question types and the overall dialog system accuracy.We created the dialog system in various stages. It should also be noted that some of the question types had very few questions to actually determine the ex- act accuracy. But the spikes in the number of ques-tions tackled successfully give a fair estimate of the impact of every module. Figure 4 shows the accu- racy of various modules against the various stages.• Stage I: This was our it baseline system handling only the factoid questions which predicted just 19 results out of 66.• StageII:Withanadditionofreasoningbasedan- swer pipeline, 75% of the reasoning based ques- tions could be answered.• Stage III: To increase the efficiency of factoid questions, local word grouper was introduced be- cause of which the efficiency spiked up to 41 questions being answered.• Stage IV: After the addition of kya question dis- criminators, nearly 85% of the questions could be answered• Stage V: The overall accuracy spiked up to 86.36% as soon as the argument completion module was put into place. (Refer section 6.4).8 ConclusionWe have presented an approach for answering Hindi questions. We emphasize on the pipeline for a com-                          Weighted-Average      lFactoid-Single Factoid-List kya Reasoning                        12345StageAccuracy
plete dialog system accepting questions in natural language and answering in natural language. The problems effectively addressed are dealing with a knowledge store in different language and retrieving answers from a structured knowledgebase.ReferencesSomnath Banerjee Sivaji Bandyopadhyay. 2012. Bengali question classification: Towards developing qa sys- tem. In 24th International Conference on Computa- tional Linguistics, page 25.Ronald A Cole, David G Novick, Pieter JE Vermeulen, Stephen Sutton, Mark Fanty, LFA Wessels, Jacques Ho de Villiers, Johan Schalkwyk, Brian Hansen, and Daniel Burnett. 1997. Experiments with a spoken di- alogue system for taking the us census. Speech Com- munication, 23(3):243–260.Oscar Ferra ́ndez, Rube ́n Izquierdo, Sergio Ferra ́ndez, and Jose ́ Luis Vicedo. 2009. Addressing ontology-based question answering with collections of user queries. Information Processing & Management, 45:175–188.Poonam Gupta and Vishal Gupta. 2013. Algorithm for punjabi question answering system. Intl. Journal.Ryuichiro Higashinaka and Hideki Isozaki. 2008. Corpus-based question answering for why-questions. In IJCNLP, pages 418–425.Daniel Jurafsky and H James. 2000. Speech and lan- guage processing an introduction to natural language processing, computational linguistics, and speech.Jaspreet Kaur and Vishal Gupta. 2013. Comparative analysis of question answering system in indian lan- guages. International Journal, 3(7).Praveen Kumar, Shrikant Kashyap, Ankush Mittal, and Sumit Gupta. 2003. A query answering system for e-learning hindi documents. South Asian Language Review, 13(1&2).Vanessa Lopez, Michele Pasin, and Enrico Motta. 2005. Aqualog: An ontology-portable question answering system for the semantic web. In The Semantic Web: Research and Applications, pages 546–562. Springer.Mark T Maybury. 2003. Toward a question answering roadmap. New Directions in Question Answering.PK Mayee, R Sangal, and S Paul. 2010. Automatic ex- traction and incorporation of purpose data into pur- posenet. In Computer Engineering and Technology (ICCET), 2010 2nd International Conference on, vol- ume 6, pages V6–154. IEEE.P Kiran Mayee, Rajeev Sangal, and Soma Paul. 2011. Action semantics in purposenet. In Information and Communication Technologies (WICT), 2011 World Congress on, pages 1299–1304. IEEE.Deborah L McGuinness, Frank Van Harmelen, et al. 2004. Owl web ontology language overview. W3C recommendation, 10(2004-03):10.David Milward and Martin Beveridge. 2003. Ontology- based dialogue systems. In Proc. 3rd Workshop on Knowledge and reasoning in practical dialogue sys- tems (IJCAI03), pages 9–18. Citeseer.Michael Minock. 2005. Where are the killer applications of restricted domain question answering. In Proceed- ings of the IJCAI Workshop on Knowledge Reasoning in Question Answering, page 4.Jong-Hoon Oh, Kentaro Torisawa, Chikara Hashimoto, Motoki Sano, Stijn De Saeger, and Kiyonori Ohtake. 2013. Why-question answering using intra-and inter- sentential causal relations. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013).Deepak Ravichandran and Eduard Hovy. 2002. Learning surface text patterns for a question answering system. In Proceedings of the 40th Annual Meeting on Associ- ation for Computational Linguistics, pages 41–47. As- sociation for Computational Linguistics.Rami Reddy Nandi Reddy and Sivaji Bandyopadhyay. 2006. Dialogue based question answering system in telugu. In Workshop on Multilingual Question An- swering. Association for Computational Linguistics.Shriya Sahu, Nandkishor Vasnik, and Devshri Roy. 2012. Prashnottar: A hindi question answering system. In- ternational Journal of Computer Science & Informa- tion Technology, 4(2).Rajeev Sangal, Soma Paul, and P Kiran Mayee. 2013. Purposenet: A knowledge base organized around pur- pose. In Conceptual Structures for STEM Research and Education, pages 29–30. Springer.Satoshi Sekine and Ralph Grishman. 2003. Hindi- english cross-lingual question-answering system. ACM Transactions on Asian Language Information Processing (TALIP), 2(3):181–192.Chandni Singh and Rishabh Srivastava. 2011. Study and population of artifacts. International Journal of Computer Technology and Electronics Engineering, (NCETCSIT-Dec’2011):1–5.Evren Sirin, Bijan Parsia, Bernardo Cuenca Grau, Aditya Kalyanpur, and Yarden Katz. 2007. Pellet: A practical owl-dl reasoner. Web Semantics: science, services and agents on the World Wide Web, 5(2):51–53.Eriks Sneiders. 2002. Automated question answering us- ing question templates that cover the conceptual model of the database. In Natural Language Processing and Information Systems, pages 235–239. Springer.Maria Vargas-Vera and Enrico Motta. 2004. Aqua– ontology-based question answering system. In MICAI 2004: Advances in Artificial Intelligence, pages 468– 477. Springer.