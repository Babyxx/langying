High-order Graph-based Neural Dependency ParsingAbstractIn this work, we present a novel way of using neural network for graph-based dependency parsing, which fits the neural network into a simple probabilistic model and can be further- more generalized to high-order parsing. In- stead of the sparse features used in traditional methods, we utilize distributed dense feature representations for neural network, which give better feature representations. The proposed parsers are evaluated on English and Chinese Penn Treebanks. Compared to existing work, our parsers give competitive performance with much more efficient inference.1 IntroductionThere have been two classes of typical approaches for dependency parsing: transition-based parsing and graph-based parsing. The former parses sen- tences by making a series of shift-reduce decisions (Yamada and Matsumoto, 2003; Nivre, 2003), while the latter searches for a tree through graph algo- rithms by decomposing trees into factors. This paper will focus on graph-based methods, which are based on dynamic programming strategies (Eisner, 1996; McDonald et al., 2005; McDonald and Pereira, 2006). In this recent decade, extensions have been made to use high-order factors (Carreras, 2007; Koo and Collins, 2010) in graph models and the high- est one considers fourth-order (Ma and Zhao, 2012). However, all those methods usually use sparse indi- cator features as inputs and linear models to get the scores for later inference process. They are easy to∗Correspondence author.suffer from the problem of sparsity, and linear mod- els can be insufficient to effectively integrate all the sparse features in spite of various rich context that can be potentially exploited.Distributed representations and neural network provide a way to alleviate such a drawback (Bengio et al., 2003; Collobert et al., 2011). Instead of high- dimensional sparse indicator feature vectors, dis- tributed representations use low-dimensional dense vectors (also known as embeddings) to represent the features, and then they are usually used in a neu- ral network. For example, in the traditional meth- ods, a word is usually expressed by a one-hot vector; while distributed representations use a dense vector. By appropriate representation learning (usually by back-propagations in neural network), these embed- dings can replace traditional sparse features and per- form quite well together with neural network.In recent years, using distributed representations and neural network has gradually gained popularity in natural language processing (NLP) since the pio- neer work of (Bengio et al., 2003). Several neural network language models have reported exciting re- sults for the tasks of machine translation and speech recognition (Schwenk, 2007; Mikolov et al., 2010; Wang et al., 2013; Wang et al., 2014; Wang et al., 2015). Many other tasks of NLP have also been re- considered using neural network, the SENNA sys- tem1 (Collobert et al., 2011) solved the tasks of part- of-speech (POS) tagging, chunking, named entity recognition and semantic role labeling.In this work, we utilize neural network for first- order, second-order and third-order graph-based de-1http://ronan.collobert.com/senna/  
 pendency parsing, with the help of the existing graph-based parsing algorithms. For high-order parsing, it is performed after the first-order parser prunes unlikely parts of the parsing tree. We use neural network to learn dense representations for word, POS and distance information, and predict how likely the dependency relationships are for a sub-tree factor in the dependency tree. For unlabeled projective dependency parsing, we have put a free distribution of our implementation on the Internet2.The remainder of the paper is organized as fol- lows: Section 2 discusses related work, Section 3 gives the background for graph-based dependency parsing, Section 4 describes our neural network model and how we utilize it with graph-based pars- ing and Section 5 presents our experiments, results and some discussions. We summarize this paper in Section 6.2 Related WorkThere has been a few of attempts to parse with neural network. For dependency parsing, (Chen and Manning, 2014) uses neural network for greedy transition-based dependency parsing. We explore graph-based methods in this work, which might be difficultly utilized with neural network. (Le and Zuidema, 2014) implements a generative depen- dency model with a recursive neural network, but the model is used for re-ranking which needs k-best candidates.For constituency parsing, (Collobert, 2011) uses a convolutional neural network and solves the prob- lem with a hierarchical tagging process. (Socher et al., 2010) and (Socher et al., 2013) use recur- sive neural network to model phrase-based parse trees, but their methods might be unlikely general- ized to dependency parsing because a dependency parse tree has no non-terminal nodes while con- stituency parse trees are derived from the phrases structure.Semi-supervised methods usually incorporate word representations as the embeddings for words in the projection layer in neural network; they usually make use of lots of unlabeled data to find the pat- terns in natural languages. If we utilize pre-trained word vectors (see in Section 5.1), our models can be2 https://github.com/zzsfornlp/nngdparserFigure 1: An example dependency tree.regarded as semi-supervised to some extent. (Koo et al., 2008) uses Brown clustering algorithm to ob- tain word representations, but then transforms them into sparse features as additional features and again uses the traditional methods; while in neural net- work models including this work, the embeddings directly replace sparse features for inputs.3 3.1Graph-based Dependency Parsing Background of Dependency ParsingSyntax information is important for many other tasks (Zhang and Zhao, 2013; Chen et al., 2015). As a classic syntactic problem, dependency pars- ing aims to predict a dependency tree, which di- rectly represents head-modifier relationships be- tween words in a sentence. Figure 1 shows a de- pendency tree, in which all the links connect head- modifier pairs. By enforcing that all the nodes must have one and only one parent and the result- ing graphs should be acyclic and connected, we can get a directed dependency tree for a sentence (we usually add a dummy node ⟨root⟩ for the sentence as the highest level node).Labels or dependency category can also be de- fined for the links in the dependency tree, however, this work will focus on unlabeled dependency pars- ing, because once the parsing tree has been built, la- beling can be very effectively performed. Most de- pendency trees for most treebanks follow a useful constrain that is called projectiveness, i.e., no cross links exist in the tree. In treebanks for major lan- guages such as English, nearly all sentences are pro- jective. Therefore this work also considers projec- tive dependency parsing only.3.2 Graph-based Methods and their DecompositionsIn graph-based methods, dependency trees are de- composed into specific factors that do not influence with each other. Each factor, usually represented by a sub-tree, is given a score individually based on its 
 Figure 2: The decompositions of factors.features. The score for a whole dependency tree T is the summation of the scores of all the factors:parsers also take these algorithms as backbones and use them for inference.In the traditional methods, scores are usually ob- tained directly from a linear model. In the learn- ing phase, parameter estimation methods for struc- tured linear models may adopt averaged perceptron (Collins, 2002; Collins and Roark, 2004) and max- margin methods (Taskar et al., 2004).Still using all the existing parsing algorithms, this work focuses on improving scoring for the factors. In detail, our work uses neural network to deter- mine the scores. Nevertheless the traditional meth- ods might be difficultly extended to neural network because of the non-linearity. Therefore, we do not directly obtain scores from neural network. Instead we utilize a probabilistic model and obtain scores by some transformations, and then use these exist- ing parsing algorithms for inference.Scoretree(T) = p∈f actors(T )Scorefactor(p)According to the sub-tree size of the factors, we can define the order of the graph model, some of the decomposition methods are shown in Figure 2. As the simplest case, the first-order model just con- siders sub-tree factor of single edge and its score is obtained by adding all the scores of the edges. For second-order models, another node is added into the factor, which can be either sibling or grandpar- ent. For third-order models, the simplest form is the grand-sibling decomposition, which adds both sibling and grandparent nodes. Existing work also applied various decompositions, such as third-order tri-sibling (Koo and Collins, 2010) which considers two siblings of the modifier and fourth-order grand- tri-sibling (Ma and Zhao, 2012) which adds a grand- parent node on tri-siblings.For the sake of simplicity and the convenient use of neural network, we only consider four models dis- cussed above (the sub-tree patterns of their factors are also shown in Figure 2). The notations for the four models are defined as follows:• o1, first-order model• o2sib, second-order model with sibling nodes• o2g, second-order model with grandparent nodes• o3g, third-order model with both sibling and grand- parent nodes3.3 Parsing AlgorithmsGraph-based methods usually need to use dynamic programming based parsing algorithms, which make use of the scores of sub-trees for larger sub-trees in a bottom-up way. These algorithms solve the inference problem, that is, how to get an optimal tree given the scores for the parts. Our proposed4 4.1Neural Network Parsers The Probabilistic ModelFor graph-based dependency parsing, it is not straightforward to extend the linear models to the more powerful nonlinear neural network, because we need to figure out the scores for the factors of the tree, which are not specified in the original tree- bank. That is, we only know which factors are in the correct parsing tree, but there are no natural ways to indicate how they are scored; the only intuition is to give high scores to the right factors and low scores to the wrong ones.In this work, a simple probabilistic model is adopted for the neural network parsers. It is one of Eisner’s models (Eisner, 1996). Precisely, Eisner’s model A is chosen and slightly modified for scor- ing. The model describes bi-gram lexical affinities, and it gives each possible link an affinity probability. The final probability of drawing a parsing tree for a sentence is the product of all the affinity probabili- ties. The original model also considers probabilities of words and tags and its formula is given as follows:P r(words, tags, links)= P r(words, tags)·P r(links present or not|words, tags)≈Pr(words,tags)·   Pr(Lhm|tword(h),tword(m)) 1≤h,m≤n
Unlike the original model, we determine only the probability for the parsing tree (the existence of the links):Using single-headed constraint again, for all the factors with the same node as the children, only one can exist in a legal parsing tree. The similar trans- formations can be performed and then again we will take the transformed scores as inputs to the corre- sponding parsing algorithms.We describe the high-order extension by taking the o2g model as an example and other models can be handled in a similar way. We will use the simi- lar notations: Lghm is the binary variable that indi- cates the factor with node g as grandparent, node h as head and node m as modifier exists in the parse tree. We continuously use H as the parent of m and G as its grandparent so that LGHm is 1 (representing an existing factor) in the parser tree. The logarithmic probability can be given by the following equation:log(Pr(T|S))=   Pr(Lghm|context(g,h,m)) g,h,mP r(T |S) =   0≤h≤length(S)0<m≤length(S)P r(Lhm|context(h, m))Here Lhm is a binary variable with Bernoulli distri- bution which means whether node h is the head of node m and context(h, m) means the context of the two nodes which includes words, POS tags and dis- tance.When looking for the best tree, we simply find the tree with highest probability (we use logarithmic form for more convenient computations). Consider- ing the single-headed constrain for dependency tree construction, if we assign 1 to LHm, which makes H the parent of m, we must assign 0 to all other Lhm, h means all the nodes that are not equal to H. The logarithmic probability can be rewritten as follows: log Pr(LGHm =1)  log Pr(Lghm =0)  log(Pr(T|S))= +    =   0<m≤length(S)log Pr(LHm =1)+  0<m≤length(S)    h,g0≤h≤length(S) h̸=H,h̸=mHere H represents the real parent node of node m. The formula is in the form of summation of the fac- tor scores, which are defined as:Score(H, m) = log P r(LHm = 1) 4.3+   0≤h≤length(S)h̸=H,h̸=mlog Pr(Lhm =0) Now we adopt feed-forward neural network to learn and compute the probability for a factor. The inputs for the network are features for a factor such as word forms, POS tags and distance, and the output will be the probability that the factor exists in the parse tree. Figure 3 shows the structure of our neural network for the o2g model, the networks for other models will be similar.For the architecture of the neural network, as usual, the first layer is the projection layer or the em- bedding layer, which performs the concatenation for the embeddings. All features are treated equally and mapped to embeddings of the same dimension. So, the embedding or projection matrix E ∈ Rd×N in- cludes the embeddings for features, where d is the dimension for the embedding, N is total number of possible features.For the rest of the network, it can be viewed as a fully-connected feed-forward neural network with two hidden layers and a probabilistic output layer (we use a two-way softmax output unit to computeAfter defining the score of each dependency fac- tor, we can apply the scores to the existing parsing algorithms (Eisner, 1996; McDonald et al., 2005).4.2 High-Order ParsingWe now generalize the model to high-order parsing. In the first-order model, we define probabilities for the head-modifier pair, which is the factor for first- order parsing. Naturally, we can define probabilities for high-order factors. The probability of a parse tree is the product of all its factors (either existing ones or wrong ones), the probability for one factor is again a binary value which means whether the factor exists in the dependency tree.log Pr(Lhm = 0) h̸=H,g̸=G h̸=m,g̸=mNeural Network Model
 Figure 3: The structure of neural network for o2g model for an example input factor “cake → on → table”. Here we only demonstrate the case of a three-word window. Features for the head node:wh−1, wh, wh+1; th−1, th, th+1; dg,h   Features for the modifier node:wm−1, wm, wm+1; tm−1, tm, tm+1; dh,m   Features for the grandparent node:wg−1, wg, wg+1; tg−1, tg, tg+1 the probability). For the hidden layers, we use hy- perbolic tangent as activation function.The training objective is to maximize the log- arithmic probability of parse trees with an L2- regularization term to avoid over-fitting, which equals to minimizing the cross-entropy loss with L2- regularization:L(θ) = −   log P r(T |S)  + λ · ∥θ∥2Table 1: Features for the o2g model (with three-word windows). w: words, t: POS tags, d: distance. +1 and -1 means neighboring indexes.the features in Table 1 when considering three-word window, there will be three word forms and three tags for each node, h and m both have one distance feature while g does not have one because its parent is unknown at this time. In fact, larger-sized context can be included and a seven-word window is actu- ally considered for later experiments.4.5 Integrating Lower-order Models for Higher-order ParsingFollowing standard practice for high-order models (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), we integrate the lower-order scores into the higher order parsing for better perfor- mance. For o2sib and o2g models in this work, we integrate the scores computed from the first-order model into second order factors. And for o3g model, two lower-order scores are integrated. Specifically, the score for the factor (g, h, s, m) will include the lower-order scores of o1 and o2sib in addition to the third-order score o3gScore(g, h, s, m) from o3g model. The integration of the scores can be shown by the following equation: S2Here θ means parameters of the neural network and λ is the hyper-parameter for weight decay. We ini- tialize all the weights with random values and use mini-batch stochastic gradient descent for training.4.4 Feature SetsWe utilize three kinds of features:• Word forms (inside a specified sized window) • POS tags (for each word)• Distance (to the node’s parent in the factor)Using embeddings and neural network, we only need to provide unigram features, which will be mapped to embeddings in the neural network. The connections between features will be exploited by the non-linear computations of the neural network. Those three kinds of features are treated in the same way as strings in the vocabulary, and special prefix strings are added to POS and distance fea- tures to differ them from word features (“POS ” and “ distance ” respectively).Again, take the situation for o2g model as an ex- ample, there are three nodes in a factor: g for grand- parent, h for head and m for modifier. We show   
 #Number of sentences    Score(g, h, s, m)= o1Score(h, m)+ o2sibScore(h, s, m) + o3gScore(g, h, s, m)Corpus PTB CTBTrain 39832 16091Dev Test 1700 2416 803 1910   #Number of tokens Corpus Train     More importantly, we may let the first-order model to serve as an edge-filter for high-order pars- ing. This type of pruning has been used by many graph-based models (Koo and Collins, 2010; Rush and Petrov, 2012) to avoid too expensive operations in high-order parsing. For our model, we utilize our own first-order neural network model which will produce the probabilities for all the edges in the graph. We simply set a pruning threshold so that all edges whose probabilities are under the thresh- old will be discarded for high-order parsing.4.6 Efficient Neural Network ComputationThis subsection introduces two techniques to speed up neural network computation.Efficient computation strategies have been ex- plored extensively for neural network language models (Morin and Bengio, 2005; Mnih and Hinton, 2008; Vaswani et al., 2013). These models consider speeding up the output softmax layer which contains thousands of neurons. However, it is not the case for our neural network as the output layer of our net- work only has two neurons. Main computation cost in our network is from the first hidden layer, which needs matrix multiplications and the hyperbolic tan- gent activation calculations for the hidden neurons.Similar to some previous work (Devlin et al., 2014; Chen and Manning, 2014), we apply the pre- calculation strategy to speed up the most concerned computation. This can be implemented as calculat- ing a lookup table for the first hidden layer (val- ues before computing activation function), which can replace the operations of the looking-up for em- bedding layer and the matrix multiplication for sec- ond layer (first hidden layer after). With the pre- calculation table, we only need to look up the corre- sponding matrix multiplication results for each posi- tion’s input and add them together to get the values for the first hidden layer.Another technique is to pre-calculate a hyperbolic tangent table, which will replace the computation forDev Test PTB 950348 40121 56702 CTB 437990 20454 50315 Table 2: Statistics for the data sets for dependency pars- ing.the activation function with a table looking-up pro- cess.5 Experiments and DiscussionsThe proposed parsers are evaluated on English Penn Treebank (PTB3.0) and Chinese Penn Tree- bank(CTB7.0). For all the results, we report unla- beled attachment scores (UAS) excluding punctua- tions3 as in previous work (Koo and Collins, 2010; Zhang and Clark, 2008). In Table 2, we show statis- tics of both treebanks.For English, we follow the splitting conventions, using sections 2-21 for training, 22 for developing and 23 for test. We patch the Treebank using Vadas’ NP bracketing4 (Vadas and Curran, 2007) and use the LTH Converter5 (Johansson and Nugues, 2007) to get the dependency treebank. We use Stanford POS tagger (Toutanova et al., 2003) to get predicted POS tags for development and test sets, and the ac- curacies for their tags are 97.2% and 97.4%, respec- tively.For Chinese, we follow the convention described in (Zhang and Clark, 2008). The dependencies are converted with Penn2Malt tool6. As in previous work, we use gold segmentation and POS tags.For both treebanks, all the graph-based parsers run on the same machine with Intel Xeon 3.47GHz CPU using single core.5.1 Different Embedding InitializationsWe initialize the embedding matrix (only the parts for the embeddings of words) with some trained3Punctuations are the tokens whose gold POS tag is one of {“”: ,.}forPTBandPU forCTB.4 http://sydney.edu.au/engineering/it/∼dvadas1 5http://nlp.cs.lth.se/software/treebank converter 6http://stp.lingfil.uu.se/ nivre/research/Penn2Malt.html  
   Source  –Collobert et al. (2011) Pennington et al. (2014) Mikolov et al. (2013)          Wt Wd   0.47 1.41 0.13 0.58 0.02 0.13      Initialize random SENNA7 GloVe8 word2vec9UAS 91.79 91.75 91.73 91.81Threshold #inst(M) Time(min.)Acc.0.01 315 29 0.001 764 650.0001 2591 220Table 4: Effects of pruning methods with different thresh-olds (on English dev set with the o2sib model). 5.3 Main ResultsAs for detailed neural network setting, we use em- beddings of 50 dimensions, and the size of the two hidden layers are 200 and 40, respectively. We ini- tialize the learning rate as 0.1. After each iteration, the parser is tested on the development set and if the accuracy decreases, the learning rate will be halved. We train the models for 10 iterations and select the ones that perform best on the development set.For the inputs, we consider a seven-word win- dow. Notice that only with distributed represen- tations, can we incorporate such very-long-context features. We ignore the words that occur less than 3 times in the training treebank and use a default token to represent unknown words.Our evaluations will follow the setting in (Chen and Manning, 2014), which reported results of the transition-based neural network parser. For graph- based parsers, in order to get exact comparisons be- tween traditional methods and neural network meth- ods, we run the traditional graph-based parsers un- der the same executing environment as our parsers. In detail, MSTParser10 for o1 and o2sib models and MaxParser11 (Ma and Zhao, 2012) for o2g and o3g models are respectively used for comparison. Notice that in recent years, there have been plenty of graph- based parsers which utilize various techniques and obtain state-of-art results (Rush and Petrov, 2012; Zhang and McDonald, 2012), however, they will not be included in the comparisons for the reason that we only concern about basic graph-based parsing al- gorithms.We report three accuracy metrics, UAS, Root (percentage of the root words correctly identified), CM (complete rate, percentage of sentences for which the whole tree is correct) and Speed (num- ber of sentences per second). For Chinese, the UAS10 http://sourceforge.net/projects/mstparser/11http://sourceforge.net/projects/maxparser/, this is a C++ implementation for several high-order graph-based parsers  92.41 92.47 92.43  Table 3: Accuracies for different initializations, with first-order models on dev set.word embeddings or word vectors as shown in Table 3. Compared to the random initialization method, using pre-trained embeddings does not bring too sig- nificant improvements. We contribute this mostly to already large enough training set. In fact, the num- ber of the training samples fed to the network is over 20 million. Another possible reason is that the em- bedding initialization only works for word form fea- tures and other features such as POS tags and dis- tance will have to be initialized with random val- ues. Those two types of initializations existing in the same space may cause possible inconsistence. Based on the above empirical results and compari- son, we will only use random initialization for our parsers.5.2 PruningFor high-order models, their full training can be computationally expensive or even impossible, so we must prune unlikely dependencies as we stated before in Section 4.5. We use a simple strategy by setting a fixed probability threshold and the results of different thresholds are shown in Table 4. In this table, the notations are defined as the following:• Wt = %edges wrongly pruned in training set • Wd = %edges wrongly pruned in dev set• #inst = number of instances for one iteration • Time = time for one iteration• Acc. = UAS on dev setWith a large threshold, we might prune some cor- rect dependencies, but if the threshold is set smaller, more incorrect dependencies will remain and the training will be more expensive. Even though those wrongly pruned dependencies are allowed, their scores are also too low to influence the inference. A threshold of 0.001 is finally chosen for other ex- periments in this work. 
   UAS Root CM  91.77 92.35 92.18 92.52       96.61 96.40 96.85 96.81    35.89 39.86 38.45 41.10     91.31 91.99 92.12 92.60       95.12 95.90 96.03 96.31    36.67 39.74 40.11 42.63     92.0 – –Parser Speed o1-nn 150 o2sib-nn 109 o2g-nn 89 o3g-nn 38 o1-M st 18 o2sib-M st 14 o2g-M ax 2 o3g-M ax 0.3 transition 1013Table 5: Results on PTB, the English treebank.ing training, we specify a factor as a positive sample only if all the dependencies in it are correct because we only do a binary classification. This might be the limitation for our high-order model and might ex- plain the reason why some of our high-order parsers do not surpass traditional ones in accuracy, we might need more appropriate object functions to improve its learning.Compared to the features of traditional methods, the only information beyond the proposed feature set is the words that fall out of the windows between the nodes in the factor (previously called in-between features) because so far we only use fixed-size inputs for the feed-forward neural network. Extra opera- tions for embedding vectors (like adding embedding vectors) and other forms of neural networks (such as convolutional neural network which can consider the context of a whole sentence) might be explored in the future.6 ConclusionsIn this paper, we show a way to use neural network for graph-based dependency parsing and the method is also suitable for high-order parsing. We show that using distributed representations for neural net- work to replace traditional sparse features in tradi- tional graph models can be suitable for dependency parsing, even though only using a feed-forward net- work. From the evaluation results and comparison with existing models, we show that the proposed parsers get good results with quite efficient infer- ence even though graph-based models usually need at least cubic-time for inference.AcknowledgmentsWe appreciate the anonymous reviewers for valuable comments and suggestions on our paper. Hai Zhao was partially supported by the National Natural Sci- ence Foundation of China (No. 60903119, No. 61170114, and No. 61272248), the National Basic Research Program of China (No. 2013CB329401), the Science and Technology Commission of Shang- hai Municipality (No. 13511500200), the European Union Seventh Framework Program (No. 247619), the Cai Yuanpei Program (CSC fund 201304490199 and 201304490171), and the art and science inter- discipline funds of Shanghai Jiao Tong University.       UAS Root CM  83.59 86.00 84.13 86.01       76.86 77.59 77.75 78.06    26.60 31.94 27.59 31.88     83.31 85.34 84.96 86.41       71.57 75.60 76.32 78.22    27.49 32.98 31.94 34.82     83.9 – –Parsero1-nn 112o2sib-nno2g-nno3g-nn 11 o1-M st 9 o2sib-M st 8 o2g-M ax 1 o3g-M ax 0.1 transition 936Table 6: Results on CTB, the Chinese treebank.and CM both consider root words.Tables 5 and 6 show the results for PTB and CTB.As for name suffix in the tables, nn means our neu- ral network graph-based parsers, Mst means Mst- Parser, M ax means MaxParser, transition means the transition-based neural network parser (Chen and Manning, 2014).From the results, we can see that our parsers can get similar or even better results compared to the traditional graph-based models of the correspond- ing orders. In addition, our speed is faster (notice that even our o3g parser is faster than the tradi- tional first-order graph-based parser). Compared to the transition-based neural network parser, although our parsers are not that fast (transition-based parsers usually have O(n) time complexity), they give better performance in accuracies.5.4 DiscussionsWe find that integrating lower-order models into high-order parsing leads to better results. Although the high-order factors already include the lower- order parts, it might be hard for the neural network to decide whether the whole factor is correct. Dur-Speed 70 49   
ReferencesYoshua Bengio, Re ́jean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic lan- guage model. Journal of Machine Learning Research (JMLR), 3:1137–1155, March.Xavier Carreras. 2007. Experiments with a higher-order projective dependency parser. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 957–961, Prague, Czech Republic, June. Asso- ciation for Computational Linguistics.Danqi Chen and Christopher Manning. 2014. A fast and accurate dependency parser using neural networks. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 740–750, Doha, Qatar, October. Association for Computational Linguistics.Changge Chen, Peilu Wang, and Hai Zhao. 2015. Shal- low discourse parsing using constituent parsing tree. In Proceedings of the Nineteenth Conference on Com- putational Natural Language Learning - Shared Task, pages 37–41, Beijing, China, July. Association for Computational Linguistics.Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proceedings of the 42nd Meeting of the Association for Computa- tional Linguistics (ACL’04), Main Volume, pages 111– 118, Barcelona, Spain, July.Michael Collins. 2002. Discriminative training meth- ods for hidden markov models: Theory and experi- ments with perceptron algorithms. In Proceedings of the 2002 Conference on Empirical Methods in Natu- ral Language Processing, pages 1–8. Association for Computational Linguistics, July.Ronan Collobert, Jason Weston, Le ́on Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research (JMLR), 12:2493–2537, August.Ronan Collobert. 2011. Deep learning for efficient dis- criminative parsing. In AISTATS.Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul. 2014. Fast and robust neural network joint models for sta- tistical machine translation. In Proceedings of the 52nd Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pages 1370–1380, Baltimore, Maryland, June. Association for Computational Linguistics.Jason M. Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proceed- ings of the 16th International Conference on Compu- tational Linguistics, pages 340–345, Copenhagen, Au- gust.Richard Johansson and Pierre Nugues. 2007. Extended constituent-to-dependency conversion for english. In Proceedings of NODALIDA 2007.Terry Koo and Michael Collins. 2010. Efficient third- order dependency parsers. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1–11, Uppsala, Sweden, July. Asso- ciation for Computational Linguistics.Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Pro- ceedings of ACL-08: HLT, pages 595–603, Columbus, Ohio, June. Association for Computational Linguis- tics.Phong Le and Willem Zuidema. 2014. The inside- outside recursive neural network model for depen- dency parsing. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Process- ing (EMNLP), pages 729–739, Doha, Qatar, October. Association for Computational Linguistics.Xuezhe Ma and Hai Zhao. 2012. Fourth-order depen- dency parsing. In Proceedings of COLING 2012: Posters, pages 785–796, Mumbai, India, December. The COLING 2012 Organizing Committee.Ryan McDonald and Fernando Pereira. 2006. On- line learning of approximate dependency parsing al- gorithms. In Proceedings of the 11th Conference of the European Chapter of the ACL (EACL 2006), pages 81–88. Association for Computational Linguistics.Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of the 43rd Annual Meet- ing of the Association for Computational Linguistics (ACL’05), pages 91–98, Ann Arbor, Michigan, June. Association for Computational Linguistics.Tomas Mikolov, Martin Karafia ́t, Lukas Burget, Jan Cer- nocky`, and Sanjeev Khudanpur. 2010. Recurrent neu- ral network based language model. In Proceedings of INTERSPEECH-2010, pages 1045–1048.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor- rado, and Jeff Dean. 2013. Distributed representa- tions of words and phrases and their compositionality. In NIPS.Andriy Mnih and Geoffrey Hinton. 2008. A scalable hierarchical distributed language model. In NIPS.Frederic Morin and Yoshua Bengio. 2005. Hierarchical probabilistic neural network language model. In AIS- TATS05, pages 246–252.Joakim Nivre. 2003. An efficient algorithm for pro- jective dependency parsing. In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT), pages 149–160, April.Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word rep- resentation. In Proceedings of the 2014 Conference
on Empirical Methods in Natural Language Process- ing (EMNLP), pages 1532–1543, Doha, Qatar, Octo- ber. Association for Computational Linguistics.Alexander Rush and Slav Petrov. 2012. Vine pruning for efficient multi-pass dependency parsing. In Pro- ceedings of the 2012 Conference of the North Ameri- can Chapter of the Association for Computational Lin- guistics: Human Language Technologies, pages 498– 507, Montre ́al, Canada, June. Association for Compu- tational Linguistics.Holger Schwenk. 2007. Continuous space language models. Computer Speech and Language, 21(3):492– 518.Richard Socher, Christopher D. Manning, and Andrew Y. Ng. 2010. Learning continuous phrase representa- tions and syntactic parsing with recursive neural net- works. In Proceedings of the NIPS-2010 Deep Learn- ing and Unsupervised Feature Learning Workshop.Richard Socher, John Bauer, Christopher D. Manning, and Ng Andrew Y. 2013. Parsing with compositional vector grammars. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguis- tics (Volume 1: Long Papers), pages 455–465, Sofia, Bulgaria, August. Association for Computational Lin- guistics.Ben Taskar, Dan Klein, Mike Collins, Daphne Koller, and Christopher Manning. 2004. Max-margin parsing. In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 1–8, Barcelona, Spain, July. As- sociation for Computational Linguistics.Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In PRO- CEEDINGS OF HLT-NAACL, pages 252–259.David Vadas and James Curran. 2007. Adding noun phrase structure to the penn treebank. In Proceed- ings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 240–247, Prague, Czech Republic, June. Association for Computational Linguistics.Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and David Chiang. 2013. Decoding with large-scale neu- ral language models improves translation. In Pro- ceedings of EMNLP-2013, pages 1387–1392, Seattle, Washington, USA, October. Association for Computa- tional Linguistics.Rui Wang, Masao Utiyama, Isao Goto, Eiichro Sumita, Hai Zhao, and Bao-Liang Lu. 2013. Converting continuous-space language models into n-gram lan- guage models for statistical machine translation. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 845– 850, Seattle, Washington, USA, October. Association for Computational Linguistics.Rui Wang, Hai Zhao, Bao-Liang Lu, Masao Utiyama, and Eiichiro Sumita. 2014. Neural network based bilin- gual language model growing for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Process- ing (EMNLP), pages 189–195, Doha, Qatar, October. Association for Computational Linguistics.Rui Wang, Hai Zhao, Bao-Liang Lu, Masao Utiyama, and Eiichiro Sumita. 2015. Bilingual continuous-space language model growing for statistical machine trans- lation. IEEE/ACM Transactions on Audio, Speech, and Languange Processing, 23:1209–1220.Hiroyasu Yamada and Yuji Matsumoto. 2003. Statisti- cal dependency analysis with support vector machines. In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT), pages 195–206, April.Yue Zhang and Stephen Clark. 2008. A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing. In Proceedings of the 2008 Conference on Empirical Methods in Nat- ural Language Processing, pages 562–571, Honolulu, Hawaii, October. Association for Computational Lin- guistics.Hao Zhang and Ryan McDonald. 2012. General- ized higher-order dependency parsing with cube prun- ing. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Process- ing and Computational Natural Language Learning, pages 320–331, Jeju Island, Korea, July. Association for Computational Linguistics.Jingyi Zhang and Hai Zhao. 2013. Improving func- tion word alignment with frequency and syntactic in- formation. In IJCAI-2013, pages 2211–2217, Beijing, China, August.