   How to extract collocations of length three: bigram ++ unigram is not the same as unigram ++ bigram XXX XXXAbstractA method for extracting collocations of length three is described: a trigram is partitioned into a bigram and a follow- ing unigram; frequencies of these components enter into the calculation of such association measures as log-likeli- hood-ratio and pointwise mutual information; those pairs of a bigram and a unigram whose measures fall above the threhold <28.0, 14.0> are filtered.This method has been applied to the Lancaster Corpus of Mandarin Chinese and the resulting trigrams turn out to be satisfactory, its subsection achieving the precision of 0.52.Viewing a trigram as a unigram followed by a bigram, quite surprisingly, gives a different set of trigrams. This asymmetry is discussed in detail.1. IntroductionThe notion of collocations has been a hot topic of discussions in the areas of lexicography, foreign and second language teaching, and computational linguistics for the past three decades. Much progress has been made in automatic acquisition of collocations from language corpora. Measures of correlation like pointwise mutual information and log likelihood ratio are by now known to be the right indicators of pairs of words that form collocations.Collocations come not only as pairs of adjacent words but also as triples or quadruples of adjacent words.In fact, a human-compiled dictionary of collocations, such as [BBI97], includes about 1.4 times as many1 trigrams or longer ngrams as bigrams .Descriptions of how to acquire collocations from corpora, however, usually deal just with bigrams, i.e. collocations whose length is 2. In this paper, we describe a simple method of automatically acquiring candidates of trigram collocations from corpora. The language of our choice is Mandarin Chinese and the corpus used is the Lancaster Corpus of Mandarin Chinese ([McEneryXiao]). In Section 2 we describe the basic concepts involved in the acquisition of trigram-collocations. In Section 4, we note that different assumptions can be made with respect to the partitioning of a trigram: a trigram can be seen as a bigram followed by a unigram or as a unigram followed by a bigram. That different assumptions result in different sets of collocation candidates will be shown in this section. The final section sums up the findings and discusses some consequences of these findings for automatic acquisition of collocation candidates of length greater than three.1A cursory count of a random page from [BBI97] reveals that it consists of 83 bigrams, 98 trigrams, 16 quadrigrams, and 3 longer ngrams. A mainreason for the number of trigrams exceeding that of bigrams in this dictionary seems to be the fact that count nouns in English have to cooccur with a determiner to form a noun phrase, thus making the most common type of collocations such as conduct an initiation and a discreet inquiry one of length 3 rather than of length 2.    1
bigram++unigram vs. unigram++bigram2. A trigram can be thought of as a bigram fol- lowed by a unigramIf two words, say, w1 and w2, occur together very often in corpora, so much so that the frequency of the sequence w1 w2 is several thousand times as high as the independent frequencies of w1 and of w2 would lead us to predict, then the bigram is a good candidate of a collocation. In a similar fashion, if three words, say w1, w2 and w3, occur together very often in corpora, the trigram "w1 w2 w3" would also be a candidate.The notion of "often enough" to count as a collocation has been made quite clear by works like [Church- Hanks90] and [Dunning93]. [ChurchHanks90] propose pointwise mutual information as the relevant mea- sure:Equation 1. Pointwise mutual informationThe higher the pmi, the more likely to be a collocation the sequence of words, "x y", would be.This measure of association turns out to be fairly reliable except where the sequence happens to be a rare one. It does injustice to sequences with low frequency. [Dunning93] offers log likelihood ratio to overcome the problem of rare events getting evaluated too highly.Equation 2. Log likelihood ratioIt has been advised, by, among others, [Moore04], to use the following equation in place of the esoteric Equation 2, “Log likelihood ratio”, where p(y|x) is the conditional probability of y given x and C(x,¬y) is the frequency of the pair whose first member is x and whose second member is any word but y.Equation 3. Log likelihood ratio (re-expressed)If a given bigram has a pair of association measures and if each of the figures falls above the threshold, it would count as a potential collocation. Our experience shows that the threshold of 28.0 for the log- likelihood-ratio and that of 14.0 for the pointwise mutual information are reasonable for a corpus of one million words.Let us now turn to the problem of extracting collocations of length three. A corpus of n words yields n unigrams; it yields (n - 1) bigrams; it yields (n - 2) trigrams, and so on. As the association measures given above measure the strength of association between two members in a sequence, we need to view a trigram as having two parts, if we want to use the same measures in extracting trigram collocations. Viewing a trigram as a sequence of a bigram followed by a unigram will do the job.Figure 1. A corpus as a sequence of bigram-unigram pairs...                                                                                                                                                                                                                                            2
bigram++unigram vs. unigram++bigram3. Collocations extracted as heavy-headed tri- gramsThere are 9,878 pairs (whose first member is a bigram and whose second member is a unigram) whose association measures fall above the thresholds. One such pair is: 大都 D 已 D 齐备 A. As is illustrated in Figure 2, “An example of a hevy-headed trigram”, the trigram has the bigram 大都 D 已 D as its first component and the unigram 齐备 A as its second.Figure 2. An example of a hevy-headed trigramIn order to get the hang of the expected precision of these collocations, one can go through the following sample list of a hundred and fifty pairs out of the 9,878.        大都D开发区 N 科技 N 园区 陆续 D 从 P 烤炉 NN亚运会 N 的 U 圆满 A 计划单列市 N 和 C 经济特区 L 汇编语言 L 程序 N 设计 N 动荡 An 不 D 宁 Ag资产 N 耗竭 V 补偿费 N马达 N 的 U 轰鸣声 N帐户 N 的 U 种类 N鸿宾 Nr 的 U 遗产 N罗马 Ns 教廷 N 驻华 Vn东濮 Ns 凹陷 A 来说 C双维 Ns 地区 N 围歼 V转变 V 了 U 话题 N诛杀 V 武林 N 败类 N经济 N 各个 R 环节国际 N 麻风 N 学会负担 N 自己 R 区间车轮 N 在 P 铁轨 N灾变 N 中 F 尽数 D条条 Q 金色 N 丝线这样 R 优异 A 战绩发挥 V 农业 N 区划开发 V 太石村 Ns 马骝岗 Ns 组织 V 必须 D 尽责 Ad 钻进 V 革命 Vn 阵营 N 创业 Vn 奖 N 银奖 N从 P 本地 R 实际 Ad对 P 不规则 A 笔划 N届 Q 乒乓球 N 锦标赛 N场 Q 爱情 N 争夺战 N双 Q 音节 N 单纯词 N她 R 身下 N 咯吱咯吱 O围 V 过来 V 问长问短 I开 V 紧急 A 电话会议 N罩 V 得 U 严严实实 Z惊恐 An 和 C 愤懑 An红娘 N 只得 D 败兴 A对待 V 两岸 N 经贸 J碰到 V 多少 R 艰难险阻 I显得 V 清清爽爽 Z 娉娉婷婷 Z缴税 V 的 U 营业额 N审计 Vn 的 U 专题 N处理 V 两岸 N 经贸 J优美 A 而 C 昂扬 A战士 N 们 K 齐声 D 青山绿水 I 的 U 纤丽产生 V 感应 N 电动势东西 N 一个个 M 笨头笨脑 A个人 N 收入 N 调节税开展 V 全民 N 义务 D审察 V 一下 D 当时 A农村 N 社会 N 总产值显出 V 有点 D 别扭 A 乱七八糟 I 的 U 涂涂画画 V 楼板 N 3. M 顶层 F 长篇小说 L 的 U 畅销 An飘动 V 的 U 风筝 N 颤颤 V 地 U 抖动 V 复议 Vn 和 C 应诉 V 记叙文 N 与 C 议论文 N 针对性 N 地 U 选学 V 青 A 企 Ng 协 Dg 变A倍D值 A横 A 的 U 线条 N饥 Ag 一 M 顿 Dg 单B家N独 D双 B 音节 N 单纯词 N 双 B 音节 N 词语 N 从 C 天上 S 飞过 V 碴 C 和 C 图钉 N电话机 N 和 C 课桌 顺 A 县 N 转业军人 娇 A 声 N 软语 N 红 A 6 M 军团 N 挤 A 人 N 疙疙瘩瘩 广 Ad 益 Vg 酱园 N 与 C 简单 A 劳动 N 与 C 数理 J 统计学 与 C 丧葬 N 仪节 N 并 C 签定 V 协议 N 虽 C 凭着 P 银钱 N 与 C 感应 N 电动势 则 D 可用 A 喷壶 NN NZNNA N已DN N NN N N齐备ANN   3
两 M 排长 N 条凳鱼 N 窝头 N 镇镇鸟 N 随 P 鸾凤 N县 N 级 Q 医疗 N乡 N 老年 T 协会组 N 舍 V 橱柜 N额 N 相应 Z 专项风 N 送来 V 娱乐县 N 以上 F 邮戳时 Ng 声音 N 浑浊 An 种 Ng 却 D 继续 D员 Ng 侵吞 V 过桥费 N 质 Ng 或 C 导体 N刚 Nr 面色 N 惨变 N仪 Nr 在 P 国军 N经 P 团中央 Nt 书记处 N 从 P 武昌 Ns 汉阳门码头 Ns 对 P 本 R 决议 NN NND N N感觉 N 身上 S 虚飘飘 Z 感到 V 疲劳 An 紧张 An 江南 Ns 一带 N 响当当 Z 系统 N 设备 N 订货会 N 古城 N 东门外 S 桥儿沟 Ns 故乡 N 江阴县 Ns 长泾镇 Ns极 N 为 V 轴线 N钍 Ng 钨 N 阴极 N坛 Ng 一 M 绝 Ag脸 Q 的 U 严肃 A抢 V 过 V 话头 N刨 V 性 K 测试 V缦 V 着 U 窑窝 N谓 Vg 宾 N 结构 N 栏杆 N 和 C 窗户 N 参看 V 整体 M 图样 N 参考 V 整体 N 图样 N 贫困 An 不能 V 负担 N 办公厅 N 关于 P 转发 V 轻功 N 提 V 纵 Ag 将官 N 定 V 贴现率 N 农机 N 流动 Vn 劳务 N 济南 Ns 箱包 N 总厂 Nbigram++unigram vs. unigram++bigram   较 D 起 V 真儿 N 尽 D 操 V 国语 N 间 F 松 V 垮垮 A 苏 J 边贸 J 运输队单 Dg 管 V 饲养 V 连 Dg 背 V 带 Dg 翳 G 子 K 还 C华 J 鸡 N 连锁店 NN举行 V 奥尼尔 Nr 戏剧节 N15. M 自制 V 杀虫剂 N2 M 大英 Nr 懵懵懂懂 V4 M 个 Q 战胜国 N加拿大 Ns 总理 N 马尔罗尼 Nr贼 N 党 N 发觉 V曲线 N 显得 V 潇洒局 N 少将 N 总务处其 R 土地 N 补偿费那 C 无比 Z 执诚 A那 R 根 Q 铁栅栏 N那 R 全民 N 饥馑 A堂 Ng 中 F 张灯结彩 I 大都 D 已 D 齐备 A 大少爷 N 并非 D 区区 B 倒闭 V 的 U 长青 A 挑水 V 时 Ng 艰难 Ad4. They can just as well be heavy-tailed tri- gramsIt doesn't take much thought to realize that there would be a different way of doing the same thing. Why don't we simply take a trigram as a unigram followed by a bigram rather than the other way around. It would be a mirror-image case of the way the trigram was partitioned in the previous section.The result is a bit surprising: the number of pairs whose association measures fall above the thresholds turns out to be different from the mirror-image case. It is 8,369 pairs, smaller than the 9,878.This difference calls for a close examination of the members. Is this second group properly included in the first? The answer is even more surprising: they are largely disjoint.Table 1. Sizes of two setsheavy-headed set proper heavy-tailed set proper8,087 6,578Only 18.13% of the heavy-headed trigrams have the same yields as the heavy-tailed trigrams. Just 21.4% of the heavy-tailed trigrams have the same yields as the heavy-headed trigrams. The bulk of the extracted trigrams belong to one group.It would be instructive to take a look at how a certain trigram gets passing association measures when interpreted as heavy-tailed while it fails to get passing measures when interpreted as heavy-headed. Here is a contingency table for the counts related to the trigram 乌黑 Z 的 U 长发 N:An N N      in both sets      1,791           4
bigram++unigram vs. unigram++bigramTable 2. Contingency table (heavy-headed)c(长发) c(¬长发)Total8 999,990 999,998                2    6      16      999,974     c([乌黑 ++ 的]) c(¬ [乌黑 ++ 的]) Total18 999,980              Table 2, “Contingency table (heavy-headed)” contrasts with another contingency table for the same tri- gram, this time viewed as a unigram followed by a bigram.Table 3. Contingency table (heavy-tailed)c([的 ++ 长发) c(¬ [的 ++ 长发])Total12 999,986 999,998             2    10      4      999,982     c(乌黑) c(¬乌黑) Total6 999,992              The actual calculation of the log-likelihood-ratio along the line of Equation 3, “Log likelihood ratio (re- expressed)” is somewhat involved. We simply note that the strengths of association between components of the trigram under consideration, whether it is taken as heavy-headed or as heavy-tailed, are about the same and that they fall above the threshold.However, it is easy to note that figures of the pointwise mutual information are different depending on the partition of the trigram: the heavy-headed trigram gets the value half that of the heavy-tailed trigram: 8 * 18 = 2 * (6 * 12). The threshold of 14.0 lies somewhere between the two figures and the heavy-headed trigram ((乌黑 ++ 的) 长发) gets filtered out.5. More about the asymmetry and conclusionThe asymmetry is certainly linked to the thresholds. The relative size of the intersection would get bigger as the thresholds become lower. If the thresholds become (0,0), the two sets would be exactly the same.The reason why we set certain thresholds to begin with, is that the resulting set of trigrams would more likely contain promising trigrams, i.e., the ones that are indeed good collocations. Thus, it becomes a question of interest whether there is a difference in expected precision between the two sets of trigrams.2We asked a native Chinese speaker to select trigrams that sound like a collocation . One hundred trigrams3 each constituted one of the three sets. The numbers of surviving trigrams differ :2Our thanks go to Dr. Liu Wei, who went over the lists.3In order to give the reader a feeling for the informant's standards, here is the trigrams that belong to both sets (Note that the whole third columnof the 150 trigrams list in Section 3 is just half of the base against which our informant's intuition is applied to give the twenty two, from 汇编语 言 L 程序 N 设计 N to 济南 Ns 箱包 N 总厂 N, in this list): 汇编语言 L 程序 N 设计 N 动荡 An 不 D 宁 Ag资产 N 耗竭 V 补偿费 N 马达 N 的 U 轰鸣声 N 帐户 N 的 U 种类 N转变 V 了 U 话题 N 诛杀 V 武林 N 败类 N谓 Vg 宾 N 结构 N 栏杆 N 和 C 窗户 N 参看 V 整体 M 图样 N 参考 V 整体 N 图样 N 济南 Ns 箱包 N 总厂 N 电流表 N 的 U 读数 N 虚无主义 N 的 U 论调 N听取 V 和 C 审议 V320 M 个 Q 长线 N SY-132 Nx 型 K 卡车 N m Q 的 U 车间 N加长 V 大 A 卡车 N看涨 V 的 U 估测 V敢于 Z 冒 V 风险 N   5
bigram++unigram vs. unigram++bigram   Table 4. Expected precisionheavy-headed set proper.52 8,087heavy-tailed set proper.42 6,578      in both sets      .51      1,791            Noteworthy are the fact that the heavy-headed set (of the type bigram ++ unigram) has higher presicion than the heavy-tailed and that the intersection, i.e., consisting of trigrams that belong to both sets, does not get any higher precision than the two.The asymmetry might set researchers thinking about its consequences on the taxonomy of collocations and about what language-typological consequences it has.It becomes by now clear that basically the same asymmetry would hold in the extraction of collocations of quadrigrams. A quadrigram may be viewed as a unigram followed by a trigram, as a bigram followed by another bigram, or as a trigram followed by a unigram. Which of these three sets would contain the most quadrigrams, which would have the highest precision, and which paterns of collocations belong to which sets, would be some of the more interesting questions. We hope this paper has made it clear that a trigram is not to be taken simply as a bigram followed by a unigram. In doing so, we also hope that our colleagues take an n-gram to be a set of multiple partitions when n is greater than 2.References[BBI97] Morton Benson, Evelyn Benson, and Robert Ilson. The BBI Dictionary of English Word Combinations. Am- sterdam/Philadelphia. John Benjamins Publishing Company. 1997.[ChurchHanks90] Ken Church and Patrick Hanks. “Word association norms, mutual information and lexicography”. Computational Linguistics. 16. 1. 22-29. 1990.[Dunning93] Ted Dunning. “Accurate methods for the statistics of surprise and coincidence”. Computational Linguis- tics. 19. 1. 61-74. 1993.[McEneryXiao] Anthony McEnery and Zhonghua Xiao. The Lancaster Corpus of Mandarin Chinese. A corpus for monolingual and contrastive language study. Religion. 17. 3-4. 2004.[Moore04] Robert C. Moore. “On log-likelihood-ratios and the significance of rare events”. Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, Barcelona, Spain. 333-340. 2004. 缴税 V 的 U 营业额 N 飘动 V 的 U 风筝 N 颤颤 V 地 U 抖动 V 记叙文 N 与 C 议论文 N 针对性 N 地 U 选学 V 双 B 音节 N 单纯词 N 双 B 音节 N 词语 N从 C 天上 S 飞过 V 单 Dg 管 V 饲养 V 抢 V 过 V 话头 N炮兵 N 团 N 参谋长 N 绮瑞娜 Nr 的 U 质问 Vn 达成 V 某种 R 协议 N达成 V 停火 V 协议 N清水 N 冲 V 净 D龙王庙 N 水位 N 观测点 N 奥委会 J 主席 N 萨马兰奇 Nr 衬衣 N 和 C 裤衩 N转移 V 国际 N 视听 N 哑子 N 吃 V 黄连 N强权政治 L 和 C 霸权主义 N 客居 V 四川 Ns 郜鸣山 Ns 古典 B 中国式 B 装饰 Vn 苦口婆心 I 的 U 劝说 Vn 高新技术 N 产业 N 开发区 N 高低杠 N 上 F 腾飞 V空洞 A 的 U 说教 Vn 狂放 V 不 A 羁 Ag 交感神经 L 的 U 兴奋性 N 校舍 N 己 R 达标 V   6