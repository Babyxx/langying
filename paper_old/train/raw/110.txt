Paraphrase Detection Based on Identical Phrase and Similar Word MatchingAbstractParaphrase detection has numerous important applications in natural language processing (such as clustering, summarizing, and detect- ing plagiarism). One approach to detecting paraphrases is to use predicate argument tu- ples. Although this approach achieves high paraphrase recall, its accuracy is generally low. Other approaches focus on matching similar words, but word meaning is often contextual (e.g., ‘get along with,’ ‘look for- ward to’). An effective approach to detect- ing plagiarism would take into account the fact that plagiarists frequently cut and paste whole phrases and/or replace several words with similar words. This generally results in the paraphrased text containing identical phrases and similar words. Moreover, pla- giarists usually insert and/or remove various minor words (prepositions, conjunctions, etc.) to both improve the naturalness and disguise the paraphrasing. We have developed a sim- ilarity matching (SimMat) metric for de- tecting paraphrases that is based on match- ing identical phrases and similar words and quantifying the minor words. The metric achieved the highest paraphrase detection ac- curacy (77.6%) when it was combined with eight standard machine translation metrics. This accuracy is better than the 77.4% rate achieved with the state-of-the-art approach for paraphrase detection.1 IntroductionParaphrase detection is used to determine whether two texts (phrases, sentences, paragraphs, docu- ments, etc.) of arbitrary lengths have the samemeaning. Such detection is widely used to remove the tremendous amount of duplicate information on the Internet. It is also used to handle the over- lap of semantic components in texts. Such compo- nents are used in various natural language applica- tions such as word sense discrimination, summariza- tion, automatic thesaurus extraction, question-and- answer generation, machine translation, and plagia- rist or analogical relation identification.Some researchers in the field of paraphrase de- tection have used vector-based similarity to iden- tify the differences between two sentences (Mihal- cea et al., 2006; Blacoe and Lapata, 2012). The two sentences are represented by two vectors based on the frequency of their words in text corpora. The vectors are compared to estimate sentence similar- ity. Plagiarists attempt to thwart this comparison by modifying the copied sentence by inserting or re- moving a few minor words, replacing words with similar words that have different usage frequencies, etc. Such modification reduces the effectiveness of vector-based similarity analysis.Other researchers have analyzed the difference in meaning between two sentences on the basis of their syntactic parsing trees (Socher et al., 2011; Qiu et al., 2006; Das and Smith, 2009). The structure of the trees is a major factor used various sophisticated algorithms such as recursive autoencoders (Socher et al., 2011), heuristic similarity (Qiu et al., 2006), and probabilistic inference (Das and Smith, 2009). However, these algorithms are affected by manipu- lation (deleting, inserting, reordering, etc.) of the words in the sentences. Such manipulations can sig- nificantly change the structures of the parsing trees.{yusuke,iechizen}@nii.ac.jp
Other researchers (Mihalcea et al., 2006; Chan and Ng, 2008) have used matching algorithms to de- termine the similarity of two sentences. Mihalcea et al. (2006), for example, proposed a method for find- ing the best matching of a word in a sentence with the nearest word in the other sentence. However, word meaning is often contextual (e.g., ‘make sure of,’ ‘take care of’).Machine translation (MT) metrics, which are gen- erally used to evaluate the quality of translated text, can also be used to judge two texts in the same language. Due to the similarity of machine trans- lation and paraphrase detection, many MT metrics have been applied to paraphrase detection (Finch et al., 2005; Madnani et al., 2012). For exam- ple, eight standard MT metrics have been combined to create a state-of-the-art paraphrase detection ap- proach (Madnani et al., 2012). However, the objec- tives of machine translation and paraphrase detec- tion differ: machine translation tries to effectively translate text from one language to another while paraphrase detection tries to identify paraphrased text. This difference affects the application of MT metrics to paraphrase detection.A paraphrase is a restatement of the meaning of a text using other words. It is a specific type of plagiarisms. We identify common practices plagia- rizers who try to paraphrase a text. The Microsoft Research Paraphrase (MSRP) corpus (Dolan et al., 2004) is commonly used to identify the common practices. An example paraphrase pair extracted from this corpus is shown in Figure 1.Plagiarists frequently cut and paste several phrases of different lengths. This can result in a sentence pair containing identical phrases. The two sentences in Figure 1 have two identical phrases: “Intelligence officials” and “a week ago to expect a terrorist attack in Saudi Arabia.” Plagiarists also add and delete minor words to improve the natural- ness of the text. In the example pair, the preposition “in” (in bold) in the second sentence is considered a minor word.Moreover, plagiarists can replace several words with similar words without changing the sentence meaning to avoid paraphrase detection. The words connected by dashed lines with arrows in the exam- ple are most likely such replacements. The remain- ing words are probably the combination of a few ma-nipulations (reorganization, deletion, insertion, re- placement, etc.). Such modifications are typically intended to ensure that the paraphrased sentence has the same meaning as the original sentence.We make several contributions based on an anal- ysis of related work and the common practices of plagiarists in this paper.• We present a heuristic algorithm for finding an optimal matching of identical phrases with maximum lengths.• We suggest removing the minor words from the words remaining in the sentences. These minor words include prepositions, subordinating con- junctions (‘at,’ ‘in,’ etc.), modal verbs, posses- sive pronouns (‘its,’ ‘their,’ etc.), and periods (‘.’).• We present an algorithm for determining the perfect matching of similar words by using the matching algorithm proposed by Kuhn and Munkres (Kuhn, 1955; Munkres, 1957). The degree of similarity between two similar words is identified using WordNet (Pedersen et al., 2004). These similarities are used as weights for the matching algorithm.• We present a related matching (RelMat) met- ric for quantifying the relationship between two sentences on the basis of matching identical phrases and similar words.• We present a brevity penalty metric to reduce the effect of paraphrased sentence modification. This metric is combined with the RelM at met- ric into a similarity matching SimMat metric for effectively detecting paraphrases.We used the MSRP corpus to evaluate the SimMat metric. Our method using the SimMat metric outperformed many previous methods. The SimMat metric had the highest accuracy (77.6%) when used in combination with eight standard MT metrics (MAXSIM, SEPIA, TER, TERp, METEOR, BADGER, BLEU, and NIST). The accuracy was higher than with the state-of-the-art approach (accu- racy=77.4%). The result shows that our method ef- fectively uses the paraphrasing practices commonly used by plagiarists to detect them.
Fig 1 (introduction: main example Intelligence officials told key senators a week ago to expect a terrorist attack in Saudi Arabia, Sen. Pat Roberts (R-Kan.) said yesterday. Intelligence officials in Washington warned lawmakers a week ago to expect a terrorist attack in Saudi Arabia, it was reported today.          2 2.1Related work Paraphrase detectioncea et al., 2006). The similarity of matching two words is based on WordNet. However, the weaknessof this method is that a word in a first sentence is probably matched to more than one word in the sec-ond sentence. This means that a very short sentencecan be detected as a paraphrase of a long sentencein some cases. Another problem with word match-ing is that the meaning of some words depends on 1 the context. For example, the basic meaning of ‘get’ changes when used in the phrasal verb ‘get along with.’Commonly used techniques for detecting para- phrases are based on MT metrics. This is because the translation task is very similar to the paraphrase detection task for text in the same language. For example, Finch et al. (2005) extended a MT met- ric (PER) and combined it with three other standard metrics (BLEU, NIST, and WER) into a method for detecting paraphrases. Another method developed by Madnani et al. (2012) is based on the integration of eight metrics (TER, TERp, BADGER, SEPIA, BLEU, NIST, METEOR, and MAXSIM). However, the main purpose of these metrics is for translating, and their integration is unsuitable for detecting para- phrases. To overcome these weaknesses, we devel- oped a similarity metric and combined it with eight standard metrics, as described below.2.2 Standard MT metricsTwo basic MT metrics for measuring the similarity of two text segments are based on finding the mini- mum number of operators needed to change one seg- ment so that it matches the other one. The transla- tion edit rate (TER) metric (Snover et al., 2006) sup- ports standard operators, including shift, substitu- tion, deletion, and insertion. The TER-Plus (TERp) metric (Snover et al., 2009) supports even more op- erators, including stemming and synonymizing.The BADGER MT metric (Parker, 2008) uses compression and information theory. It is used to calculate the compression distance of two text seg- ments by using Burrows-Wheeler transformation.Figure 1: Example paraphrase pair taken from MSRP corpus.The baseline for paraphrase detection is based on vector-based similarity. Each source mes- sage and target message is represented as a vec- tor using the frequencies of its words (such as term frequency (Mihalcea et al., 2006) and co- occurrence (Blacoe and Lapata, 2012)). The simi- larity of the two vectors is quantified using various measures (e.g., cosine (Mihalcea et al., 2006), addi- tion and point-wise multiplication (Blacoe and La- pata, 2012)). The problem with vector-based meth- ods is to focus on the frequency of separate words or phrases. However, plagiarists can paraphrase by replacing words with similar words that have a very different frequency. Moreover, they can delete and/or insert minor words that do not change the meaning of the original sentences. Such manipula- tions change the quality of the representation vector, which reduces paraphrase detection performance.Several methods have been proposed for over- coming the manipulation problem that use syntactic parsing trees of messages. The replacement of simi- lar words and the use of minor words do not change the basic structure of the trees. Qiu et al. (2006) re- ported a method that detects the similarity of two sentences by heuristically comparing their predicate argument tuples, which are a type of syntactic pars- ing tree. The high paraphrase recall (93%) it attained shows that most paraphrases have the same predicate argument tuples. However, the accuracy was very low (72%). Parsing trees were used for probabilistic inference of paraphrases by Das and Smith (2009).Another method considers these trees as input for a paraphrase detection system based on recursive au- toencoders (Socher et al., 2011). The drawback of the parsing tree approach is that parsing trees are af- fected by the reordering words in a sentence such as the conversion of a sentence from passive voice to active voice. Another method finds the maximum matching for each word in two sentences (Mihal-
This distance represents for probability that one seg- ment is a paraphrase of the other.The SEPIA MT metric (Habash and Elkholy, 2008) is based on the dependence tree and is used to calculate the similarity of two text segments. It extends the tree to obtain the surface span, which is used as the main component of the similarity score. After the components of the tree are matched, a brevity penalty factor is suggested for deciding the difference in tree lengths for the two text segments.Two other MT metrics commonly used in ma- chine translation are the bilingual evaluation under- study (BLEU) metric (Papineni et al., 2002) and the NIST metric (Doddington, 2002) (an extension of the BLEU metric). Both also quantify similarity on the basis of matching words in the original text seg- ment with words in the translated segment. Whereas the BLEU metric simply calculates the number of matching words, the NIST metric takes into account the importance of matching with different levels. The main drawback of these word matching metrics is that a word in a segment can match more than one word in the other segment.Two MT metrics based on non-duplicate match- ing have been devised to overcome this problem. The METEOR metric (Denkowski and Lavie, 2010) uses explicit ordering to identify matching tuples with minimized cross edges. However, it simply per- forms word-by-word matching. The maximum sim- ilarity (MAXSIM) metric (Chan and Ng, 2008) finds the maximum matching of unigram, bigram, and tri- gram words by using the Kuhn-Munkres algorithm. However, the maximum length of the phrase is a tri- gram. Moreover, the similarities of the phrases (uni- gram, bigram, and trigram) are disjointly combined. To overcome these drawbacks with the standard MT metrics, we have developed a heuristic method for finding the maximum of matching tuples up to the length of the text segments being compared. We also developed a metric for sophisticatedly quantifying the similarity on the basis of the matching tuples.3 Similarity matching (SimMat) metricOur proposed similarity metric (SimMat) for quan- tifying the similarity of input text comprises four steps, as illustrated in Figure 2. The following is a step-by-step description of our method using twoFigure 3: Matching identical phrases with their maximum lengths (Step 1).sentences, which is an actual paraphrase pair from the MSRP corpus.s1: “The study is being published today in the journal Science”s2: “Their findings were published today in Sci- ence.”3.1 Match identical phrases (Step 1)The individual words in the two input sentences are normalized using lemmas. The Natural Lan- guage Processing (NLP) library of Stanford Univer- sity (Manning et al., 2014) is used to identify the lemmas. The lemmas for the two example sentences are shown in Figure 3.The heuristic algorithm we developed for match- ing the lemmas in the two sentences repeatedly finds a new matching pair in each round. In each round, a new pair with the maximum phrase length is es- tablished. The pseudo code of the algorithm is illus- trated in Algorithm 1. The stop condition is when there is no new matching pair. For example, two identical lemma of phrases, “be publish today in” and “science,” are matched (as shown as Figure 3).In algorithm 1, the function getLemmas(s) ex- tracts the lemmas of sentence s using the NLP li- brary. The function lenL gets the number of ele- ments in set L. The function match(L1[i],L2[j]) finds the maximum length matching of phrase L1, which starts at the i-th position in the first sentence, and that of phrase L2, which starts at the j-th posi- tion in the second sentence.3.2 Remove minor words (Step 2)The words remaining after phrase matching in Step 1 are used for removing minor words. First, the part of speech (POS) for each word is identified. The Stanford library tool (Manning et al., 2014) is used for this purpose. The POSs for the words in two the example sentences are shown in Figure 4.Fig 3: matching identical phrases (updated)s1: The study is being published today in the journal Science Lem:the study be be publish today in the journal scienceLem:they find be publish today in science . s2: Their findings were published today in Science .     
 s1 s2SimMat    Step 1: Match identical phrasesStep 2:Remove minor wordsStep 3: Matchsimilar wordsStep 4: Calculate      Figure 2: Four steps in calculation of similarity matching (SimMat) metric. Algorithm 1 Match identical phrases.Fig 4: Removing msimilarity metricinor words (updated)  POS:DT NN VBZVBG VBN NN IN DT NN NN s1: The study is being published today in the journal Science Lem:the study be be publish today in the journal science 1: 2: 3: 4: 5: 6: 7: 8: 9:10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22:functionMATCHIDENTICALPHRASES(s1,s2) L1 ← getLemmas(s1);L2 ← getLemmas(s2);P←⊘;repeatnew ← ⊘; fori=0tolenL1 −1doforj=0tolenL2 −1doif {L1[i], L2[j]} ̸∈ P thentmp ← match(L1[i],L2[j]); if lentmp > lennew thennew ← tmp; end ifend if end forend forif new is not null thenP = P   new; end ifuntil (new = ⊘);return P ; endfunctionFigure 4: Remove minor words (Step 2).Figure 6: Find perfect matching of similar words using Kuhn-Munkres algorithm (Kuhn, 1955; Munkres, 1957) (Step 3).gorithm we developed on the basis of the Kuhn- Munkres algorithm (Kuhn, 1955; Munkres, 1957). The weights of each pair in the algorithm are calcu- lated from the similarity of the two lemmas of the words using the path metric (Pedersen et al., 2004). The path(w1, w2) metric computes the shortest path (pathLength) between two words w1 and w2 in the ‘is-a’ hierarchies of WordNet, as shown in Eq. 1. The pathLength is constrained to be a positive in- teger to ensure that 0 <= path <= 1. For example, the path metric for the “study” and “find” pair is 0.33. The perfect matching found for the two ex- ample sentences is shown in Figure 6. The word “study” in sentence s1 is matched with a similar word, “findings,” in sentence s2.4   Lem:they find be publish today in science . s : Their findings were published today in Science .  2 Fig 6: Matching words (updated)POS:PRP$ NNS VBD VBN NN IN NNP . POS:DT NNVBZVBG VBN NN IN DT NN NN s1: The study is being published today in the journal Science Lem:the study be be publish today in the journal science0.33Lem:they find be publish today in science . s2: Their findings were published today in Science . POS:PRP$ NNS VBD VBN NN IN NNP .       Our analysis of the common practices of plagia- rists showed that four types of minor words should be removed: prepositions and subordinating con- junctions (IN), modal verbs (MD), possessive pro- nouns (PRP$), and periods (“.”). These minor POSs generally do not change the meaning of the para- phrased text as they are often used to simply improve the naturalness of the paraphrased text. For example, the two minor POSs (PRP$ and “.”) were deleted from sentence s2 in Figure 4. An example of prepo- sition deletion is illustrated in Figure 1. Detection of remaining type of minor words (modal verbs) is illustrated for an actual paraphrase pair in Figure 5.3.3 Match similar words (Step 3)After minor word deletion in Step 2, the perfect matching of similar words is done using the al-3.41path(w1, w2) = pathLength(w1, w2) (1)Calculate similarity metric (Step 4) Finally, the RelMat metric is calculated using the results of identical phrase matching in Step 1 and similar word matching in Step 3:
 NNSTODT NN VBDNNPNNPPOSNN MD VBVBNDT NN IN NN INNNPNNP . Aides to the general said Mr. Segal 's arrival could have been the source of friction with Mr. Fowler . aide to the general say Mr. Segal 's arrival could have be the source of friction with Mr. Fowler .campaign official say the move may have be a source of some friction with Fowler . Campaign officials said the moves may have been a source of some friction with Fowler . NN NNS VBDDT NNS MD VB VBN DT NN IN DT NN IN NNP .              RelMat(s1,s2) =#Np +  N−1 len(pi)α +  M−1 path(wj)αmodification is typically intended to improve the naturalness of text. Therefore, the two sentences be- ing compared frequently have different lengths. To reduce this effect, we developed a brevity penalty metric p based on the METEOR metric (Denkowski and Lavie, 2010). It is calculated as shown in Eq. 3, where #ReW (s) is the number of words remaining in sentence s after phrase matching and minor word removal. Penalty p is combined with RelMat into the similarity matching SimMat metric, as shown in Eq. 4.p(s1, s2) == 0.5 × ( |#ReW (s1) − #ReW (s2)| )3 (3)i=0 j=0= #Np+#Nw+ N−1len(pi)α + M−11α,(2)where #Np is the total number of words in the matched identical phrases, #Nw is the number of matched similar words, N and M are the cor- responding numbers of matched identical phrases and similar words, pi is the i-th matched phrase in Step 1, len(pi) is the number of words in the phrase pi, and path(wj) is the path metric of the j-th matched word in Step 3.Eq. 2 ensures that 0 <= RelMat <= 1. The RelMat metric equals 1 only if the two sentences are identical. Using #Np only in the numera- tor means that the matching of identical phrases is more important than the matching of similar words. The len(pi)α and path(wj )α with α >= 0 indicate the respective contributions of matched phrase pi and matched word wj to the RelMat metric. The greater the value of α, the greater the contribution of the identical phrases and the lesser the contribution of the similar words. Because 0 <= path(wj ) <= 1, we use 1α to normalize the contributions of the matched words.Threshold α is set to an optimal value of 0.2, as described in more detail in Section 5. The RelMat metric for the two example sentences is calculated using5 + (40.2 + 10.2) + 0.330.2 RelMat= 5+1+(40.2 +10.2)+10.2 =0.87.The remaining words are probably modified by few manipulations (e.g., insertion, deletion). SuchFigure 5: Example of removing minor words (modal verbs). i=0 j=0 max(#ReW (s1), #ReW (s2)) SimMat = RelMat × (1 − p)(4)Penalty metric p and the SimMat metric are re- spectively calculated for the example sentences us- ing Eq. 5 and Eq. 6. To calculate the #ReW, the remaining words (in bold) are shown in Figure 6.p(s1,s2)=0.5×( |5−1| )3 =0.26 (5) max(5, 1)SimMat = 0.87 × (1 − 0.26) = 0.64 (6) Combination of SimMat metric and MTmetricsWe proposed paraphrase detection method by com- bining the SimMat metric with the eight standard MT metrics described above, as shown in Figure 7. The last two steps are described in detail below. 4 
   Step 1: Calculate SimMat metric    Step 3: Detect paraphrases  s1 Paraphrase s2 detection1 dimension15 dimensionsResultof detection73.1% 72.9% 72.7% 72.5% 72.3% 72.1% 71.9% 71.7% 71.5% 71.3% 71.1%0.2, 73.0%0.0, 72.7%0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 Threshold      Step 2: Calculate MT metricsFigure 7: Combination of SimMat metric with eight MT metrics.4.1 Calculate MT metrics (Step 2)The eight standard MT metrics are calculated for the two sentences. Eight libraries are used to quan- tify them. These libraries are suggested by NIST and the state-of-the-art approach for paraphrase de- tection (Madnani et al., 2012). The libraries are described in more detail in the evaluation section. The first six MT metrics (MAXSIM, SEPIA, TER, TERp, METEOR, and BADGER) create six dimen- sions in total. The two remaining metrics (BLEU and NIST) using the n-gram model create four (n=1..4) and five (n=1..5) dimensions, respectively. These 15 dimensions metrics are combined with that of our proposed metric (SimMat) for detect- ing paraphrases in the last step.4.2 Detecting paraphrases (Step 3)The 16 dimensions, 15 from the MT metrics and 1 from our proposed metric (SimMat) are combined for detecting paraphrases using a machine learning approach. Several commonly used machine learn- ing algorithms (including support vector machine, logistic regression, etc.) were evaluated with these dimensions. Such algorithms are run with 10-fold cross validation in the training set of the MRPS cor- pus for choosing the best classifier. Logistic regres- sion had the best performance and was used for de- tection.5 Evaluation 5.1 MSRP corpusWe used the MSRP corpus to evaluate our method. It contains 5801 sentences pairs including 4076 for training and the remaining 1705 for testing.The corpus has 2753 (67.5%) and 1147 (66.5%) paraphrase cases corresponding to training and test- ing datasets. The corpus was annotated by two na- tive speakers. Disagreements in annotation were re- solved by a third native speaker. Agreement between the two annotators was moderate to high (averagingFigure 8: Estimated threshold α.83%). This means that a perfect algorithm for de-tecting paraphrases would have 83% accuracy.5.2 Estimating threshold α for SimMat metricA threshold α is used to adjust the contributions of matched identical phrases and matched similar words. It was estimated using Eq. 2 and the training dataset of the MRPS corpus. The SimMat metric was used as the single dimension for the logistic re- gression algorithm with 10-fold cross validation, as shown in Figure 8. Using only the training dataset ensured that the results did not overfit the test data.The higher the threshold α, the greater the con- tribution of the matched identical phrases and the lesser the contribution of the matched similar words. If α is small, the contributions of identical phrases are low and the contributions of similar words are high, resulting in lower accuracy. However, the SimMat metric is over-estimated if the value of α is too large, resulting in lower accuracy. The high- est accuracy (73.0%) was achieved for α = 0.2. Therefore, α was set to 0.2 for the subsequent ex- periments.5.3 MT metrics resultIn our approach, the proposed metric (SimMat) is combined with eight MT metrics (MAXSIM, SEPIA, TER, TERp, METEOR, BADGER, BLEU, and NIST). These metrics are integrated to create what we call the MTMETRICS algorithm, which is state of the art for paraphrase detection. The eight metrics are re-implemented on the basis of standard libraries suggested by both of the state of the art and a well-known organization – NIST. The details ofAccuracy (%)
  MT metric  Re-implementation  MTMETRICS   Ver. Acc.   F1  Acc.  F1 MAXSIM  1.0167.5%  79.4% 67.2% 79.4% SEPIA  0.268.3%  79.8% 68.1% 79.8% TER  1.0170.1%  81.0% 69.9% 80.9% TERP  1.070.7%  81.0% 74.3% 81.8% BADGER  2.067.2%  79.9% 67.6% 79.9% METEOR  1.571.7%  80.0% 73.1% 81.0% BLEU  13a72.1%  80.8% 72.3% 80.9% NIST  13a71.8%  80.4% 72.8% 81.2% Integration 76.6%  83.1% 77.4% 84.1%  Method Accuracy  F-score Vector Based Similarity (baseline) 65.4%  75.3% Mihalcea et al. (2006) 70.3%  81.3% Qiu et al. (2006) 72.0%  81.6% SimMat 72.7%  81.3% Blacoe and Lapata (2012) 73.0%  82.3% Finch et al. (2005) 75.0%  82.7% Das and Smith (2009) 76.1%  82.7% Madnani et al. (2012) (re-implemented) 76.6%  83.1% Socher et al. (2011) 76.8%  83.6% Madnani et al. (2012) 77.4%  84.1% Combination 77.6%  83.9%                   Table 1: Results for re-implemented MT metrics and MT- METRICS algorithm (Madnani et al., 2012).the re-implementation are shown in Table 1.The versions of the eight libraries for the re- implemented metrics are shown in column 2. They were the latest for each library, for which we used the default settings. Since the versions and settings are not shown for MTMETRICS, there is little dif- ference between the re-implemented metric results and the MTMETRICS results. The results for the integration of the eight re-implemented metrics (ac- curacy=76.6%, F1=83.1%) also differ from the MT-METRICS results (accuracy=77.4%, F1=84.1%).5.4 Comparison with previous methodsThe results of our comparison with previous meth- ods are summarized in Table 2. These methods were also evaluated using the MRPS corpus. Our proposed metric (SimMat) was evaluated using a threshold α of 0.2. This single metric outper- formed many previous methods. The combination of SimMat with the eight MT metrics had the highest accuracy (77.6%).6 ConclusionOur proposed similarity matching (SimMat) met- ric quantifies the similarity between two sentences and can be used to detect whether one is a para- phrase of the other. It is calculated using the match- ing of identical phrases and similar words. Phrase- by-phrase matching is done using a heuristic algo- rithm that determines the longest duplicate phrase in each iteration. Word matching is done using the Kuhn-Munkres algorithm. WordNet is used for de-Table 2: Accuracy and F-score of our method (SimMat), previous methods, and combination of SimMat with eight MT metrics.termining the similarity of two words. This simi- larity is used as the weights for the word-matching algorithm. Minor words, which are often added or removed from paraphrased text to improve natural- ness, can create noise when detecting paraphrases. They are thus removed as doing so generally does not change the meaning. A brevity penalty metric is combined with the SimMat metric to quantify the effect of inserting and/or deleting words.Evaluation using the MSRP corpus showed that the SimMat metric detects paraphrases more ef- fectively than previous methods. The SimMat metric was combined with eight machine trans- lation metrics. Although the accuracy of the eight re-implemented metrics (accuracy=76.6%, F- score=83.1%) was lower than the published result (accuracy=77.4%, F-score=84.1%), their combina- tion with the SimMat metric achieved the best ac- curacy (77.6%), which was higher than with the state-of-the-art approach (77.4%). Moreover, the F- score of the combination (83.9%) is nearly similar with the-state-of-the-art approach (84.1%). These results show that our method is promising approach to detecting paraphrasing.Future work includes quantifying the weights of words in matched phrases, determining the effect of a word’s position in a sentence, and analyzing mis- classified pairs to improve performance.   
ReferencesWilliam Blacoe and Mirella Lapata. 2012. A comparison of vector-based representations for semantic composi- tion. In EMNLP, pages 546–556.Yee Seng Chan and Hwee Tou Ng. 2008. Maxsim: A maximum similarity metric for machine translation evaluation. In ACL, pages 55–62.Dipanjan Das and Noah A. Smith. 2009. Paraphrase identification as probabilistic quasi-synchronous recognition. In ACL, pages 468–476.Michael Denkowski and Alon Lavie. 2010. Extending the meteor machine translation evaluation metric to the phrase level. In NAACL, pages 250–253.George Doddington. 2002. Automatic evaluation of ma- chine translation quality using n-gram co-occurrence statistics. In Proc. of the 2nd International Confer- ence on Human Language Technology Research, pages 138–145.Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un- supervised construction of large paraphrase corpora: Exploiting massively parallel news sources. In COL- ING, pages 350–356.Andrew Finch, Young-Sook Hwang, and Eiichiro Sumita. 2005. Using machine translation evalua- tion techniques to determine sentence-level semantic equivalence. In Proc. of the 3rd International Work- shop on Paraphrasing, pages 17–24.Nizar Habash and Ahmed Elkholy. 2008. Sepia: sur- face span extension to syntactic dependency precision- based mt evaluation. In Proc. of Association for Ma- chine Translation in the Americas.Harold W. Kuhn. 1955. The hungarian method for the assignment problem. Naval research logistics quar- terly, 2(1):83–97.Nitin Madnani, Joel Tetreault, and Martin Chodorow. 2012. Re-examining machine translation metrics for paraphrase identification. In NAACL, pages 182–190.Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language pro- cessing toolkit. In ACL, pages 55–60.Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-based and knowledge-based measures of text semantic similarity. In AAAI, volume 6, pages 775–780.James Munkres. 1957. Algorithms for the assignment and transportation problems. Journal of the Society for Industrial & Applied Mathematics, 5(1):32–38.Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic evalua- tion of machine translation. In ACL, pages 311–318.Steven Parker. 2008. Badger: A new machine translation metric. In Proc. of Association for Machine Transla- tion in the Americas.Ted Pedersen, Siddharth Patwardhan, and Jason Miche- lizzi. 2004. Wordnet:: Similarity: measuring the relat- edness of concepts. In NAACL: Demonstration, pages 38–41.Long Qiu, Min-Yen Kan, and Tat-Seng Chua. 2006. Paraphrase recognition via dissimilarity significance classification. In EMNLP, pages 18–26.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin- nea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proc. of Association for Machine Translation in the Americas, pages 223–231.Matthew G. Snover, Nitin Madnani, Bonnie Dorr, and Richard Schwartz. 2009. Ter-plus: paraphrase, se- mantic, and alignment enhancements to translation edit rate. Machine Translation, 23(2-3):117–127.Richard Socher, Eric H. Huang, Jeffrey Pennin, Christo- pher D. Manning, and Andrew Y. Ng. 2011. Dynamic pooling and unfolding recursive autoencoders for para- phrase detection. In NIPS, pages 801–809.